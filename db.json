{"meta":{"version":1,"warehouse":"1.0.3"},"models":{"Asset":[{"_id":"source/robots.txt","path":"robots.txt","modified":0},{"_id":"themes/light/source/totop.png","path":"totop.png","modified":0},{"_id":"themes/light/source/segmentfault.jpeg","path":"segmentfault.jpeg","modified":0},{"_id":"themes/light/source/kiwenlau.jpg","path":"kiwenlau.jpg","modified":0},{"_id":"themes/light/source/kiwenlau.ico","path":"kiwenlau.ico","modified":0},{"_id":"themes/light/source/js/totop.js","path":"js/totop.js","modified":0},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":0},{"_id":"themes/light/source/js/gallery.js","path":"js/gallery.js","modified":0},{"_id":"themes/light/source/github.png","path":"github.png","modified":0},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0},{"_id":"themes/light/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0},{"_id":"themes/light/source/css/style.styl","path":"css/style.styl","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","path":"css/font/fontawesome-webfont.woff","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","path":"css/font/fontawesome-webfont.ttf","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","path":"css/font/fontawesome-webfont.svg","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","path":"css/font/fontawesome-webfont.eot","modified":0}],"Cache":[{"_id":"themes/light/source/css/_base/utils.styl","shasum":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1466942673000},{"_id":"themes/light/_config.yml","shasum":"754a105642725b8aa0260ea55fe088358776426b","modified":1467728915000},{"_id":"themes/light/languages/de.yml","shasum":"e076c7f2eb29ebcfb04d94861bf3063c4b08078c","modified":1466942673000},{"_id":"themes/light/languages/default.yml","shasum":"fd7397be7789b43c1c163ab4faf106318811c2a8","modified":1466942673000},{"_id":"themes/light/languages/es.yml","shasum":"de273af604b27812cfd4195e7b7f28ceff2734b3","modified":1466942673000},{"_id":"themes/light/languages/ru.yml","shasum":"35aadf8fdd28aaff8a1c8f50e80201dcf8ce0604","modified":1466942673000},{"_id":"themes/light/languages/zh-CN.yml","shasum":"16979cea6653f9ed345b6ed2c09823d75a622ded","modified":1466942673000},{"_id":"themes/light/languages/zh-TW.yml","shasum":"bcfd502bf073d4c451c13d05d2571f814d1766c9","modified":1466942673000},{"_id":"themes/light/layout/_partial/after_footer.ejs","shasum":"12c76068869b3bd8b12d454642235d516c8fb7c7","modified":1466942673000},{"_id":"themes/light/layout/_partial/archive.ejs","shasum":"7e4f7c2909b1b90241424ea2ff8e7b4761d8360f","modified":1466942673000},{"_id":"themes/light/layout/_partial/article.ejs","shasum":"1c78b15b13a95c877cd19b7682173fa14916a3a0","modified":1467267831000},{"_id":"themes/light/layout/_partial/duoshuo.ejs","shasum":"55335191ea821ba78b14203a7dc61a2d652b6509","modified":1466942673000},{"_id":"themes/light/layout/_partial/copyright.ejs","shasum":"837f3e58f9e7a3d9092286f6e06929e45376542d","modified":1467267237000},{"_id":"themes/light/layout/_partial/footer.ejs","shasum":"61224149335ae515b0c23e27070c11b05d78749d","modified":1466942673000},{"_id":"themes/light/layout/_partial/head.ejs","shasum":"abf936294933dab80ce0b20ccc09b3a0cdc829a1","modified":1466942673000},{"_id":"themes/light/layout/_partial/pagination.ejs","shasum":"1206b630a07444e8744365f14ddb26095c925ae1","modified":1466942673000},{"_id":"themes/light/layout/_partial/header.ejs","shasum":"d9a99aca97d8b41ed907fbf5b25df05da3ffa4f6","modified":1466942673000},{"_id":"themes/light/layout/_partial/post/category.ejs","shasum":"be740939c5c2d4ffdbed9557b4e63a590058b476","modified":1466942673000},{"_id":"themes/light/layout/_partial/post/gallery.ejs","shasum":"fafc2501d7e65983b0f5c2b58151ca12e57c0574","modified":1466942673000},{"_id":"themes/light/layout/_partial/post/tag.ejs","shasum":"095418df66a27a28cbab16d7cb0d16001b0e23f1","modified":1466942673000},{"_id":"themes/light/layout/_partial/post/title.ejs","shasum":"d7fbc575d35ae68f9045a382c651450e4131f335","modified":1466942673000},{"_id":"themes/light/layout/_partial/post/share.ejs","shasum":"24c04b319f1b19e887c42db961b90a7e0ab26fdc","modified":1466942673000},{"_id":"themes/light/layout/_partial/totop.ejs","shasum":"30a6520a82568c9862bd2112aead19a3926e3764","modified":1466942673000},{"_id":"themes/light/layout/_partial/sidebar.ejs","shasum":"caf351797a18d03d8ee945ceb9f83785c50c09f9","modified":1466942673000},{"_id":"themes/light/layout/_widget/recent_posts.ejs","shasum":"8f2f3963bd568c681d7585bb8099fb1b3e1d4c81","modified":1466942673000},{"_id":"themes/light/layout/_widget/category.ejs","shasum":"8a2b90dc29661371f060f710668929c3588e15e4","modified":1466942673000},{"_id":"themes/light/layout/_widget/link.ejs","shasum":"d6854930ec54fcfa8acbb2e9210203688357126c","modified":1467688504000},{"_id":"themes/light/layout/_widget/photo.ejs","shasum":"69c5b89327c72cc739e3d58642401201abfc5650","modified":1467688428000},{"_id":"themes/light/layout/_widget/search.ejs","shasum":"e517c8e6941d8d3bb4a2042fce258cc966488c53","modified":1466942673000},{"_id":"themes/light/layout/_widget/tag.ejs","shasum":"1914db78bea49c333067d79fe7ad9567d2b08d00","modified":1466942673000},{"_id":"themes/light/layout/category.ejs","shasum":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1466942673000},{"_id":"themes/light/layout/archive.ejs","shasum":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1466942673000},{"_id":"themes/light/layout/_widget/tagcloud.ejs","shasum":"9053f62cdbadbd109d6569f4a0381b7a05ba5dfc","modified":1467688587000},{"_id":"themes/light/layout/_widget/toc.ejs","shasum":"b95422f579ed3747124cae680c8d4283e28ed267","modified":1467692928000},{"_id":"themes/light/layout/index.ejs","shasum":"e569d8fe0741a24efb89e44781f9e616da17e036","modified":1466942673000},{"_id":"themes/light/layout/layout.ejs","shasum":"72da76881ebf00e71d7cc196f377e37a17ec7a6f","modified":1466942673000},{"_id":"themes/light/layout/page.ejs","shasum":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1466942673000},{"_id":"themes/light/layout/post.ejs","shasum":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1466942673000},{"_id":"themes/light/source/css/_base/layout.styl","shasum":"1b58c21aa48a8f9f7f811af681ac182dd058e23d","modified":1466942673000},{"_id":"themes/light/layout/tag.ejs","shasum":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1466942673000},{"_id":"themes/light/source/css/_partial/archive.styl","shasum":"072e9b8c5ee9acf95ac7cce9c34706d41e412229","modified":1466942673000},{"_id":"themes/light/source/css/_base/variable.styl","shasum":"c4e07cb7f4ec980553cd19d9d2cd8a1a44d4cd82","modified":1466942673000},{"_id":"themes/light/source/css/_partial/footer.styl","shasum":"1757872dbdbd09295a625f13e356aa798a8bb308","modified":1466942673000},{"_id":"themes/light/source/css/_partial/comment.styl","shasum":"a74254a6e713a522134cca4c644fde681c502823","modified":1466942673000},{"_id":"themes/light/source/css/_partial/article.styl","shasum":"8a31547de29ee62e2106044c45a75d1e4903262b","modified":1466942673000},{"_id":"themes/light/source/css/_partial/header.styl","shasum":"0f932c9514d13fea70fe109242e17ee633a2b28a","modified":1466942673000},{"_id":"themes/light/source/css/_partial/index.styl","shasum":"7a8c0ec6ab99a9f8e00c9687aca29d31752424a2","modified":1466942673000},{"_id":"themes/light/source/css/_partial/sidebar.styl","shasum":"e21feea627a06c711f90d9830356c85e150f162e","modified":1467690892000},{"_id":"themes/light/source/css/_partial/syntax.styl","shasum":"418b304ca39948bfbf720f3465962a1846f9a0e6","modified":1466942673000},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","shasum":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1466942673000},{"_id":"themes/light/source/css/style.styl","shasum":"c03b2520e4a85b981e29516cadc0a365e6500e3d","modified":1466942673000},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","shasum":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1466942673000},{"_id":"themes/light/source/fancybox/blank.gif","shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1466942673000},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1466942673000},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","shasum":"273b123496a42ba45c3416adb027cd99745058b0","modified":1466942673000},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1466942673000},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","shasum":"17df19f97628e77be09c352bf27425faea248251","modified":1466942673000},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1466942673000},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","shasum":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1466942673000},{"_id":"themes/light/source/github.png","shasum":"8e4af8e0da1da6fb685e86c19afb5322d559a8ca","modified":1366059086000},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","shasum":"53360764b429c212f424399384417ccc233bb3be","modified":1466942673000},{"_id":"themes/light/source/js/gallery.js","shasum":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1466942673000},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","shasum":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1466942673000},{"_id":"themes/light/source/js/totop.js","shasum":"c6bf5b76a6526ea87ced257f832395f6434cefc0","modified":1466942673000},{"_id":"themes/light/source/segmentfault.jpeg","shasum":"023e7eef6165635b2fee0fc013a8c45b0358a547","modified":1466919662000},{"_id":"themes/light/source/totop.png","shasum":"74f4b1e0f3b47a1a27cee4a8cb712eb33994ca41","modified":1466942673000},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","shasum":"d162419c91b8bab3a4fd327c933a0fcf3799c251","modified":1466942673000},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","shasum":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1466942673000},{"_id":"themes/light/source/kiwenlau.ico","shasum":"987d9c1658c3779037d96f49d8779b9bab18440b","modified":1466942673000},{"_id":"source/_posts/2015/06/08/150608-hadoop-cluster-docker/image-architecture.graffle","shasum":"363be32d8d54a021835f2125e7d93ab5f07ee3e1","modified":1467090844000},{"_id":"source/_posts/2015/06/08/150608-hadoop-cluster-docker/image-architecture.png","shasum":"018c8ad81bcf1ad54778e761bf6c630403eb5e80","modified":1467090692000},{"_id":"source/_posts/2015/06/08/150608-hadoop-cluster-docker.md","shasum":"659fb635866560790aae7efa58b177355b4bb5f4","modified":1467547323000},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/single-mesos-marathon.graffle","shasum":"7dcd48d344e3dc1c7bbcce0871aa44cc4f1f10a8","modified":1467043733000},{"_id":"source/_posts/2015/11/28/151128-single-kubernetes-docker/kubernetes-multiple-docker.graffle","shasum":"851a2ee0c92e642e4dffedaac3fcaef10d216806","modified":1467043439000},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker.md","shasum":"a46ac2531bceff80cc62ff78de417dac746fac84","modified":1467546684000},{"_id":"source/_posts/2015/11/28/151128-single-kubernetes-docker.md","shasum":"73c87cbd4d2cea2c2903c8620c041e1a2295bc2f","modified":1467546633000},{"_id":"source/_posts/2016/01/09/160109-multiple-processes--docker-container/kubernetes-multiple-docker.graffle","shasum":"851a2ee0c92e642e4dffedaac3fcaef10d216806","modified":1467043842000},{"_id":"source/_posts/2016/01/09/160109-multiple-processes--docker-container/kubernetes-single-docker.graffle","shasum":"927aa0c3be32943949d17fa5455c2602a56c87fa","modified":1467043331000},{"_id":"source/_posts/2016/01/09/160109-multiple-processes--docker-container.md","shasum":"5819960205910a689ccf40443f198edc1ea79a52","modified":1467560916000},{"_id":"source/_posts/2016/05/29/160529-compile-hadoop-ubuntu.md","shasum":"af07ccf8c42a27f516f4d239c7c4c26d12dc3cf3","modified":1467546369000},{"_id":"source/_posts/2016/06/05/160605-compile-hadoop-docker/hadoop-docker.graffle","shasum":"33e193fb0f5600d82ebc8da745b09b2ae8d88e15","modified":1467043014000},{"_id":"source/_posts/2016/05/29/160529-compile-hadoop-ubuntu/ubuntu-hadoop.graffle","shasum":"9a2bac1e19c9299faf7e698b2feabde9717bad76","modified":1467043837000},{"_id":"source/_posts/2016/06/12/160612-hadoop-cluster-docker-update/hadoop-cluster-docker.graffle","shasum":"87e9b82b8aebd6a0239bc1582dec43e8356bf0d1","modified":1467042800000},{"_id":"source/_posts/2016/06/05/160605-compile-hadoop-docker.md","shasum":"fadb6cd129c08bcfd9bf0196d5a970894767cbbd","modified":1473259694000},{"_id":"source/_posts/2016/06/12/160612-hadoop-cluster-docker-update.md","shasum":"332aa8fd51e9a2850ea5ae1a78ff9db8bb73f5f9","modified":1467547170000},{"_id":"source/_posts/2016/06/19/160619-vagrant-virtual-machine.md","shasum":"cdbdc494436a96dd6faaf0cf335ac4e1e63ae5a3","modified":1467547115000},{"_id":"source/_posts/2016/06/26/hadoop-cluster-docker-update-english/hadoop-cluster-docker.graffle","shasum":"281797ad8328ad5a147bfaeb4aa33c127d839e8f","modified":1467042750000},{"_id":"source/_posts/2016/06/19/160619-vagrant-virtual-machine/vagrant-vm.graffle","shasum":"7b5bd9e08cc3b4fc85dc7a04f106be0de6edd34b","modified":1467042638000},{"_id":"source/_posts/2016/06/26/hadoop-cluster-docker-update-english.md","shasum":"14b37b76e1c94a152c9a97ed07e94179649f4fb3","modified":1467546166000},{"_id":"source/_posts/2016/07/03/vagrant-vm-cluster/vagrant-vm-cluster.graffle","shasum":"b3e35ad7747c984f5069ad3589677e599395e34d","modified":1467009010000},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/marathon.png","shasum":"a3846ef25a058da2f9d67e5690bd501a3d1f94ff","modified":1467894231000},{"_id":"source/_posts/2016/07/03/vagrant-vm-cluster.md","shasum":"895758b7c475183c6d8bc293db38f96fc01a2de6","modified":1467683906000},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/nginx.png","shasum":"e98b0adf233b13140e2a03a4feb8dc0f8f459af5","modified":1467988777000},{"_id":"source/_posts/2016/09/04/what-is-service-discovery/service-discovery.graffle","shasum":"d1c17c50056f8e3757d9a3fe686223cf62322d13","modified":1472887160000},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform.md","shasum":"acc74e6800d9d634c2f5cfe483fe10c2c8a34fef","modified":1468164651000},{"_id":"source/_posts/2016/09/11/mongodb-inverted-index.md","shasum":"1ac6dd6fbb48468edab950e373632b320f7315ae","modified":1473494980000},{"_id":"source/robots.txt","shasum":"b7d8a5701d5fb60fda9e17342c5fed7aec6b3ab2","modified":1466942673000},{"_id":"source/_posts/2016/09/11/mongodb-inverted-index/mongodb-inverted-index.graffle","shasum":"638bc27d843964c09b8b8c18a541ebafed3de3ba","modified":1473494764000},{"_id":"source/_posts/2016/09/04/what-is-service-discovery.md","shasum":"9852717fd5cbf5655e9b42bdcdeba9f6dba96497","modified":1473001142000},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/single-mesos-marathon.png","shasum":"f5b2995f300b1302e437ef8d4736341a3d5e7661","modified":1467043771000},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/hello.png","shasum":"732ba80324b66a3ff47fafbde89f4c61dfb04a08","modified":1466942673000},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/Marathon.png","shasum":"58abd8a1ba30eedef47649093a37291e29f21e00","modified":1466942673000},{"_id":"source/_posts/2016/01/09/160109-multiple-processes--docker-container/kubernetes-single-docker.png","shasum":"ae7bd83a3077c4f1a1f85211321932ab2e8df3cd","modified":1467043318000},{"_id":"source/_posts/2016/06/26/hadoop-cluster-docker-update-english/hadoop-cluster-docker.png","shasum":"a5ac1810938b8750268386ac39bbacfbf196c546","modified":1467042748000},{"_id":"source/_posts/2016/06/12/160612-hadoop-cluster-docker-update/hadoop-cluster-docker.png","shasum":"a5ac1810938b8750268386ac39bbacfbf196c546","modified":1467042796000},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/mesos.png","shasum":"79e6dfb76d3585a5af23df5a2b185631ede2f179","modified":1467905241000},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/zookeeper.png","shasum":"24357ecf44a1046d4f9191bc7d27ffa4f5957b8c","modified":1467905275000},{"_id":"source/_posts/2016/09/04/what-is-service-discovery/service-discovery.png","shasum":"8295624fb096ba56f23e89e821a6453f54d55d25","modified":1472887174000},{"_id":"source/_posts/2016/05/29/160529-compile-hadoop-ubuntu/ubuntu-hadoop.png","shasum":"a4470187ff958413325ce3241dfea734d05ad4ef","modified":1467043008000},{"_id":"source/_posts/2016/01/09/160109-multiple-processes--docker-container/kubernetes-multiple-docker.png","shasum":"c3762c5e96023233997aef60d1c1e678d1998ba2","modified":1467043447000},{"_id":"source/_posts/2015/11/28/151128-single-kubernetes-docker/kubernetes-multiple-docker.png","shasum":"c3762c5e96023233997aef60d1c1e678d1998ba2","modified":1467043447000},{"_id":"source/_posts/2016/07/03/vagrant-vm-cluster/vagrant-vm-cluster.png","shasum":"d980b3c3af38a43af0168fd7defe0ded28be9652","modified":1467008903000},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/Mesos.png","shasum":"5fbab0dbc5d46b0caa4fab577a0de8c174c0eab6","modified":1467045513000},{"_id":"themes/light/source/kiwenlau.jpg","shasum":"e163949f9617ee5a0b92ef131f6e2bb57a2c0e36","modified":1466942673000},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/mesos-marathon-platform.png","shasum":"aa5290c71ba1721e39ed42fbef45465df0636699","modified":1468155272000},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/mesos-marathon-platform.graffle","shasum":"4383e6048a4699bb494d20ce8d1490d10c550b35","modified":1468155199000},{"_id":"source/_posts/2016/06/05/160605-compile-hadoop-docker/hadoop-docker.png","shasum":"d877b65ca99e60fd05e04df49c83decda3e59e50","modified":1467042878000},{"_id":"source/_posts/2016/06/19/160619-vagrant-virtual-machine/vagrant-vm.png","shasum":"a54ce970b4aef1d48bb0eadd2b2b8290d8987557","modified":1467042651000},{"_id":"public/robots.txt","modified":1473435351643,"shasum":"b7d8a5701d5fb60fda9e17342c5fed7aec6b3ab2"},{"_id":"public/totop.png","modified":1473435351647,"shasum":"74f4b1e0f3b47a1a27cee4a8cb712eb33994ca41"},{"_id":"public/segmentfault.jpeg","modified":1473435351649,"shasum":"023e7eef6165635b2fee0fc013a8c45b0358a547"},{"_id":"public/kiwenlau.jpg","modified":1473435351653,"shasum":"e163949f9617ee5a0b92ef131f6e2bb57a2c0e36"},{"_id":"public/kiwenlau.ico","modified":1473435351656,"shasum":"987d9c1658c3779037d96f49d8779b9bab18440b"},{"_id":"public/js/totop.js","modified":1473435351659,"shasum":"c6bf5b76a6526ea87ced257f832395f6434cefc0"},{"_id":"public/js/jquery.imagesloaded.min.js","modified":1473435351660,"shasum":"4109837b1f6477bacc6b095a863b1b95b1b3693f"},{"_id":"public/js/gallery.js","modified":1473435351662,"shasum":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed"},{"_id":"public/github.png","modified":1473435351664,"shasum":"8e4af8e0da1da6fb685e86c19afb5322d559a8ca"},{"_id":"public/fancybox/jquery.fancybox.pack.js","modified":1473435351666,"shasum":"53360764b429c212f424399384417ccc233bb3be"},{"_id":"public/fancybox/jquery.fancybox.css","modified":1473435351668,"shasum":"5f163444617b6cf267342f06ac166a237bb62df9"},{"_id":"public/fancybox/fancybox_sprite@2x.png","modified":1473435351670,"shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8"},{"_id":"public/fancybox/fancybox_sprite.png","modified":1473435351672,"shasum":"17df19f97628e77be09c352bf27425faea248251"},{"_id":"public/fancybox/fancybox_overlay.png","modified":1473435351676,"shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0"},{"_id":"public/fancybox/fancybox_loading@2x.gif","modified":1473435351677,"shasum":"273b123496a42ba45c3416adb027cd99745058b0"},{"_id":"public/fancybox/fancybox_loading.gif","modified":1473435351679,"shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c"},{"_id":"public/fancybox/blank.gif","modified":1473435351680,"shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a"},{"_id":"public/css/style.css","modified":1473435352119,"shasum":"efa9eb4adba0b5a110023c71189f850ea5842e24"},{"_id":"public/css/font/fontawesome-webfont.woff","modified":1473435352220,"shasum":"0612cddf2f835cceffccc88fd194f97367d0b024"},{"_id":"public/css/font/fontawesome-webfont.ttf","modified":1473435352221,"shasum":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c"},{"_id":"public/css/font/fontawesome-webfont.svg","modified":1473435352223,"shasum":"d162419c91b8bab3a4fd327c933a0fcf3799c251"},{"_id":"public/css/font/fontawesome-webfont.eot","modified":1473435352225,"shasum":"d775f599ff3f23be082e6a9604b4898718923a37"},{"_id":"public/2016/09/11/mongodb-inverted-index/mongodb-inverted-index.graffle","modified":1473494842114,"shasum":"638bc27d843964c09b8b8c18a541ebafed3de3ba"},{"_id":"public/2016/09/04/what-is-service-discovery/service-discovery.graffle","modified":1473435352233,"shasum":"d1c17c50056f8e3757d9a3fe686223cf62322d13"},{"_id":"public/2016/09/04/what-is-service-discovery/service-discovery.png","modified":1473435352235,"shasum":"8295624fb096ba56f23e89e821a6453f54d55d25"},{"_id":"public/2016/07/10/mesos-marathon-platform/marathon.png","modified":1473435352236,"shasum":"a3846ef25a058da2f9d67e5690bd501a3d1f94ff"},{"_id":"public/2016/07/10/mesos-marathon-platform/mesos-marathon-platform.graffle","modified":1473435352239,"shasum":"4383e6048a4699bb494d20ce8d1490d10c550b35"},{"_id":"public/2016/07/10/mesos-marathon-platform/mesos-marathon-platform.png","modified":1473435352242,"shasum":"aa5290c71ba1721e39ed42fbef45465df0636699"},{"_id":"public/2016/07/10/mesos-marathon-platform/mesos.png","modified":1473435352245,"shasum":"79e6dfb76d3585a5af23df5a2b185631ede2f179"},{"_id":"public/2016/07/10/mesos-marathon-platform/nginx.png","modified":1473435352249,"shasum":"e98b0adf233b13140e2a03a4feb8dc0f8f459af5"},{"_id":"public/2016/07/10/mesos-marathon-platform/zookeeper.png","modified":1473435352260,"shasum":"24357ecf44a1046d4f9191bc7d27ffa4f5957b8c"},{"_id":"public/2016/07/03/vagrant-vm-cluster/vagrant-vm-cluster.graffle","modified":1473435352267,"shasum":"b3e35ad7747c984f5069ad3589677e599395e34d"},{"_id":"public/2016/07/03/vagrant-vm-cluster/vagrant-vm-cluster.png","modified":1473435352271,"shasum":"d980b3c3af38a43af0168fd7defe0ded28be9652"},{"_id":"public/2016/06/26/hadoop-cluster-docker-update-english/hadoop-cluster-docker.graffle","modified":1473435352274,"shasum":"281797ad8328ad5a147bfaeb4aa33c127d839e8f"},{"_id":"public/2016/06/26/hadoop-cluster-docker-update-english/hadoop-cluster-docker.png","modified":1473435352277,"shasum":"a5ac1810938b8750268386ac39bbacfbf196c546"},{"_id":"public/2016/06/19/160619-vagrant-virtual-machine/vagrant-vm.graffle","modified":1473435352281,"shasum":"7b5bd9e08cc3b4fc85dc7a04f106be0de6edd34b"},{"_id":"public/2016/06/19/160619-vagrant-virtual-machine/vagrant-vm.png","modified":1473435352285,"shasum":"a54ce970b4aef1d48bb0eadd2b2b8290d8987557"},{"_id":"public/2016/06/12/160612-hadoop-cluster-docker-update/hadoop-cluster-docker.graffle","modified":1473435352292,"shasum":"87e9b82b8aebd6a0239bc1582dec43e8356bf0d1"},{"_id":"public/2016/06/12/160612-hadoop-cluster-docker-update/hadoop-cluster-docker.png","modified":1473435352303,"shasum":"a5ac1810938b8750268386ac39bbacfbf196c546"},{"_id":"public/2016/06/05/160605-compile-hadoop-docker/hadoop-docker.graffle","modified":1473435352307,"shasum":"33e193fb0f5600d82ebc8da745b09b2ae8d88e15"},{"_id":"public/2016/06/05/160605-compile-hadoop-docker/hadoop-docker.png","modified":1473435352311,"shasum":"d877b65ca99e60fd05e04df49c83decda3e59e50"},{"_id":"public/2016/05/29/160529-compile-hadoop-ubuntu/ubuntu-hadoop.graffle","modified":1473435352317,"shasum":"9a2bac1e19c9299faf7e698b2feabde9717bad76"},{"_id":"public/2016/05/29/160529-compile-hadoop-ubuntu/ubuntu-hadoop.png","modified":1473435352320,"shasum":"a4470187ff958413325ce3241dfea734d05ad4ef"},{"_id":"public/2016/01/09/160109-multiple-processes--docker-container/kubernetes-multiple-docker.graffle","modified":1473435352323,"shasum":"851a2ee0c92e642e4dffedaac3fcaef10d216806"},{"_id":"public/2016/01/09/160109-multiple-processes--docker-container/kubernetes-multiple-docker.png","modified":1473435352325,"shasum":"c3762c5e96023233997aef60d1c1e678d1998ba2"},{"_id":"public/2016/01/09/160109-multiple-processes--docker-container/kubernetes-single-docker.graffle","modified":1473435352328,"shasum":"927aa0c3be32943949d17fa5455c2602a56c87fa"},{"_id":"public/2016/01/09/160109-multiple-processes--docker-container/kubernetes-single-docker.png","modified":1473435352332,"shasum":"ae7bd83a3077c4f1a1f85211321932ab2e8df3cd"},{"_id":"public/2015/11/28/151128-single-kubernetes-docker/kubernetes-multiple-docker.graffle","modified":1473435352334,"shasum":"851a2ee0c92e642e4dffedaac3fcaef10d216806"},{"_id":"public/2015/11/28/151128-single-kubernetes-docker/kubernetes-multiple-docker.png","modified":1473435352336,"shasum":"c3762c5e96023233997aef60d1c1e678d1998ba2"},{"_id":"public/2015/09/18/150918-single-mesos-docker/Marathon.png","modified":1473435352339,"shasum":"58abd8a1ba30eedef47649093a37291e29f21e00"},{"_id":"public/2015/09/18/150918-single-mesos-docker/Mesos.png","modified":1473435352343,"shasum":"5fbab0dbc5d46b0caa4fab577a0de8c174c0eab6"},{"_id":"public/2015/09/18/150918-single-mesos-docker/hello.png","modified":1473435352349,"shasum":"732ba80324b66a3ff47fafbde89f4c61dfb04a08"},{"_id":"public/2015/09/18/150918-single-mesos-docker/single-mesos-marathon.graffle","modified":1473435352353,"shasum":"7dcd48d344e3dc1c7bbcce0871aa44cc4f1f10a8"},{"_id":"public/2015/09/18/150918-single-mesos-docker/single-mesos-marathon.png","modified":1473435352355,"shasum":"f5b2995f300b1302e437ef8d4736341a3d5e7661"},{"_id":"public/2015/06/08/150608-hadoop-cluster-docker/image-architecture.graffle","modified":1473435352359,"shasum":"363be32d8d54a021835f2125e7d93ab5f07ee3e1"},{"_id":"public/2015/06/08/150608-hadoop-cluster-docker/image-architecture.png","modified":1473435352361,"shasum":"018c8ad81bcf1ad54778e761bf6c630403eb5e80"},{"_id":"public/2016/09/11/mongodb-inverted-index/index.html","modified":1473494842182,"shasum":"be0de604d356d1688adf3ca1b7410e19e7ea1fb7"},{"_id":"public/2016/09/04/what-is-service-discovery/index.html","modified":1473435352449,"shasum":"9bbb50dd6f1510fd7a94359caada44819eca8028"},{"_id":"public/2016/07/10/mesos-marathon-platform/index.html","modified":1473435352467,"shasum":"c1b3d374cf1438fed3f2a25b84cf2bc185de0ca0"},{"_id":"public/2016/07/03/vagrant-vm-cluster/index.html","modified":1473435352487,"shasum":"6363ef5d23723aef3fcc292aeec559acd28fe51b"},{"_id":"public/2016/06/26/hadoop-cluster-docker-update-english/index.html","modified":1473435352502,"shasum":"b29f40c188a42b360289a48d3bde84c0048e9b46"},{"_id":"public/2016/06/19/160619-vagrant-virtual-machine/index.html","modified":1473435352516,"shasum":"0a6e31c1b5ed4dcc2d9d1c8d815ec6c7b3735eb0"},{"_id":"public/2016/06/12/160612-hadoop-cluster-docker-update/index.html","modified":1473435352525,"shasum":"2524a803b42fb4da1da66a261f4e45e14915d20f"},{"_id":"public/2016/06/05/160605-compile-hadoop-docker/index.html","modified":1473435352535,"shasum":"f35073d01355456fa6649bf1b0f51a3a05e73b8e"},{"_id":"public/2016/05/29/160529-compile-hadoop-ubuntu/index.html","modified":1473435352542,"shasum":"8db0502116372da3c7ef798eccc20f8faa729f8e"},{"_id":"public/2016/01/09/160109-multiple-processes--docker-container/index.html","modified":1473435352556,"shasum":"ad125831e106b7e934c985be425e39ed4bdff00a"},{"_id":"public/2015/11/28/151128-single-kubernetes-docker/index.html","modified":1473435352569,"shasum":"7af00f98350f562c47d4b87b133dc51a6d59ef97"},{"_id":"public/2015/09/18/150918-single-mesos-docker/index.html","modified":1473435352606,"shasum":"f830a97f1be96fa25627a9907e3f5e854f30ce62"},{"_id":"public/2015/06/08/150608-hadoop-cluster-docker/index.html","modified":1473435352634,"shasum":"51b38a59e65eafdc9780cd8a7580b5b23bf5bd51"},{"_id":"public/archives/index.html","modified":1473435352658,"shasum":"a0249848b54c1b766b650811383d29718d6597ed"},{"_id":"public/archives/page/2/index.html","modified":1473435352672,"shasum":"757baa6bc788c3603f5f07028b58374b894b3728"},{"_id":"public/archives/2015/index.html","modified":1473435352685,"shasum":"2e5fb5749dc2ddc56776c7eef44477337aef89f0"},{"_id":"public/archives/2015/06/index.html","modified":1473435352698,"shasum":"25c84f540cced3e791777ee61aac35808ed38557"},{"_id":"public/archives/2015/09/index.html","modified":1473435352709,"shasum":"8d054769b3d0a3ec358dca2c30f021823bf643f3"},{"_id":"public/archives/2015/11/index.html","modified":1473435352718,"shasum":"890af9b45e922891afdd07e51c5663dbd13306ab"},{"_id":"public/archives/2016/index.html","modified":1473435352731,"shasum":"4e32bcbe3661263dc9a2ec4bde891acaacdb7300"},{"_id":"public/archives/2016/01/index.html","modified":1473435352747,"shasum":"30f1534a0866b38729f0ffb22a37e155d21c9022"},{"_id":"public/archives/2016/05/index.html","modified":1473435352767,"shasum":"c63b1185da2a5881db5c0e1b0721c8d98db7ca68"},{"_id":"public/archives/2016/06/index.html","modified":1473435352787,"shasum":"b79a24b9cdae27b64f91f850a608e80cf9fb35f8"},{"_id":"public/archives/2016/07/index.html","modified":1473435352798,"shasum":"6d86d21d1b5b5537a86f715eb32949bed545915e"},{"_id":"public/archives/2016/09/index.html","modified":1473435352810,"shasum":"3675b5a7bfbbbafa57085d9949cb33bb203ff7af"},{"_id":"public/baidusitemap.xml","modified":1473494842338,"shasum":"135c0a5570ce84474d25bfe35e7ef846a7e5ab1a"},{"_id":"public/sitemap.xml","modified":1473494842366,"shasum":"7100533a080eb6745264b5538ca5c1a73829f736"},{"_id":"public/index.html","modified":1473435352849,"shasum":"3ecf4db27a6c0674c45b3fa1f66ebeafa86a8b5b"},{"_id":"public/page/2/index.html","modified":1473435352864,"shasum":"ba87666496a02e052fdfbfd63ebc33e476bc5029"},{"_id":"public/atom.xml","modified":1473494842363,"shasum":"eb9d8fa2b83238c00dff39389cdd08c9d3cdc6e9"},{"_id":"public/tags/MongoDB/index.html","modified":1473435352873,"shasum":"7ab4f1e7f0a39179379abb36324a19a33b1ffc8f"},{"_id":"public/tags/服务发现/index.html","modified":1473435352881,"shasum":"6d88c976151f42d192b2a8b62be3eeebac175bd6"},{"_id":"public/tags/Docker/index.html","modified":1473435352893,"shasum":"0462f7dd3c992a3b68f4af4cbf8d262ef07d9ca4"},{"_id":"public/tags/Mesos/index.html","modified":1473435352901,"shasum":"9edd545a540a7bf48f2febc8aeee43c6a1c13d92"},{"_id":"public/tags/Marathon/index.html","modified":1473435352910,"shasum":"516356e9adfbc33ff69338c0c7fdf54b49a6d3fb"},{"_id":"public/tags/Vagrant/index.html","modified":1473435352923,"shasum":"fa2d22a989023a27e727cd5e59630bcc91c6b568"},{"_id":"public/tags/Hadoop/index.html","modified":1473435352938,"shasum":"b926e07f098d2977c14f737ecb67b8c4e2c94df2"},{"_id":"public/tags/Kubernetes/index.html","modified":1473435352947,"shasum":"2af908eb6706e0a2bd769e32211e5b57ab46b465"},{"_id":"source/_posts/2016/09/11/mongodb-inverted-index/mongodb-inverted-index.png","shasum":"0e96c1b51dcd1102d2c000f2510fedeb356d2b02","modified":1473494925000},{"_id":"public/2016/09/11/mongodb-inverted-index/mongodb-inverted-index.png","modified":1473494842159,"shasum":"1062126201c6963a1481f5034d8af6c4a8b6b3c0"}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"MongoDB优化之倒排索引","date":"2016-09-11T01:00:00.000Z","_content":"\n**摘要:** 为MongoDB中的数据构建倒排索引(Inverted Index)，然后缓存到内存中，可以大幅提升搜索性能。本文将介绍两种构建倒排索引的方法：[MapReduce](https://docs.mongodb.com/manual/core/map-reduce/)和[Aggregation Pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/)。\n\n**GitHub地址:**\n\n- [kiwenlau/mongodb-inverted-index](https://github.com/kiwenlau/mongodb-inverted-index)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-09-11](http://kiwenlau.com/2016/09/11/mongodb-inverted-index/)\n\n<img src=\"mongodb-inverted-index/mongodb-inverted-index.png\" width = \"500\"/>\n\n## 一. 倒排索引\n\n倒排索引(Inverted Index)，也称为反向索引，[维基百科](https://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95)的定义是这样的:\n\n> 是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。\n\n这个定义比较学术，也就是比较反人类，忽略...\n\n倒排索引是搜索引擎中的重要数据结构。搜索引擎的爬虫获取的网页数据可以视作很多键值对，其中Key是网页地址(url)，而Value是网页内容。网页的内容是由很多关键词(word)组成的，可以视作关键词数组。\n\n```\n<url1, [word2, word3]>\n<url2, [word2]>\n<url3, [word1, word2]>\n```\n\n但是，用户是通过关键词搜索的，直接使用原始数据进行查询的话则需要遍历所有键值对，效率是非常低的。\n\n因此，用于搜索的数据结构应该以关键词(word)为Key，以网页地址(url)为Value:\n\n```\n<word1, [url3]>\n<word2, [ur1, url2, url3]>\n<word3, [url1]>\n```\n\n这样的话，查询关键词word2，立即能够获取结果: [ur1, url2, url3]。\n\n简单地说，倒排索引就是把Key与Value对调之后的索引，构建倒排索引的目的是提升搜索性能。\n\n## 二. 测试数据\n\n[MongoDB](https://www.mongodb.com/)是文档型数据库，其数据有三个层级: 数据库(database)，集合(collection)和文档(document)，分别对应关系型数据库中的三个层级的: 数据库(database), 表(table)，行(row)。MongDB中每个的文档是一个JSON文件，例如，本文使用的movie集合中的一个文档如下所示:\n\n```\n{\n\t\"_id\" : ObjectId(\"57d02d60b128567fc130287d\"),\n\t\"movie\" : \"Pride & Prejudice\",\n\t\"starList\" : [\n\t\t\"Keira Knightley\",\n\t\t\"Matthew Macfadyen\"\n\t],\n\t\"__v\" : 0\n}\n```\n\n该文档一共有4个属性:\n\n- _id: 文档ID，由MongoDB自动生成。\n- __v: 文档版本，由MongoDB的NodeJS接口Mongoose自动生成。\n- movie: 电影名称。\n- starList: 演员列表。 \n\n可知，这个文档表示电影[《傲慢与偏见》](https://movie.douban.com/subject/1418200/)，由女神[凯拉·奈特莉](https://movie.douban.com/celebrity/1054448/)主演。\n\n忽略_id与__v，movie集合的数据如下:\n\n```\n{\n    \"movie\": \"Pride & Prejudice\",\n    \"starList\": [\"Keira Knightley\", \"Matthew Macfadyen\"]\n},\n{\n    \"movie\": \"Begin Again\",\n    \"starList\": [\"Keira Knightley\", \"Mark Ruffalo\"]\n},\n{\n    \"movie\": \"The Imitation Game\",\n    \"starList\": [\"Keira Knightley\", \"Benedict Cumberbatch\"]\n}\n```\n\n其中Key为电影名称(movie)，而Value为演员列表(starList)。\n\n这时查询Keira Knightley所主演的电影的NodeJS[代码](https://github.com/kiwenlau/mongodb-inverted-index/blob/master/search.js)如下:\n\n```\nMovie.find(\n{\n    starList: \"Keira Knightley\"\n},\n{\n    _id: 0,\n    movie: 1\n}, function(err, results)\n{\n    if (err)\n    {\n        console.log(err);\n        process.exit(1);\n    }\n    console.log(\"search movie success:\\n\");\n    console.log(JSON.stringify(results, null, 4));\n    process.exit(0);\n});\n```\n\n- 注：本文所有代码使用了MongoDB的NodeJS接口[Mongoose](http://mongoosejs.com/)，它与MongoDB Shell的接口基本一致。\n\n代码并不复杂，但是数据量大时查询性能会很差，因为需要:\n\n- 遍历整个movie集合的所有文档\n- 遍历每个文档的startList数组\n\n因此构建倒排索引可以有效地提升搜索性能。本文将介绍MongoDB中两种构建倒排索引的方法：[MapReduce](https://docs.mongodb.com/manual/core/map-reduce/)和[Aggregation Pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/)。\n\n\n## 三 MapReduce\n\n[MapReduce](http://research.google.com/archive/mapreduce.html)是由谷歌提出的编程模型，适用于多种大数据处理场景，在搜索引擎中，MapReduce可以用于构建网页数据的倒排索引，也可以用于编写网页排序算法PageRank(由谷歌创始人佩奇和布林提出)。\n\nMapReduce的输入数据与输出数据均为键值对集合。MapReduce分为两个函数: Map与Reduce。\n\n- Map函数将输入键值`<k1, v1>`对进行变换，输出中间键值对`<k2, v2>`。\n- MapReduce框架会自动对中间键值对`<k2, v2>`进行分组，Key相同的键值对会被合并为一个键值对`<k2, list(v2)>`。\n- Reduce函数对`<k2, list(v2)>`的Value进行合并，生成结果键值对`<k2, v3>`。\n\n使用MapReduce构建倒排索引的NodeJS[代码](https://github.com/kiwenlau/mongodb-inverted-index/blob/master/mapreduce.js)如下:\n\n```\nvar option = {};\n\noption.map = function()\n{\n    var movie = this.movie;\n    this.starList.forEach(function(star)\n    {\n        emit(star,\n        {\n            movieList: [movie]\n        });\n    });\n};\n\noption.reduce = function(key, values)\n{\n    var movieList = [];\n    values.forEach(function(value)\n    {\n        movieList.push(value.movieList[0]);\n    });\n    return {\n        movieList: movieList\n    };\n};\n\nMovie.mapReduce(option, function(err, results)\n{\n    if (err)\n    {\n        console.log(err);\n        process.exit(1);\n    }\n    console.log(\"create inverted index success:\\n\");\n    console.log(JSON.stringify(results, null, 4));\n    process.exit(0);\n});\n```\n\n代码解释:\n\n- Map函数的输入数据是Movie集合中的各个文档，在代码中用this表示。文档的movie与starList属性构成键值对`<movie, starList>`。Map函数遍历starList，对每个start生成键值对`<star, movieList>`。这时Key与Value进行了对调，且starList被拆分了，此时movieList仅包含单个movie。\n- MongoDB的MapReduce执行框架对成键值对`<star, movieList>`进行分组，star相同的键值对会被合并为一个键值对`<star, list(movieList)>`。这一步是自动进行的，因此在代码中并没有体现。\n- Reduce函数的输入数据是键值对`<star, list(movieList)>`，在代码中，star即为key，而list(movieList)即为values，两者为Reduce函数的参数。Reduce函数合并list(movieList)，从而得到键值对`<star, movieList>`，最终，movieList中将包含该star的所有movie。\n\n在代码中，Map函数与Reduce返回的键值对中的Value是一个对象`{ movieList: movieList }`而不是数组`movieList`，因此代码和结果都显得比较奇怪。MongoDB的MapReduce框架不支持Reduce函数返回数组，因此只能将movieList放在对象里面返回。\n\n输出结果:\n\n```\n[\n    {\n        \"_id\": \"Benedict Cumberbatch\",\n        \"value\": {\n            \"movieList\": [\n                \"The Imitation Game\"\n            ]\n        }\n    },\n    {\n        \"_id\": \"Keira Knightley\",\n        \"value\": {\n            \"movieList\": [\n                \"Pride & Prejudice\",\n                \"Begin Again\",\n                \"The Imitation Game\"\n            ]\n        }\n    },\n    {\n        \"_id\": \"Mark Ruffalo\",\n        \"value\": {\n            \"movieList\": [\n                \"Begin Again\"\n            ]\n        }\n    },\n    {\n        \"_id\": \"Matthew Macfadyen\",\n        \"value\": {\n            \"movieList\": [\n                \"Pride & Prejudice\"\n            ]\n        }\n    }\n]\n```\n\n## 四. Aggregation Pipeline\n\n[Aggregation Pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/)，中文称作聚合管道，用于汇总MongoDB中多个文档中的数据，也可以用于构建倒排索引。\n\nAggregation Pipeline有两个特点，可以进行各种[聚合操作](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/)，并且可以将多个聚合操作组合使用，类似于Linux中的管道操作，前一个操作的输出是下一个操作的输入。\n\n使用Aggregation Pipeline构建倒排索引的NodeJS[代码](https://github.com/kiwenlau/mongodb-inverted-index/blob/master/aggregation Pipeline.js)如下:\n\n```\nMovie.aggregate([\n{\n    \"$unwind\": \"$starList\"\n},\n{\n    \"$group\":\n    {\n        \"_id\": \"$starList\",\n        \"movieList\":\n        {\n            \"$push\": \"$movie\"\n        }\n    }\n},\n{\n    \"$project\":\n    {\n        \"_id\": 0,\n        \"star\": \"$_id\",\n        \"movieList\": 1\n    }\n}], function(err, results)\n{\n    if (err)\n    {\n        console.log(err);\n        process.exit(1);\n    }\n    console.log(\"create inverted index success:\\n\");\n    console.log(JSON.stringify(results, null, 4));\n    process.exit(0);\n});\n```\n\n代码解释:\n\n- $unwind: 将starList拆分，输出结果(忽略_id与__v)为:\n\n```\n[\n    {\n        \"movie\": \"Pride & Prejudice\",\n        \"starList\": \"Keira Knightley\"\n    },\n    {\n        \"movie\": \"Pride & Prejudice\",\n        \"starList\": \"Matthew Macfadyen\"\n    },\n    {\n        \"movie\": \"Begin Again\",\n        \"starList\": \"Keira Knightley\"\n    },\n    {\n        \"movie\": \"Begin Again\",\n        \"starList\": \"Mark Ruffalo\"\n    },\n    {\n        \"movie\": \"The Imitation Game\",\n        \"starList\": \"Keira Knightley\"\n    },\n    {\n        \"movie\": \"The Imitation Game\",\n        \"starList\": \"Benedict Cumberbatch\"\n    }\n]\n```\n\n- $group: 根据文档的starList属性进行分组，然后将分组文档的movie属性合并为movieList，输出结果(忽略_id与__v)为:\n\n```\n[\n    {\n        \"_id\": \"Benedict Cumberbatch\",\n        \"movieList\": [\n            \"The Imitation Game\"\n        ]\n    },\n    {\n        \"_id\": \"Matthew Macfadyen\",\n        \"movieList\": [\n            \"Pride & Prejudice\"\n        ]\n    },\n    {\n        \"_id\": \"Mark Ruffalo\",\n        \"movieList\": [\n            \"Begin Again\"\n        ]\n    },\n    {\n        \"_id\": \"Keira Knightley\",\n        \"movieList\": [\n            \"Pride & Prejudice\",\n            \"Begin Again\",\n            \"The Imitation Game\"\n        ]\n    }\n]\n```\n","source":"_posts/2016/09/11/mongodb-inverted-index.md","raw":"title: MongoDB优化之倒排索引\n\ndate: 2016-09-11 10:00\n\ntags: [MongoDB]\n\n---\n\n**摘要:** 为MongoDB中的数据构建倒排索引(Inverted Index)，然后缓存到内存中，可以大幅提升搜索性能。本文将介绍两种构建倒排索引的方法：[MapReduce](https://docs.mongodb.com/manual/core/map-reduce/)和[Aggregation Pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/)。\n\n**GitHub地址:**\n\n- [kiwenlau/mongodb-inverted-index](https://github.com/kiwenlau/mongodb-inverted-index)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-09-11](http://kiwenlau.com/2016/09/11/mongodb-inverted-index/)\n\n<img src=\"mongodb-inverted-index/mongodb-inverted-index.png\" width = \"500\"/>\n\n## 一. 倒排索引\n\n倒排索引(Inverted Index)，也称为反向索引，[维基百科](https://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95)的定义是这样的:\n\n> 是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。\n\n这个定义比较学术，也就是比较反人类，忽略...\n\n倒排索引是搜索引擎中的重要数据结构。搜索引擎的爬虫获取的网页数据可以视作很多键值对，其中Key是网页地址(url)，而Value是网页内容。网页的内容是由很多关键词(word)组成的，可以视作关键词数组。\n\n```\n<url1, [word2, word3]>\n<url2, [word2]>\n<url3, [word1, word2]>\n```\n\n但是，用户是通过关键词搜索的，直接使用原始数据进行查询的话则需要遍历所有键值对，效率是非常低的。\n\n因此，用于搜索的数据结构应该以关键词(word)为Key，以网页地址(url)为Value:\n\n```\n<word1, [url3]>\n<word2, [ur1, url2, url3]>\n<word3, [url1]>\n```\n\n这样的话，查询关键词word2，立即能够获取结果: [ur1, url2, url3]。\n\n简单地说，倒排索引就是把Key与Value对调之后的索引，构建倒排索引的目的是提升搜索性能。\n\n## 二. 测试数据\n\n[MongoDB](https://www.mongodb.com/)是文档型数据库，其数据有三个层级: 数据库(database)，集合(collection)和文档(document)，分别对应关系型数据库中的三个层级的: 数据库(database), 表(table)，行(row)。MongDB中每个的文档是一个JSON文件，例如，本文使用的movie集合中的一个文档如下所示:\n\n```\n{\n\t\"_id\" : ObjectId(\"57d02d60b128567fc130287d\"),\n\t\"movie\" : \"Pride & Prejudice\",\n\t\"starList\" : [\n\t\t\"Keira Knightley\",\n\t\t\"Matthew Macfadyen\"\n\t],\n\t\"__v\" : 0\n}\n```\n\n该文档一共有4个属性:\n\n- _id: 文档ID，由MongoDB自动生成。\n- __v: 文档版本，由MongoDB的NodeJS接口Mongoose自动生成。\n- movie: 电影名称。\n- starList: 演员列表。 \n\n可知，这个文档表示电影[《傲慢与偏见》](https://movie.douban.com/subject/1418200/)，由女神[凯拉·奈特莉](https://movie.douban.com/celebrity/1054448/)主演。\n\n忽略_id与__v，movie集合的数据如下:\n\n```\n{\n    \"movie\": \"Pride & Prejudice\",\n    \"starList\": [\"Keira Knightley\", \"Matthew Macfadyen\"]\n},\n{\n    \"movie\": \"Begin Again\",\n    \"starList\": [\"Keira Knightley\", \"Mark Ruffalo\"]\n},\n{\n    \"movie\": \"The Imitation Game\",\n    \"starList\": [\"Keira Knightley\", \"Benedict Cumberbatch\"]\n}\n```\n\n其中Key为电影名称(movie)，而Value为演员列表(starList)。\n\n这时查询Keira Knightley所主演的电影的NodeJS[代码](https://github.com/kiwenlau/mongodb-inverted-index/blob/master/search.js)如下:\n\n```\nMovie.find(\n{\n    starList: \"Keira Knightley\"\n},\n{\n    _id: 0,\n    movie: 1\n}, function(err, results)\n{\n    if (err)\n    {\n        console.log(err);\n        process.exit(1);\n    }\n    console.log(\"search movie success:\\n\");\n    console.log(JSON.stringify(results, null, 4));\n    process.exit(0);\n});\n```\n\n- 注：本文所有代码使用了MongoDB的NodeJS接口[Mongoose](http://mongoosejs.com/)，它与MongoDB Shell的接口基本一致。\n\n代码并不复杂，但是数据量大时查询性能会很差，因为需要:\n\n- 遍历整个movie集合的所有文档\n- 遍历每个文档的startList数组\n\n因此构建倒排索引可以有效地提升搜索性能。本文将介绍MongoDB中两种构建倒排索引的方法：[MapReduce](https://docs.mongodb.com/manual/core/map-reduce/)和[Aggregation Pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/)。\n\n\n## 三 MapReduce\n\n[MapReduce](http://research.google.com/archive/mapreduce.html)是由谷歌提出的编程模型，适用于多种大数据处理场景，在搜索引擎中，MapReduce可以用于构建网页数据的倒排索引，也可以用于编写网页排序算法PageRank(由谷歌创始人佩奇和布林提出)。\n\nMapReduce的输入数据与输出数据均为键值对集合。MapReduce分为两个函数: Map与Reduce。\n\n- Map函数将输入键值`<k1, v1>`对进行变换，输出中间键值对`<k2, v2>`。\n- MapReduce框架会自动对中间键值对`<k2, v2>`进行分组，Key相同的键值对会被合并为一个键值对`<k2, list(v2)>`。\n- Reduce函数对`<k2, list(v2)>`的Value进行合并，生成结果键值对`<k2, v3>`。\n\n使用MapReduce构建倒排索引的NodeJS[代码](https://github.com/kiwenlau/mongodb-inverted-index/blob/master/mapreduce.js)如下:\n\n```\nvar option = {};\n\noption.map = function()\n{\n    var movie = this.movie;\n    this.starList.forEach(function(star)\n    {\n        emit(star,\n        {\n            movieList: [movie]\n        });\n    });\n};\n\noption.reduce = function(key, values)\n{\n    var movieList = [];\n    values.forEach(function(value)\n    {\n        movieList.push(value.movieList[0]);\n    });\n    return {\n        movieList: movieList\n    };\n};\n\nMovie.mapReduce(option, function(err, results)\n{\n    if (err)\n    {\n        console.log(err);\n        process.exit(1);\n    }\n    console.log(\"create inverted index success:\\n\");\n    console.log(JSON.stringify(results, null, 4));\n    process.exit(0);\n});\n```\n\n代码解释:\n\n- Map函数的输入数据是Movie集合中的各个文档，在代码中用this表示。文档的movie与starList属性构成键值对`<movie, starList>`。Map函数遍历starList，对每个start生成键值对`<star, movieList>`。这时Key与Value进行了对调，且starList被拆分了，此时movieList仅包含单个movie。\n- MongoDB的MapReduce执行框架对成键值对`<star, movieList>`进行分组，star相同的键值对会被合并为一个键值对`<star, list(movieList)>`。这一步是自动进行的，因此在代码中并没有体现。\n- Reduce函数的输入数据是键值对`<star, list(movieList)>`，在代码中，star即为key，而list(movieList)即为values，两者为Reduce函数的参数。Reduce函数合并list(movieList)，从而得到键值对`<star, movieList>`，最终，movieList中将包含该star的所有movie。\n\n在代码中，Map函数与Reduce返回的键值对中的Value是一个对象`{ movieList: movieList }`而不是数组`movieList`，因此代码和结果都显得比较奇怪。MongoDB的MapReduce框架不支持Reduce函数返回数组，因此只能将movieList放在对象里面返回。\n\n输出结果:\n\n```\n[\n    {\n        \"_id\": \"Benedict Cumberbatch\",\n        \"value\": {\n            \"movieList\": [\n                \"The Imitation Game\"\n            ]\n        }\n    },\n    {\n        \"_id\": \"Keira Knightley\",\n        \"value\": {\n            \"movieList\": [\n                \"Pride & Prejudice\",\n                \"Begin Again\",\n                \"The Imitation Game\"\n            ]\n        }\n    },\n    {\n        \"_id\": \"Mark Ruffalo\",\n        \"value\": {\n            \"movieList\": [\n                \"Begin Again\"\n            ]\n        }\n    },\n    {\n        \"_id\": \"Matthew Macfadyen\",\n        \"value\": {\n            \"movieList\": [\n                \"Pride & Prejudice\"\n            ]\n        }\n    }\n]\n```\n\n## 四. Aggregation Pipeline\n\n[Aggregation Pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/)，中文称作聚合管道，用于汇总MongoDB中多个文档中的数据，也可以用于构建倒排索引。\n\nAggregation Pipeline有两个特点，可以进行各种[聚合操作](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/)，并且可以将多个聚合操作组合使用，类似于Linux中的管道操作，前一个操作的输出是下一个操作的输入。\n\n使用Aggregation Pipeline构建倒排索引的NodeJS[代码](https://github.com/kiwenlau/mongodb-inverted-index/blob/master/aggregation Pipeline.js)如下:\n\n```\nMovie.aggregate([\n{\n    \"$unwind\": \"$starList\"\n},\n{\n    \"$group\":\n    {\n        \"_id\": \"$starList\",\n        \"movieList\":\n        {\n            \"$push\": \"$movie\"\n        }\n    }\n},\n{\n    \"$project\":\n    {\n        \"_id\": 0,\n        \"star\": \"$_id\",\n        \"movieList\": 1\n    }\n}], function(err, results)\n{\n    if (err)\n    {\n        console.log(err);\n        process.exit(1);\n    }\n    console.log(\"create inverted index success:\\n\");\n    console.log(JSON.stringify(results, null, 4));\n    process.exit(0);\n});\n```\n\n代码解释:\n\n- $unwind: 将starList拆分，输出结果(忽略_id与__v)为:\n\n```\n[\n    {\n        \"movie\": \"Pride & Prejudice\",\n        \"starList\": \"Keira Knightley\"\n    },\n    {\n        \"movie\": \"Pride & Prejudice\",\n        \"starList\": \"Matthew Macfadyen\"\n    },\n    {\n        \"movie\": \"Begin Again\",\n        \"starList\": \"Keira Knightley\"\n    },\n    {\n        \"movie\": \"Begin Again\",\n        \"starList\": \"Mark Ruffalo\"\n    },\n    {\n        \"movie\": \"The Imitation Game\",\n        \"starList\": \"Keira Knightley\"\n    },\n    {\n        \"movie\": \"The Imitation Game\",\n        \"starList\": \"Benedict Cumberbatch\"\n    }\n]\n```\n\n- $group: 根据文档的starList属性进行分组，然后将分组文档的movie属性合并为movieList，输出结果(忽略_id与__v)为:\n\n```\n[\n    {\n        \"_id\": \"Benedict Cumberbatch\",\n        \"movieList\": [\n            \"The Imitation Game\"\n        ]\n    },\n    {\n        \"_id\": \"Matthew Macfadyen\",\n        \"movieList\": [\n            \"Pride & Prejudice\"\n        ]\n    },\n    {\n        \"_id\": \"Mark Ruffalo\",\n        \"movieList\": [\n            \"Begin Again\"\n        ]\n    },\n    {\n        \"_id\": \"Keira Knightley\",\n        \"movieList\": [\n            \"Pride & Prejudice\",\n            \"Begin Again\",\n            \"The Imitation Game\"\n        ]\n    }\n]\n```\n","slug":"2016/09/11/mongodb-inverted-index","published":1,"updated":"2016-09-10T08:09:40.000Z","_id":"cisvxd3tq0000gq8s2krgs2zb","comments":1,"layout":"post","photos":[],"link":""},{"title":"什么是服务发现？","date":"2016-09-04T01:00:00.000Z","_content":"\n**摘要:** 将容器应用部署到集群时，其服务地址，即**IP**和**端口**, 是由集群系统动态分配的。那么，当我们需要访问这个服务时，如何确定它的地址呢？这时，就需要**服务发现(Service Discovery)**了。本文将以Nginx的部署为例，介绍服务发现的原理与实践。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-09-04](http://kiwenlau.com/2016/09/04/what-is-service-discovery/)\n\n<img src=\"what-is-service-discovery/service-discovery.png\" width = \"500\"/>\n\n## 一. 单机部署Nginx\n\n[Nginx](https://nginx.org)作为网页服务器，功能与Apache一致。使用[Docker](https://www.docker.com/), 可以快速部署Nginx。\n\n#### **1. 下载Nginx镜像**\n\n```\nsudo docker pull nginx:1.10\n```\n\n#### **2. 运行Nginx容器**\n\n```\nsudo docker run -d \\\n                -p 8080:80 \\\n                nginx:1.10\n```\n\n其中，**-d**选项表示Nginx容器在后台运行，**-p**选项表示主机的**8080**端口映射为容器的**80**端口。\n\n#### **3. 访问Nginx服务**\n\n使用浏览器服务Nginx\n\n[http://192.168.59.1:8080](http://192.168.59.1:8080)\n\n或者使用curl命令访问Nginx, 其返回结果为html文件。\n\n```\ncurl 192.168.59.1:8080\n```\n\nNginx的服务地址是192.168.59.1:8080，其中，**192.168.59.1**是运行Nginx容器的主机的IP，而**8080**是Nginx提供服务的端口。\n\n可知，将容器应用部署到单个主机上时，服务的IP即为运行容器的主机IP，而服务的端口可以通过**-p**选项手动指定，这时服务地址相当于是静态分配的，因此不存在服务发现的问题。然而，当我们将容器应用部署到多个节点的集群时呢？\n\n## 二. Mesos/Marathon集群中部署Nginx\n\n首先，可以按照[基于Docker搭建多节点Mesos/Marathon](http://kiwenlau.com/2016/07/10/mesos-marathon-platform/)介绍的方法，快速搭建3个节点的Mesos/Marathon集群。部署Nginx时，可以不使用服务发现，也可以使用[Marathon LB](https://github.com/mesosphere/marathon-lb)提供服务发现。通过对比两种方式，可以更好地理解服务发现。\n\n**1. 不使用服务发现**\n\nNginx定义(nginx1.json):\n\n```\n{\n    \"id\": \"nginx1\",\n    \"cpus\": 0.2,\n    \"mem\": 20.0,\n    \"instances\": 1,\n    \"healthChecks\": [{\n        \"path\": \"/\"\n    }],\n    \"container\": {\n        \"type\": \"DOCKER\",\n        \"docker\": {\n            \"image\": \"nginx:1.10\",\n            \"network\": \"BRIDGE\",\n            \"portMappings\": [{ \"containerPort\": 80, \"hostPort\": 0, \"protocol\": \"tcp\" }]\n        }\n    }\n}\n```\n\n其中，**instances**为1，表示仅部署单个Nginx容器; **hostPort**为0，表示Nginx容器绑定的主机端口由Marathon随机分配。\n\n部署Nginx:\n\n```\ncurl -Ss \\\n     -X POST \\\n     -H \"Content-Type: application/json\" \\\n     --data \"@nginx1.json\" \\\n     http://127.0.0.1:8080/v2/apps | python2.7 -mjson.tool\n```\n\n这时，Nginx容器可能运行node2(192.168.59.2)上，也可能运行在node3(192.168.59.3)上，因此Nginx服务的IP是无法事先确定的。而Nginx容器绑定的主机端口由Marathon随机分配，也不确定。\n\n当然，服务端口可以通过**hostPort**指定，但是这样做并不合适，因为有可能会发生端口冲突。当集群中运行了非常多不同的服务时，**静态分配端口是不现实的，也限制了集群的灵活性与扩展性**。\n\n在Slave节点上使用**docker ps**命令可以获取Nginx服务的IP与端口。\n\nnode2(192.168.59.2)\n\n```\nsudo docker ps | grep nginx\nb863d407b880        nginx:1.10              \"nginx -g 'daemon off\"   15 minutes ago      Up 15 minutes       443/tcp, 0.0.0.0:31575->80/tcp   mesos-d34d0b5b-c3b1-4020-9bb2-bb8582252bf3-S0.d2de6d05-9751-4fbe-af10-d7e35e9e6c7b\n```\n\nnode3(192.168.59.3)\n\n```\nsudo docker ps | grep nginx\n```\n\n可知Nginx服务的IP与端口分别为**192.168.59.2**和**31575**，即Nginx的服务地址为：[http://192.168.59.2:31575](http://192.168.59.2:31575)\n\n每次重新部署Nginx时，其IP和端口会发生变化，这就意味着每次都要手动去查询服务地址，这很不方便，且无法将部署任务自动化。在容器集群中，通常需要运行非常多不同的应用，这就意味着**服务发现**是容器集群系统的必备功能。\n\n\n**2. 使用[Marathon LB](https://github.com/mesosphere/marathon-lb)提供服务发现**\n\n[Marathon LB](https://github.com/mesosphere/marathon-lb)是Marathon的**服务发现**系统。Marathon LB通过使用[Haproxy](http://www.haproxy.org/)实现了代理服务器的功能。\n\n通过使用Marathon LB可以配置服务的固定端口，而服务的IP就是运行Marathon LB的节点IP。Marathon LB会监听Marathon的调度事件，获取容器实际运行的IP与端口，然后更新Haproxy的配置文件。因此，当重新部署Nginx时，我们仍然可以通过固定的IP与端口访问该服务。\n\nNginx定义(nginx2.json):\n\n```\n{\n    \"id\": \"nginx2\",\n    \"labels\": {\n        \"HAPROXY_GROUP\": \"external\"\n    },\n    \"cpus\": 0.2,\n    \"mem\": 20.0,\n    \"instances\": 1,\n    \"healthChecks\": [{\n        \"path\": \"/\"\n    }],\n    \"container\": {\n        \"type\": \"DOCKER\",\n        \"docker\": {\n            \"image\": \"nginx:1.10\",\n            \"network\": \"BRIDGE\",\n            \"portMappings\": [{ \"containerPort\": 80, \"hostPort\": 0, \"servicePort\": 10000, \"protocol\": \"tcp\" }]\n        }\n    }\n}\n```\n\n其中，nginx2.json只有**HAPROXY_GROUP**与**servicePort**两处修改。**HAPROXY_GROUP**为external，表示Nginx将使用分组为external的**Marathon LB**做服务发现。**servicePort**为10000，表示Nginx将使用**Marathon LB**节点的10000端口提供服务。\n\n部署Nginx:\n\n```\ncurl -Ss \\\n     -X POST \\\n     -H \"Content-Type: application/json\" \\\n     --data \"@nginx2.json\" \\\n     http://127.0.0.1:8080/v2/apps | python2.7 -mjson.tool\n```\n\n这时，Nginx服务的IP为运行Marathon LB的节点IP，即**192.168.59.1**，而Nginx服务的端口为**servicePort**指定的端口，即10000。因此，Nginx的服务地址为：[http://192.168.59.1:10000](http://192.168.59.1:10000)。而Nginx容器的实际地址为[http://192.168.59.2:31270](http://192.168.59.2:31270)，Marathon LB使用Haproxy作为代理服务器转发的服务请求。Marathon LB会监听Marathon的调度事件，获取容器实际运行的IP与端口，然后更新Haproxy的配置文件。下面即为Marathon LB自动生成的Haproxy配置文件: \n\n```\nfrontend nginx2_10000\n  bind *:10000\n  mode http\n  use_backend nginx2_10000\n\nbackend nginx2_10000\n  balance roundrobin\n  mode http\n  option forwardfor\n  http-request set-header X-Forwarded-Port %[dst_port]\n  http-request add-header X-Forwarded-Proto https if { ssl_fc }\n  option  httpchk GET /\n  timeout check 20s\n  server 192_168_59_2_31270 192.168.59.2:31270 check inter 60s fall 4\n```\n\n可知Haproxy中，niginx服务的前端(frontend)地址为: *:10000，而后端(backend)地址为: 192.168.59.2:31270。Haproxy负责将服务请求转发到Nginx容器。\n\n当我们重新部署Nginx时，Nginx容器的IP和端口会发生变化，Marathon LB会更新Haproxy的配置文件，因此我们仍然可以通过[http://192.168.59.1:10000](http://192.168.59.1:10000)访问服务。\n\n因此，Marathon LB的任务就是发现服务的地址(IP和端口)，然后用户就不用每次手动查询了。[bamboo](https://github.com/QubitProducts/bamboo)与[nixy](https://github.com/martensson/nixy)实现了同样的功能。\n\n## 三. 参考\n\n1. [Understanding Modern Service Discovery with Docker](http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/)\n2. [基于Docker技术构建PaaS云平台](http://www.eurekao.com/build-paas-platform-base-on-docker/)\n3. [Youzan 服务发现概述](http://tech.youzan.com/haunt-youzan-service-discovery/)\n\n\n","source":"_posts/2016/09/04/what-is-service-discovery.md","raw":"title: 什么是服务发现？\n\ndate: 2016-09-04 10:00\n\ntags: [服务发现]\n\n---\n\n**摘要:** 将容器应用部署到集群时，其服务地址，即**IP**和**端口**, 是由集群系统动态分配的。那么，当我们需要访问这个服务时，如何确定它的地址呢？这时，就需要**服务发现(Service Discovery)**了。本文将以Nginx的部署为例，介绍服务发现的原理与实践。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-09-04](http://kiwenlau.com/2016/09/04/what-is-service-discovery/)\n\n<img src=\"what-is-service-discovery/service-discovery.png\" width = \"500\"/>\n\n## 一. 单机部署Nginx\n\n[Nginx](https://nginx.org)作为网页服务器，功能与Apache一致。使用[Docker](https://www.docker.com/), 可以快速部署Nginx。\n\n#### **1. 下载Nginx镜像**\n\n```\nsudo docker pull nginx:1.10\n```\n\n#### **2. 运行Nginx容器**\n\n```\nsudo docker run -d \\\n                -p 8080:80 \\\n                nginx:1.10\n```\n\n其中，**-d**选项表示Nginx容器在后台运行，**-p**选项表示主机的**8080**端口映射为容器的**80**端口。\n\n#### **3. 访问Nginx服务**\n\n使用浏览器服务Nginx\n\n[http://192.168.59.1:8080](http://192.168.59.1:8080)\n\n或者使用curl命令访问Nginx, 其返回结果为html文件。\n\n```\ncurl 192.168.59.1:8080\n```\n\nNginx的服务地址是192.168.59.1:8080，其中，**192.168.59.1**是运行Nginx容器的主机的IP，而**8080**是Nginx提供服务的端口。\n\n可知，将容器应用部署到单个主机上时，服务的IP即为运行容器的主机IP，而服务的端口可以通过**-p**选项手动指定，这时服务地址相当于是静态分配的，因此不存在服务发现的问题。然而，当我们将容器应用部署到多个节点的集群时呢？\n\n## 二. Mesos/Marathon集群中部署Nginx\n\n首先，可以按照[基于Docker搭建多节点Mesos/Marathon](http://kiwenlau.com/2016/07/10/mesos-marathon-platform/)介绍的方法，快速搭建3个节点的Mesos/Marathon集群。部署Nginx时，可以不使用服务发现，也可以使用[Marathon LB](https://github.com/mesosphere/marathon-lb)提供服务发现。通过对比两种方式，可以更好地理解服务发现。\n\n**1. 不使用服务发现**\n\nNginx定义(nginx1.json):\n\n```\n{\n    \"id\": \"nginx1\",\n    \"cpus\": 0.2,\n    \"mem\": 20.0,\n    \"instances\": 1,\n    \"healthChecks\": [{\n        \"path\": \"/\"\n    }],\n    \"container\": {\n        \"type\": \"DOCKER\",\n        \"docker\": {\n            \"image\": \"nginx:1.10\",\n            \"network\": \"BRIDGE\",\n            \"portMappings\": [{ \"containerPort\": 80, \"hostPort\": 0, \"protocol\": \"tcp\" }]\n        }\n    }\n}\n```\n\n其中，**instances**为1，表示仅部署单个Nginx容器; **hostPort**为0，表示Nginx容器绑定的主机端口由Marathon随机分配。\n\n部署Nginx:\n\n```\ncurl -Ss \\\n     -X POST \\\n     -H \"Content-Type: application/json\" \\\n     --data \"@nginx1.json\" \\\n     http://127.0.0.1:8080/v2/apps | python2.7 -mjson.tool\n```\n\n这时，Nginx容器可能运行node2(192.168.59.2)上，也可能运行在node3(192.168.59.3)上，因此Nginx服务的IP是无法事先确定的。而Nginx容器绑定的主机端口由Marathon随机分配，也不确定。\n\n当然，服务端口可以通过**hostPort**指定，但是这样做并不合适，因为有可能会发生端口冲突。当集群中运行了非常多不同的服务时，**静态分配端口是不现实的，也限制了集群的灵活性与扩展性**。\n\n在Slave节点上使用**docker ps**命令可以获取Nginx服务的IP与端口。\n\nnode2(192.168.59.2)\n\n```\nsudo docker ps | grep nginx\nb863d407b880        nginx:1.10              \"nginx -g 'daemon off\"   15 minutes ago      Up 15 minutes       443/tcp, 0.0.0.0:31575->80/tcp   mesos-d34d0b5b-c3b1-4020-9bb2-bb8582252bf3-S0.d2de6d05-9751-4fbe-af10-d7e35e9e6c7b\n```\n\nnode3(192.168.59.3)\n\n```\nsudo docker ps | grep nginx\n```\n\n可知Nginx服务的IP与端口分别为**192.168.59.2**和**31575**，即Nginx的服务地址为：[http://192.168.59.2:31575](http://192.168.59.2:31575)\n\n每次重新部署Nginx时，其IP和端口会发生变化，这就意味着每次都要手动去查询服务地址，这很不方便，且无法将部署任务自动化。在容器集群中，通常需要运行非常多不同的应用，这就意味着**服务发现**是容器集群系统的必备功能。\n\n\n**2. 使用[Marathon LB](https://github.com/mesosphere/marathon-lb)提供服务发现**\n\n[Marathon LB](https://github.com/mesosphere/marathon-lb)是Marathon的**服务发现**系统。Marathon LB通过使用[Haproxy](http://www.haproxy.org/)实现了代理服务器的功能。\n\n通过使用Marathon LB可以配置服务的固定端口，而服务的IP就是运行Marathon LB的节点IP。Marathon LB会监听Marathon的调度事件，获取容器实际运行的IP与端口，然后更新Haproxy的配置文件。因此，当重新部署Nginx时，我们仍然可以通过固定的IP与端口访问该服务。\n\nNginx定义(nginx2.json):\n\n```\n{\n    \"id\": \"nginx2\",\n    \"labels\": {\n        \"HAPROXY_GROUP\": \"external\"\n    },\n    \"cpus\": 0.2,\n    \"mem\": 20.0,\n    \"instances\": 1,\n    \"healthChecks\": [{\n        \"path\": \"/\"\n    }],\n    \"container\": {\n        \"type\": \"DOCKER\",\n        \"docker\": {\n            \"image\": \"nginx:1.10\",\n            \"network\": \"BRIDGE\",\n            \"portMappings\": [{ \"containerPort\": 80, \"hostPort\": 0, \"servicePort\": 10000, \"protocol\": \"tcp\" }]\n        }\n    }\n}\n```\n\n其中，nginx2.json只有**HAPROXY_GROUP**与**servicePort**两处修改。**HAPROXY_GROUP**为external，表示Nginx将使用分组为external的**Marathon LB**做服务发现。**servicePort**为10000，表示Nginx将使用**Marathon LB**节点的10000端口提供服务。\n\n部署Nginx:\n\n```\ncurl -Ss \\\n     -X POST \\\n     -H \"Content-Type: application/json\" \\\n     --data \"@nginx2.json\" \\\n     http://127.0.0.1:8080/v2/apps | python2.7 -mjson.tool\n```\n\n这时，Nginx服务的IP为运行Marathon LB的节点IP，即**192.168.59.1**，而Nginx服务的端口为**servicePort**指定的端口，即10000。因此，Nginx的服务地址为：[http://192.168.59.1:10000](http://192.168.59.1:10000)。而Nginx容器的实际地址为[http://192.168.59.2:31270](http://192.168.59.2:31270)，Marathon LB使用Haproxy作为代理服务器转发的服务请求。Marathon LB会监听Marathon的调度事件，获取容器实际运行的IP与端口，然后更新Haproxy的配置文件。下面即为Marathon LB自动生成的Haproxy配置文件: \n\n```\nfrontend nginx2_10000\n  bind *:10000\n  mode http\n  use_backend nginx2_10000\n\nbackend nginx2_10000\n  balance roundrobin\n  mode http\n  option forwardfor\n  http-request set-header X-Forwarded-Port %[dst_port]\n  http-request add-header X-Forwarded-Proto https if { ssl_fc }\n  option  httpchk GET /\n  timeout check 20s\n  server 192_168_59_2_31270 192.168.59.2:31270 check inter 60s fall 4\n```\n\n可知Haproxy中，niginx服务的前端(frontend)地址为: *:10000，而后端(backend)地址为: 192.168.59.2:31270。Haproxy负责将服务请求转发到Nginx容器。\n\n当我们重新部署Nginx时，Nginx容器的IP和端口会发生变化，Marathon LB会更新Haproxy的配置文件，因此我们仍然可以通过[http://192.168.59.1:10000](http://192.168.59.1:10000)访问服务。\n\n因此，Marathon LB的任务就是发现服务的地址(IP和端口)，然后用户就不用每次手动查询了。[bamboo](https://github.com/QubitProducts/bamboo)与[nixy](https://github.com/martensson/nixy)实现了同样的功能。\n\n## 三. 参考\n\n1. [Understanding Modern Service Discovery with Docker](http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/)\n2. [基于Docker技术构建PaaS云平台](http://www.eurekao.com/build-paas-platform-base-on-docker/)\n3. [Youzan 服务发现概述](http://tech.youzan.com/haunt-youzan-service-discovery/)\n\n\n","slug":"2016/09/04/what-is-service-discovery","published":1,"updated":"2016-09-04T14:59:02.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3u60003gq8shrb9zkqg"},{"title":"基于Docker搭建多节点Mesos/Marathon","date":"2016-07-10T01:00:00.000Z","_content":"\n**摘要:** 在之前的一篇博客中，我介绍了[基于Docker搭建单机版Mesos/Marathon](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)，但是仅仅使用了单个节点。而在这篇博客中，我将介绍[基于Docker搭建多节点Mesos/Marathon](http://kiwenlau.com/2016/07/10/mesos-marathon-platform/)，开发者可以使用3个节点快速地搭建一个真正的分布式容器集群系统。**服务发现**和**负载均衡**是容器集群必不可少的功能，我选择了[Marathon LB](https://github.com/mesosphere/marathon-lb)来实现。\n\n**GitHub地址:** [kiwenlau/mesos-marathon-platform](https://github.com/kiwenlau/mesos-marathon-platform)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-07-10](http://kiwenlau.com/2016/07/10/mesos-marathon-platform/)\n\n<img src=\"mesos-marathon-platform/mesos-marathon-platform.png\" width = \"500\"/>\n\n## 一. Mesos/Marathon简介\n\n#### **1. Mesos**\n\n[Mesos](http://mesos.apache.org/)是**分布式集群资源管理系统**，负责调度集群内的CPU，内存以及磁盘等资源。Hadoop MapReduce, Spark以及Storm等**分布式计算框架**很流行，但是为每一个计算框架搭建单独的集群非常地浪费资源，也无法实现数据共享，而Mesos的设计初衷就是让不同的**分布式计算框架**能够共享一个集群。\n\nMesos资源调度算法分为两个层次: Mesos监控集群的空余资源，并将空余资源按照一定规则分配给各个计算框架；而各个计算框架会根据需要选择接受或者拒绝所分配的资源。这时，Mesos与各个计算框架都参与了资源的调度: Mesos负责分配资源; 而计算框架接受或者拒绝资源。因此，Mesos的责任非常清晰而且简单：分配集群资源。Mesos的双层调度算法提高了可扩展性，并且可以更方便地支持不同的计算框架。\n\n目前主流的集群资源管理系统还有Hadoop YARN，Kubernetes以及Swarm。Hadoop YARN目前仅适合运行分布式计算框架例如Spark；Kubernetes与Swarm仅适合运行容器应用；而Mesos对分布式计算框架以及容器应用的支持都很成熟。并且，Kubernetes与Swarm可以作为计算框架运行在Mesos之上。Kubernetes的功能强大，但是有些过度设计导致复杂度很高，而Swarm的设计简单很多但是功能相对缺乏，大家可以根据需要选择。个人认为，**使用Mesos的话，最好选择Marathon作为容器编排系统**，架构非常简单，且功能丰富。\n\n#### **2. Marathon**\n\n[Marathon](https://mesosphere.github.io/marathon/)是容器编排系统，是运行于Mesos之上的众多计算框架之一。用户可以通过Marathon提交，监控并调度容器应用，然后Mesos负责运行容器。另外，[Aurora](http://aurora.apache.org/)与Marathon功能一致，同为容器编排系统。相比而言，Marathon的架构比Aurora更简单，没有从节点，且对[Docker](https://www.docker.com/)的支持更为完善。Marathon由Mesosphere公司负责开发，社区很活跃，文档也很完善。\n\nMarathon具有**容错功能**：当容器由于节点崩溃等原因意外停止运行时，Marathon会自动将容器调度到其他节点。这一点类似进程管理工具例如[Supervisor](http://supervisord.org/)：当进程意外退出时，Supervisor会重启进程。然而，自动容错功能并不适合有状态服务，即带有数据卷(volume)的容器，例如MongoDB与MySQL。因为数据很难跨节点移动，目前的技术还不够成熟。因此，**Marathon目前仅适合运行无状态的服务，而数据库等有状态服务应该单独部署**。这样做也可以提高数据的安全性。\n\n#### **3. Marathon LB**\n\n[Marathon LB](https://github.com/mesosphere/marathon-lb)是Marathon的**服务发现**与**负载均衡**系统。Marathon LB通过使用[Haproxy](http://www.haproxy.org/)实现了代理服务器的功能。\n\n当使用Marathon部署容器时，容器运行的节点(IP)与使用的端口(PORT)是Mesos/Marathon平台负责调度的，无法事先确定。这样的话，每次访问服务时，需要手动去查询容器运行的IP与PORT。并且，容器出错时会发生重新调度，IP与PORT会变化。因此，访问服务会非常不方便。通过使用Marathon LB可以配置服务的固定端口，而服务的IP就是运行Marathon LB的节点IP，这样每次部署服务时，IP与PORT是固定的，就方便很多了。Marathon LB会监听Marathon的调度事件，获取容器运行的IP与PORT，然后更新代理服务器Haproxy的配置文件。因此，当部署新的容器或者容器发生变化时，仍然可以通过固定的IP与PORT访问该容器。这就是所谓的**服务发现**。\n\n同一个服务往往对应着多个容器副本，Marathon LB作为代理服务器，同时实现了**负载均衡**的功能。服务请求能够使用Round Robin方式发送给各个容器。\n                           \n## 二. 搭建步骤\n\n#### **1. 创建虚拟机**\n\n按照[使用Vagrant创建多节点虚拟机集群](http://kiwenlau.com/2016/07/03/vagrant-vm-cluster/)，可以快速地在单个机器上创建Mesos/Marathon平台运行所需要的3个虚拟机节点。\n\n其中，node1为主机点(Master)，运行zookeeper, mesos_master, marathon以及marathon-lb容器; node2与node3为从节点(Slave)，运行mesos_slave容器；使用marathon部署nginx时，nginx容器运行在从节点上。如下表所示:\n\n| 节点    | IP           | 运行的容器                                      |\n|:------- |:-------------| :----------------------------------------------|\n| node1  | 192.168.59.1 | zookeeper, mesos_master, marathon, marathon-lb |\n| node2  | 192.168.59.2 | mesos_slave1, nginx                                   |\n| node3  | 192.168.59.3 | mesos_slave2, nginx                                   |\n\n#### **2. 开启Docker daemon的监听端口**\n\n因为**start-containers.sh**使用了远程启动容器的方式，因此需要开启Docker daemon的TCP监听端口。提醒一下，**开启Docker daemon端口是不安全的**，生产环境中不能打开，或者做好防火墙配置。不希望开启Docker daemon的监听端口的话，可以使用start-containers.sh中的命令直接运行容器，只是稍微麻烦一点。\n\nMaster, Slave1和Slave2:\n\n```\nsudo vim /etc/default/docker\n```\n\n修改**DOCKER_OPTS**\n\n```\nDOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\"\n```\n\n重启Docker\n\n```\nsudo restart docker\n```\n\n#### **3. 下载Docker镜像** \n\nMaster:\n\n```\nsudo docker pull kiwenlau/zookeeper:3.4.8 \nsudo docker pull kiwenlau/mesos:0.26.0 \nsudo docker pull kiwenlau/marathon:1.1.1  \nsudo docker pull kiwenlau/marathonlb:1.3.0\n```\n\nSlave1 和 Slave2:\n\n```\nsudo docker pull kiwenlau/mesos:0.26.0\n```\n\n#### **4. 下载GitHub仓库** \n\nMaster:\n\n```\ngit clone https://github.com/kiwenlau/mesos-marathon-platform\n```\n\n\n#### **5. 运行容器** \n\nMaster:\n\n```\ncd mesos-marathon-platform\nsudo ./start-containers.sh\n```\n\n网页管理:\n\n- Mesos: [http://192.168.59.1:5050/](http://192.168.59.1:5050/)\n- Marathon: [http://192.168.59.1:8080/](http://192.168.59.1:8080/)  \n\n\n如果需要增加Slave节点，或者配置不同的节点IP，仅需修改start-contaniers.sh脚本中以下内容：\n\n```\nMASTER_IP=192.168.59.1\nSLAVE_IP=(192.168.59.2 192.168.59.3)\n```\n\n#### **6. 运行Nginx:** \n\n下载nginx镜像(Slave1和Slave2):\n\n```\nsudo docker pull nginx:1.10\n```\n\n运行Nginx(Master):\n\n```\nsudo ./run-nginx.sh \n```\n\nNginx的的定义如下(nginx.json):\n\n```\n{\n    \"id\": \"nginx\",\n    \"labels\": {\n        \"HAPROXY_GROUP\": \"external\"\n    },\n    \"cpus\": 0.2,\n    \"mem\": 20.0,\n    \"instances\": 2,\n    \"healthChecks\": [{\n        \"path\": \"/\"\n    }],\n    \"container\": {\n        \"type\": \"DOCKER\",\n        \"docker\": {\n            \"image\": \"nginx:1.10\",\n            \"network\": \"BRIDGE\",\n            \"portMappings\": [{ \"containerPort\": 80, \"hostPort\": 0, \"servicePort\": 10000, \"protocol\": \"tcp\" }]\n        }\n    }\n}\n```\n\n其中，servicePort表示nginx绑定的端口为**10000**，而marathon-lb容器运行的节点IP为**192.168.59.1**。因此nginx的访问地址为：\n\n- [http://192.168.59.1:10000/](http://192.168.59.1:10000/)\n\n而实际上，两个nginx容器运行在Slave节点上，且端口是随机分配的。\n\n\n## 三. 参考\n\n1. [Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center](http://mesos.berkeley.edu/mesos_tech_report.pdf)\n","source":"_posts/2016/07/10/mesos-marathon-platform.md","raw":"title: 基于Docker搭建多节点Mesos/Marathon\n\ndate: 2016-07-10 10:00\n\ntags: [Docker, Mesos, Marathon]\n\n---\n\n**摘要:** 在之前的一篇博客中，我介绍了[基于Docker搭建单机版Mesos/Marathon](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)，但是仅仅使用了单个节点。而在这篇博客中，我将介绍[基于Docker搭建多节点Mesos/Marathon](http://kiwenlau.com/2016/07/10/mesos-marathon-platform/)，开发者可以使用3个节点快速地搭建一个真正的分布式容器集群系统。**服务发现**和**负载均衡**是容器集群必不可少的功能，我选择了[Marathon LB](https://github.com/mesosphere/marathon-lb)来实现。\n\n**GitHub地址:** [kiwenlau/mesos-marathon-platform](https://github.com/kiwenlau/mesos-marathon-platform)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-07-10](http://kiwenlau.com/2016/07/10/mesos-marathon-platform/)\n\n<img src=\"mesos-marathon-platform/mesos-marathon-platform.png\" width = \"500\"/>\n\n## 一. Mesos/Marathon简介\n\n#### **1. Mesos**\n\n[Mesos](http://mesos.apache.org/)是**分布式集群资源管理系统**，负责调度集群内的CPU，内存以及磁盘等资源。Hadoop MapReduce, Spark以及Storm等**分布式计算框架**很流行，但是为每一个计算框架搭建单独的集群非常地浪费资源，也无法实现数据共享，而Mesos的设计初衷就是让不同的**分布式计算框架**能够共享一个集群。\n\nMesos资源调度算法分为两个层次: Mesos监控集群的空余资源，并将空余资源按照一定规则分配给各个计算框架；而各个计算框架会根据需要选择接受或者拒绝所分配的资源。这时，Mesos与各个计算框架都参与了资源的调度: Mesos负责分配资源; 而计算框架接受或者拒绝资源。因此，Mesos的责任非常清晰而且简单：分配集群资源。Mesos的双层调度算法提高了可扩展性，并且可以更方便地支持不同的计算框架。\n\n目前主流的集群资源管理系统还有Hadoop YARN，Kubernetes以及Swarm。Hadoop YARN目前仅适合运行分布式计算框架例如Spark；Kubernetes与Swarm仅适合运行容器应用；而Mesos对分布式计算框架以及容器应用的支持都很成熟。并且，Kubernetes与Swarm可以作为计算框架运行在Mesos之上。Kubernetes的功能强大，但是有些过度设计导致复杂度很高，而Swarm的设计简单很多但是功能相对缺乏，大家可以根据需要选择。个人认为，**使用Mesos的话，最好选择Marathon作为容器编排系统**，架构非常简单，且功能丰富。\n\n#### **2. Marathon**\n\n[Marathon](https://mesosphere.github.io/marathon/)是容器编排系统，是运行于Mesos之上的众多计算框架之一。用户可以通过Marathon提交，监控并调度容器应用，然后Mesos负责运行容器。另外，[Aurora](http://aurora.apache.org/)与Marathon功能一致，同为容器编排系统。相比而言，Marathon的架构比Aurora更简单，没有从节点，且对[Docker](https://www.docker.com/)的支持更为完善。Marathon由Mesosphere公司负责开发，社区很活跃，文档也很完善。\n\nMarathon具有**容错功能**：当容器由于节点崩溃等原因意外停止运行时，Marathon会自动将容器调度到其他节点。这一点类似进程管理工具例如[Supervisor](http://supervisord.org/)：当进程意外退出时，Supervisor会重启进程。然而，自动容错功能并不适合有状态服务，即带有数据卷(volume)的容器，例如MongoDB与MySQL。因为数据很难跨节点移动，目前的技术还不够成熟。因此，**Marathon目前仅适合运行无状态的服务，而数据库等有状态服务应该单独部署**。这样做也可以提高数据的安全性。\n\n#### **3. Marathon LB**\n\n[Marathon LB](https://github.com/mesosphere/marathon-lb)是Marathon的**服务发现**与**负载均衡**系统。Marathon LB通过使用[Haproxy](http://www.haproxy.org/)实现了代理服务器的功能。\n\n当使用Marathon部署容器时，容器运行的节点(IP)与使用的端口(PORT)是Mesos/Marathon平台负责调度的，无法事先确定。这样的话，每次访问服务时，需要手动去查询容器运行的IP与PORT。并且，容器出错时会发生重新调度，IP与PORT会变化。因此，访问服务会非常不方便。通过使用Marathon LB可以配置服务的固定端口，而服务的IP就是运行Marathon LB的节点IP，这样每次部署服务时，IP与PORT是固定的，就方便很多了。Marathon LB会监听Marathon的调度事件，获取容器运行的IP与PORT，然后更新代理服务器Haproxy的配置文件。因此，当部署新的容器或者容器发生变化时，仍然可以通过固定的IP与PORT访问该容器。这就是所谓的**服务发现**。\n\n同一个服务往往对应着多个容器副本，Marathon LB作为代理服务器，同时实现了**负载均衡**的功能。服务请求能够使用Round Robin方式发送给各个容器。\n                           \n## 二. 搭建步骤\n\n#### **1. 创建虚拟机**\n\n按照[使用Vagrant创建多节点虚拟机集群](http://kiwenlau.com/2016/07/03/vagrant-vm-cluster/)，可以快速地在单个机器上创建Mesos/Marathon平台运行所需要的3个虚拟机节点。\n\n其中，node1为主机点(Master)，运行zookeeper, mesos_master, marathon以及marathon-lb容器; node2与node3为从节点(Slave)，运行mesos_slave容器；使用marathon部署nginx时，nginx容器运行在从节点上。如下表所示:\n\n| 节点    | IP           | 运行的容器                                      |\n|:------- |:-------------| :----------------------------------------------|\n| node1  | 192.168.59.1 | zookeeper, mesos_master, marathon, marathon-lb |\n| node2  | 192.168.59.2 | mesos_slave1, nginx                                   |\n| node3  | 192.168.59.3 | mesos_slave2, nginx                                   |\n\n#### **2. 开启Docker daemon的监听端口**\n\n因为**start-containers.sh**使用了远程启动容器的方式，因此需要开启Docker daemon的TCP监听端口。提醒一下，**开启Docker daemon端口是不安全的**，生产环境中不能打开，或者做好防火墙配置。不希望开启Docker daemon的监听端口的话，可以使用start-containers.sh中的命令直接运行容器，只是稍微麻烦一点。\n\nMaster, Slave1和Slave2:\n\n```\nsudo vim /etc/default/docker\n```\n\n修改**DOCKER_OPTS**\n\n```\nDOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\"\n```\n\n重启Docker\n\n```\nsudo restart docker\n```\n\n#### **3. 下载Docker镜像** \n\nMaster:\n\n```\nsudo docker pull kiwenlau/zookeeper:3.4.8 \nsudo docker pull kiwenlau/mesos:0.26.0 \nsudo docker pull kiwenlau/marathon:1.1.1  \nsudo docker pull kiwenlau/marathonlb:1.3.0\n```\n\nSlave1 和 Slave2:\n\n```\nsudo docker pull kiwenlau/mesos:0.26.0\n```\n\n#### **4. 下载GitHub仓库** \n\nMaster:\n\n```\ngit clone https://github.com/kiwenlau/mesos-marathon-platform\n```\n\n\n#### **5. 运行容器** \n\nMaster:\n\n```\ncd mesos-marathon-platform\nsudo ./start-containers.sh\n```\n\n网页管理:\n\n- Mesos: [http://192.168.59.1:5050/](http://192.168.59.1:5050/)\n- Marathon: [http://192.168.59.1:8080/](http://192.168.59.1:8080/)  \n\n\n如果需要增加Slave节点，或者配置不同的节点IP，仅需修改start-contaniers.sh脚本中以下内容：\n\n```\nMASTER_IP=192.168.59.1\nSLAVE_IP=(192.168.59.2 192.168.59.3)\n```\n\n#### **6. 运行Nginx:** \n\n下载nginx镜像(Slave1和Slave2):\n\n```\nsudo docker pull nginx:1.10\n```\n\n运行Nginx(Master):\n\n```\nsudo ./run-nginx.sh \n```\n\nNginx的的定义如下(nginx.json):\n\n```\n{\n    \"id\": \"nginx\",\n    \"labels\": {\n        \"HAPROXY_GROUP\": \"external\"\n    },\n    \"cpus\": 0.2,\n    \"mem\": 20.0,\n    \"instances\": 2,\n    \"healthChecks\": [{\n        \"path\": \"/\"\n    }],\n    \"container\": {\n        \"type\": \"DOCKER\",\n        \"docker\": {\n            \"image\": \"nginx:1.10\",\n            \"network\": \"BRIDGE\",\n            \"portMappings\": [{ \"containerPort\": 80, \"hostPort\": 0, \"servicePort\": 10000, \"protocol\": \"tcp\" }]\n        }\n    }\n}\n```\n\n其中，servicePort表示nginx绑定的端口为**10000**，而marathon-lb容器运行的节点IP为**192.168.59.1**。因此nginx的访问地址为：\n\n- [http://192.168.59.1:10000/](http://192.168.59.1:10000/)\n\n而实际上，两个nginx容器运行在Slave节点上，且端口是随机分配的。\n\n\n## 三. 参考\n\n1. [Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center](http://mesos.berkeley.edu/mesos_tech_report.pdf)\n","slug":"2016/07/10/mesos-marathon-platform","published":1,"updated":"2016-07-10T15:30:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3ug0006gq8s4hqbd3l8"},{"title":"使用Vagrant创建多节点虚拟机集群","date":"2016-07-03T01:00:00.000Z","_content":"\n**摘要:** 在前一篇博客中，我介绍了[使用Vagrant快速创建虚拟机](http://kiwenlau.com/2016/06/19/160619-vagrant-virtual-machine/)，但是所创建的只是单个虚拟机。这篇博客将介绍[使用Vagrant创建多节点虚拟机集群](http://kiwenlau.com/2016/07/03/vagrant-vm-cluster/)，可以作为[Hadoop](http://hadoop.apache.org/)，[Spark](http://spark.apache.org/)以及[Storm](http://storm.apache.org/)等分布式系统的运行环境。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-07-03](http://kiwenlau.com/2016/07/03/vagrant-vm-cluster/)\n\n<img src=\"vagrant-vm-cluster/vagrant-vm-cluster.png\" width = \"500\"/>\n\n本文所有操作是在MacBook上进行的，Windows上的操作大部分一致，但是可能会有一些小问题。\n\n## 一. 集群创建\n\n#### **1. 安装[VirtualBox](https://www.virtualbox.org/wiki/Downloads)**\n\n#### **2. 安装[Vagrant](https://www.vagrantup.com/downloads.html)**\n\n#### **3. 下载Box**\n\n```\nvagrant box add ubuntu/trusty64\n```\n\n**Box**相当于虚拟机所依赖的镜像文件。\n\n#### **4. 编辑Vagrantfile**\n\n```\nmkdir vagrant-cluster\ncd vagrant-cluster\nvim Vagrantfile\n```\n\nVagrantfile如下，可以通过注释理解每个自定义配置的含义：\n\n```\nVagrant.configure(\"2\") do |config|\n\n\t(1..3).each do |i|\n\n\t\tconfig.vm.define \"node#{i}\" do |node|\n\n\t\t# 设置虚拟机的Box\n\t\tnode.vm.box = \"ubuntu/trusty64\"\n\n\t\t# 设置虚拟机的主机名\n\t\tnode.vm.hostname=\"node#{i}\"\n\n\t\t# 设置虚拟机的IP\n\t\tnode.vm.network \"private_network\", ip: \"192.168.59.#{i}\"\n\n\t\t# 设置主机与虚拟机的共享目录\n\t\tnode.vm.synced_folder \"~/Desktop/share\", \"/home/vagrant/share\"\n\n\t\t# VirtaulBox相关配置\n\t\tnode.vm.provider \"virtualbox\" do |v|\n\n\t\t\t# 设置虚拟机的名称\n\t\t\tv.name = \"node#{i}\"\n\n\t\t\t# 设置虚拟机的内存大小  \n\t\t\tv.memory = 2048\n\n\t\t\t# 设置虚拟机的CPU个数\n\t\t\tv.cpus = 1\n\t\tend\n  \n\t\t# 使用shell脚本进行软件安装和配置\n\t\tnode.vm.provision \"shell\", inline: <<-SHELL\n\n\t\t\t# 安装docker 1.11.0\n\t\t\twget -qO- https://get.docker.com/ | sed 's/docker-engine/docker-engine=1.11.0-0~trusty/' | sh\n\t\t\tusermod -aG docker vagrant\n\t\t\t\n\t\tSHELL\n\n\t\tend\n\tend\nend\n```\n\n与创建单个虚拟机相比，创建多个虚拟机时多了一层循环，而变量i可以用于设置节点的名称与IP，使用**#{i}**取值：\n\n```\n(1..3).each do |i|\n\nend\n```\n\n可知，一共创建了3个虚拟机。\n\n#### **5. 在桌面上创建share目录**\n\n桌面上的share目录将与虚拟机内的/home/vagrant/share目录内容实时同步\n\n```\nmkdir ~/Desktop/share\n```\n\n#### **6. 创建虚拟机**\n\n```\nvagrant up\n```\n\n创建3个虚拟机大概需要15分钟，当然这和机器性能还有网速相关。安装Docker可能会比较慢，不需要的话删除下面几行就可以了：\n\n```\n# 使用shell脚本进行软件安装和配置\nnode.vm.provision \"shell\", inline: <<-SHELL\n\n\t# 安装docker 1.11.0\n\twget -qO- https://get.docker.com/ | sed 's/docker-engine/docker-engine=1.11.0-0~trusty/' | sh\n\tusermod -aG docker vagrant\n\nSHELL\n```\n\n下面是Vagrant虚拟机的配置，可以根据需要进行更改:\n\n- 用户/密码: vagrant/vagrant\n- 共享目录: 桌面上的share目录将与虚拟机内的/home/vagrant/share目录内容实时同步\n- 内存：2GB\n- CPU: 1\n\n\n## 二. 集群管理\n\n#### **1. 常用命令**\n\n下面是一些常用的Vagrant管理命令，操作特定虚拟机时仅需指定虚拟机的名称。\n\n- **vagrant ssh:** SSH登陆虚拟机\n- **vagrant halt:** 关闭虚拟机\n- **vagrant destroy:** 删除虚拟机\n- **vagrant ssh-config** 查看虚拟机SSH配置\n\n**启动单个虚拟机：**\n\n```\nvagrant up node1\n```\n\n**启动多个虚拟机：**\n\n```\nvagrant up node1 node3\n```\n\n**启动所有虚拟机：**\n\n```\nvagrant up\n```\n\n#### **2. SSH免密码登陆**\n\n使用**vagrant ssh**命令登陆虚拟机必须切换到Vagrantfile所在的目录，而直接使用虚拟机IP登陆虚拟机则更为方便:\n\n```\nssh vagrant@192.168.59.2\n```\n\n此时SSH登陆需要输入虚拟机vagrant用户的密码，即**vagrant**\n\n将主机的公钥复制到虚拟机的authorized_keys文件中即可实现SSH免密码登陆:\n\n```\ncat $HOME/.ssh/id_rsa.pub | ssh vagrant@192.168.59.2 'cat >> $HOME/.ssh/authorized_keys'\n```\n\n#### **3. 重新安装软件**\n\nVagrant中有下面一段内容：\n\n```\n# 使用shell脚本进行软件安装和配置\nnode.vm.provision \"shell\", inline: <<-SHELL\n\n\t# 安装docker 1.11.0\n\twget -qO- https://get.docker.com/ | sed 's/docker-engine/docker-engine=1.11.0-0~trusty/' | sh\n\tusermod -aG docker vagrant\nSHELL\n```\n\n其实就是嵌入了一段Shell脚本进行软件的安装和配置，这里我安装了[Docker](https://www.docker.com/)，当然也可以安装其他所需要的软件。修改此段内容之后，重新创建虚拟机需要使用\"--provision\"选项。\n\n```\nvagrant halt\nvagrant up --provision\n```\n\n#### **4. 共享目录挂载出错**\n\nVirtualBox设置共享目录时需要在虚拟机中安装VirtualBox Guest Additions，这个Vagrant会自动安装。但是，VirtualBox Guest Additions是内核模块，当虚拟机的内核升级之后，VirtualBox Guest Additions会失效，导致共享目录挂载失败，出错信息如下:\n\n```\nFailed to mount folders in Linux guest. This is usually because\nthe \"vboxsf\" file system is not available. Please verify that\nthe guest additions are properly installed in the guest and\ncan work properly. The command attempted was:\n\nmount -t vboxsf -o uid=`id -u vagrant`,gid=`getent group vagrant | cut -d: -f3` vagrant /vagrant\nmount -t vboxsf -o uid=`id -u vagrant`,gid=`id -g vagrant` vagrant /vagrant\n\nThe error output from the last command was:\n\nstdin: is not a tty\n/sbin/mount.vboxsf: mounting failed with the error: No such device\n```\n\n安装Vagrant插件[vagrant-vbguest](https://github.com/dotless-de/vagrant-vbguest)可以解决这个问题，因为该插件会在虚拟机内核升级之后重新安装VirtualBox Guest Additions。\n\n```\nvagrant plugin install vagrant-vbguest\n```","source":"_posts/2016/07/03/vagrant-vm-cluster.md","raw":"title: 使用Vagrant创建多节点虚拟机集群\n\ndate: 2016-07-03 10:00\n\ntags: [Vagrant]\n\n---\n\n**摘要:** 在前一篇博客中，我介绍了[使用Vagrant快速创建虚拟机](http://kiwenlau.com/2016/06/19/160619-vagrant-virtual-machine/)，但是所创建的只是单个虚拟机。这篇博客将介绍[使用Vagrant创建多节点虚拟机集群](http://kiwenlau.com/2016/07/03/vagrant-vm-cluster/)，可以作为[Hadoop](http://hadoop.apache.org/)，[Spark](http://spark.apache.org/)以及[Storm](http://storm.apache.org/)等分布式系统的运行环境。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-07-03](http://kiwenlau.com/2016/07/03/vagrant-vm-cluster/)\n\n<img src=\"vagrant-vm-cluster/vagrant-vm-cluster.png\" width = \"500\"/>\n\n本文所有操作是在MacBook上进行的，Windows上的操作大部分一致，但是可能会有一些小问题。\n\n## 一. 集群创建\n\n#### **1. 安装[VirtualBox](https://www.virtualbox.org/wiki/Downloads)**\n\n#### **2. 安装[Vagrant](https://www.vagrantup.com/downloads.html)**\n\n#### **3. 下载Box**\n\n```\nvagrant box add ubuntu/trusty64\n```\n\n**Box**相当于虚拟机所依赖的镜像文件。\n\n#### **4. 编辑Vagrantfile**\n\n```\nmkdir vagrant-cluster\ncd vagrant-cluster\nvim Vagrantfile\n```\n\nVagrantfile如下，可以通过注释理解每个自定义配置的含义：\n\n```\nVagrant.configure(\"2\") do |config|\n\n\t(1..3).each do |i|\n\n\t\tconfig.vm.define \"node#{i}\" do |node|\n\n\t\t# 设置虚拟机的Box\n\t\tnode.vm.box = \"ubuntu/trusty64\"\n\n\t\t# 设置虚拟机的主机名\n\t\tnode.vm.hostname=\"node#{i}\"\n\n\t\t# 设置虚拟机的IP\n\t\tnode.vm.network \"private_network\", ip: \"192.168.59.#{i}\"\n\n\t\t# 设置主机与虚拟机的共享目录\n\t\tnode.vm.synced_folder \"~/Desktop/share\", \"/home/vagrant/share\"\n\n\t\t# VirtaulBox相关配置\n\t\tnode.vm.provider \"virtualbox\" do |v|\n\n\t\t\t# 设置虚拟机的名称\n\t\t\tv.name = \"node#{i}\"\n\n\t\t\t# 设置虚拟机的内存大小  \n\t\t\tv.memory = 2048\n\n\t\t\t# 设置虚拟机的CPU个数\n\t\t\tv.cpus = 1\n\t\tend\n  \n\t\t# 使用shell脚本进行软件安装和配置\n\t\tnode.vm.provision \"shell\", inline: <<-SHELL\n\n\t\t\t# 安装docker 1.11.0\n\t\t\twget -qO- https://get.docker.com/ | sed 's/docker-engine/docker-engine=1.11.0-0~trusty/' | sh\n\t\t\tusermod -aG docker vagrant\n\t\t\t\n\t\tSHELL\n\n\t\tend\n\tend\nend\n```\n\n与创建单个虚拟机相比，创建多个虚拟机时多了一层循环，而变量i可以用于设置节点的名称与IP，使用**#{i}**取值：\n\n```\n(1..3).each do |i|\n\nend\n```\n\n可知，一共创建了3个虚拟机。\n\n#### **5. 在桌面上创建share目录**\n\n桌面上的share目录将与虚拟机内的/home/vagrant/share目录内容实时同步\n\n```\nmkdir ~/Desktop/share\n```\n\n#### **6. 创建虚拟机**\n\n```\nvagrant up\n```\n\n创建3个虚拟机大概需要15分钟，当然这和机器性能还有网速相关。安装Docker可能会比较慢，不需要的话删除下面几行就可以了：\n\n```\n# 使用shell脚本进行软件安装和配置\nnode.vm.provision \"shell\", inline: <<-SHELL\n\n\t# 安装docker 1.11.0\n\twget -qO- https://get.docker.com/ | sed 's/docker-engine/docker-engine=1.11.0-0~trusty/' | sh\n\tusermod -aG docker vagrant\n\nSHELL\n```\n\n下面是Vagrant虚拟机的配置，可以根据需要进行更改:\n\n- 用户/密码: vagrant/vagrant\n- 共享目录: 桌面上的share目录将与虚拟机内的/home/vagrant/share目录内容实时同步\n- 内存：2GB\n- CPU: 1\n\n\n## 二. 集群管理\n\n#### **1. 常用命令**\n\n下面是一些常用的Vagrant管理命令，操作特定虚拟机时仅需指定虚拟机的名称。\n\n- **vagrant ssh:** SSH登陆虚拟机\n- **vagrant halt:** 关闭虚拟机\n- **vagrant destroy:** 删除虚拟机\n- **vagrant ssh-config** 查看虚拟机SSH配置\n\n**启动单个虚拟机：**\n\n```\nvagrant up node1\n```\n\n**启动多个虚拟机：**\n\n```\nvagrant up node1 node3\n```\n\n**启动所有虚拟机：**\n\n```\nvagrant up\n```\n\n#### **2. SSH免密码登陆**\n\n使用**vagrant ssh**命令登陆虚拟机必须切换到Vagrantfile所在的目录，而直接使用虚拟机IP登陆虚拟机则更为方便:\n\n```\nssh vagrant@192.168.59.2\n```\n\n此时SSH登陆需要输入虚拟机vagrant用户的密码，即**vagrant**\n\n将主机的公钥复制到虚拟机的authorized_keys文件中即可实现SSH免密码登陆:\n\n```\ncat $HOME/.ssh/id_rsa.pub | ssh vagrant@192.168.59.2 'cat >> $HOME/.ssh/authorized_keys'\n```\n\n#### **3. 重新安装软件**\n\nVagrant中有下面一段内容：\n\n```\n# 使用shell脚本进行软件安装和配置\nnode.vm.provision \"shell\", inline: <<-SHELL\n\n\t# 安装docker 1.11.0\n\twget -qO- https://get.docker.com/ | sed 's/docker-engine/docker-engine=1.11.0-0~trusty/' | sh\n\tusermod -aG docker vagrant\nSHELL\n```\n\n其实就是嵌入了一段Shell脚本进行软件的安装和配置，这里我安装了[Docker](https://www.docker.com/)，当然也可以安装其他所需要的软件。修改此段内容之后，重新创建虚拟机需要使用\"--provision\"选项。\n\n```\nvagrant halt\nvagrant up --provision\n```\n\n#### **4. 共享目录挂载出错**\n\nVirtualBox设置共享目录时需要在虚拟机中安装VirtualBox Guest Additions，这个Vagrant会自动安装。但是，VirtualBox Guest Additions是内核模块，当虚拟机的内核升级之后，VirtualBox Guest Additions会失效，导致共享目录挂载失败，出错信息如下:\n\n```\nFailed to mount folders in Linux guest. This is usually because\nthe \"vboxsf\" file system is not available. Please verify that\nthe guest additions are properly installed in the guest and\ncan work properly. The command attempted was:\n\nmount -t vboxsf -o uid=`id -u vagrant`,gid=`getent group vagrant | cut -d: -f3` vagrant /vagrant\nmount -t vboxsf -o uid=`id -u vagrant`,gid=`id -g vagrant` vagrant /vagrant\n\nThe error output from the last command was:\n\nstdin: is not a tty\n/sbin/mount.vboxsf: mounting failed with the error: No such device\n```\n\n安装Vagrant插件[vagrant-vbguest](https://github.com/dotless-de/vagrant-vbguest)可以解决这个问题，因为该插件会在虚拟机内核升级之后重新安装VirtualBox Guest Additions。\n\n```\nvagrant plugin install vagrant-vbguest\n```","slug":"2016/07/03/vagrant-vm-cluster","published":1,"updated":"2016-07-05T01:58:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3ux000dgq8suztrt2cc"},{"title":"Run Hadoop Cluster in Docker Update","date":"2016-06-26T01:00:00.000Z","_content":"\n**Abstract:** Last year, I developed [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau/hadoop-cluster-docker) project, which aims to help user quickly build [Hadoop](http://hadoop.apache.org/) cluster on local host using [Docker](https://www.docker.com/). The project is quite popular with 250+ stars on [GitHub](https://github.com/kiwenlau/hadoop-cluster-docker) and 2500+ pulls on [Docker Hub](https://hub.docker.com/r/kiwenlau/hadoop-master/). In this blog, I'd like to introduce the update version.\n\n<!-- more -->\n\n- Author: [KiwenLau](http://kiwenlau.com/)\n- Date: [2016-06-26](http://kiwenlau.com/2016/06/26/hadoop-cluster-docker-update-english/)\n\n## Introduction\n\nBy packaging [Hadoop](http://hadoop.apache.org/) into [Docker](https://www.docker.com/) image, we can easily build a Hadoop cluster within Docker containers on local host. This is very help for beginners, who want to learn:\n\n- How to configure Hadoop cluster correctly?\n- How to run word count application?\n- How to manage HDFS?\n- How to run test program on local host?\n\nFollowing figure shows the architecture of [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau/hadoop-cluster-docker) project. Hadoop master and slaves run within different Docker containers, NameNode and ResourceManager run within hadoop-master container while DataNode and NodeManager run within hadoop-slave container. NameNode and DataNode are the components of Hadoop Distributed File System(**HDFS**), while ResourceManager and NodeManager are the components of Hadoop cluster resource management system called Yet Another Resource Manager(**YARN**). HDFS is in charge of storing input and output data, while YARN is in charge of managing CPU and Memory resources. \n\n<img src=\"hadoop-cluster-docker-update-english/hadoop-cluster-docker.png\" width = \"500\"/>\n\nIn the old version, I use [serf](https://www.serfdom.io/)/[dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html) to provide DNS service for Hadoop cluster, which is not an elegant solution because it requires extra installation/configuration and it will delay the cluster startup procedure. Thanks to the enhancement of Docker network function, we don't need to use [serf](https://www.serfdom.io/)/[dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html) any more. We can create a independent network for Hadoop cluster using following command:\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\nBy using \"--net=hadoop\" option when we start Hadoop containers, these containers will attach to the \"hadoop\" network and they are able to communicate with container name.\n\n**Key points of update：**\n\n- eliminate [serf](https://www.serfdom.io/)/[dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html) \n- merge master and slave image\n- install Hadoop using [kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n- simplify Hadoop configuration\n\n## 3 Nodes Hadoop Cluster\n\n#### **1. pull docker image**\n\n```\nsudo docker pull kiwenlau/hadoop:1.0\n```\n\n#### **2. clone github repository**\n\n```\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n#### **3. create hadoop network**\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n#### **4. start container**\n\n```\ncd hadoop-cluster-docker\nsudo ./start-container.sh\n```\n\n**output:**\n\n```\nstart hadoop-master container...\nstart hadoop-slave1 container...\nstart hadoop-slave2 container...\nroot@hadoop-master:~# \n```\n- start 3 containers with 1 master and 2 slaves\n- you will get into the /root directory of hadoop-master container\n\n#### **5. start hadoop**\n\n```\n./start-hadoop.sh\n```\n\n#### **6. run wordcount**\n\n```\n./run-wordcount.sh\n```\n\n**output**\n\n```\ninput file1.txt:\nHello Hadoop\n\ninput file2.txt:\nHello Docker\n\nwordcount output:\nDocker    1\nHadoop    1\nHello    2\n```\n\n## Arbitrary size Hadoop cluster\n\n#### **1. pull docker images and clone github repository**\n\ndo 1~3 like previous section\n\n#### **2. rebuild docker image**\n\n```\nsudo ./resize-cluster.sh 5\n```\n- specify parameter > 1: 2, 3..\n- this script just rebuild hadoop image with different **slaves** file, which pecifies the name of all slave nodes\n\n\n#### **3. start container**\n\n```\ncd hadoop-cluster-docker\nsudo ./start-container.sh 5\n```\n- use the same parameter as the step 2\n\n#### **4. run hadoop cluster** \n\ndo 5~6 like previous section\n\n## References\n\n1. [Quickly build arbitrary size Hadoop Cluster based on Docker](http://kiwenlau.blogspot.jp/2015/05/quickly-build-arbitrary-size-hadoop.html)\n2. [How to Install Hadoop on Ubuntu 13.10](https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-on-ubuntu-13-10)\n\n","source":"_posts/2016/06/26/hadoop-cluster-docker-update-english.md","raw":"title: Run Hadoop Cluster in Docker Update\n\ndate: 2016-06-26 10:00\n\ntags: [Docker, Hadoop]\n\n---\n\n**Abstract:** Last year, I developed [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau/hadoop-cluster-docker) project, which aims to help user quickly build [Hadoop](http://hadoop.apache.org/) cluster on local host using [Docker](https://www.docker.com/). The project is quite popular with 250+ stars on [GitHub](https://github.com/kiwenlau/hadoop-cluster-docker) and 2500+ pulls on [Docker Hub](https://hub.docker.com/r/kiwenlau/hadoop-master/). In this blog, I'd like to introduce the update version.\n\n<!-- more -->\n\n- Author: [KiwenLau](http://kiwenlau.com/)\n- Date: [2016-06-26](http://kiwenlau.com/2016/06/26/hadoop-cluster-docker-update-english/)\n\n## Introduction\n\nBy packaging [Hadoop](http://hadoop.apache.org/) into [Docker](https://www.docker.com/) image, we can easily build a Hadoop cluster within Docker containers on local host. This is very help for beginners, who want to learn:\n\n- How to configure Hadoop cluster correctly?\n- How to run word count application?\n- How to manage HDFS?\n- How to run test program on local host?\n\nFollowing figure shows the architecture of [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau/hadoop-cluster-docker) project. Hadoop master and slaves run within different Docker containers, NameNode and ResourceManager run within hadoop-master container while DataNode and NodeManager run within hadoop-slave container. NameNode and DataNode are the components of Hadoop Distributed File System(**HDFS**), while ResourceManager and NodeManager are the components of Hadoop cluster resource management system called Yet Another Resource Manager(**YARN**). HDFS is in charge of storing input and output data, while YARN is in charge of managing CPU and Memory resources. \n\n<img src=\"hadoop-cluster-docker-update-english/hadoop-cluster-docker.png\" width = \"500\"/>\n\nIn the old version, I use [serf](https://www.serfdom.io/)/[dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html) to provide DNS service for Hadoop cluster, which is not an elegant solution because it requires extra installation/configuration and it will delay the cluster startup procedure. Thanks to the enhancement of Docker network function, we don't need to use [serf](https://www.serfdom.io/)/[dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html) any more. We can create a independent network for Hadoop cluster using following command:\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\nBy using \"--net=hadoop\" option when we start Hadoop containers, these containers will attach to the \"hadoop\" network and they are able to communicate with container name.\n\n**Key points of update：**\n\n- eliminate [serf](https://www.serfdom.io/)/[dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html) \n- merge master and slave image\n- install Hadoop using [kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n- simplify Hadoop configuration\n\n## 3 Nodes Hadoop Cluster\n\n#### **1. pull docker image**\n\n```\nsudo docker pull kiwenlau/hadoop:1.0\n```\n\n#### **2. clone github repository**\n\n```\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n#### **3. create hadoop network**\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n#### **4. start container**\n\n```\ncd hadoop-cluster-docker\nsudo ./start-container.sh\n```\n\n**output:**\n\n```\nstart hadoop-master container...\nstart hadoop-slave1 container...\nstart hadoop-slave2 container...\nroot@hadoop-master:~# \n```\n- start 3 containers with 1 master and 2 slaves\n- you will get into the /root directory of hadoop-master container\n\n#### **5. start hadoop**\n\n```\n./start-hadoop.sh\n```\n\n#### **6. run wordcount**\n\n```\n./run-wordcount.sh\n```\n\n**output**\n\n```\ninput file1.txt:\nHello Hadoop\n\ninput file2.txt:\nHello Docker\n\nwordcount output:\nDocker    1\nHadoop    1\nHello    2\n```\n\n## Arbitrary size Hadoop cluster\n\n#### **1. pull docker images and clone github repository**\n\ndo 1~3 like previous section\n\n#### **2. rebuild docker image**\n\n```\nsudo ./resize-cluster.sh 5\n```\n- specify parameter > 1: 2, 3..\n- this script just rebuild hadoop image with different **slaves** file, which pecifies the name of all slave nodes\n\n\n#### **3. start container**\n\n```\ncd hadoop-cluster-docker\nsudo ./start-container.sh 5\n```\n- use the same parameter as the step 2\n\n#### **4. run hadoop cluster** \n\ndo 5~6 like previous section\n\n## References\n\n1. [Quickly build arbitrary size Hadoop Cluster based on Docker](http://kiwenlau.blogspot.jp/2015/05/quickly-build-arbitrary-size-hadoop.html)\n2. [How to Install Hadoop on Ubuntu 13.10](https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-on-ubuntu-13-10)\n\n","slug":"2016/06/26/hadoop-cluster-docker-update-english","published":1,"updated":"2016-07-03T11:42:46.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3v5000ggq8sq53r4uwv"},{"title":"使用Vagrant快速创建虚拟机","date":"2016-06-19T01:00:00.000Z","_content":"\n**摘要:** 手动创建虚拟机非常不方便，重装起来也很麻烦，打包成镜像的话则不易修改，很难进行版本控制，也无法移植到云端。[Vagrant](https://www.vagrantup.com/)可以将创建虚拟机的过程代码化，有效地解决了以上所提的痛点。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-06-19](http://kiwenlau.com/2016/06/19/160619-vagrant-virtual-machine/)\n\n本文所有操作是在MacBook上进行的，Windows上的操作大部分一致，但是可能会有一些小问题。\n\n<img src=\"160619-vagrant-virtual-machine/vagrant-vm.png\" width = \"300\"/>\n\n## 一. 快速入门\n\n#### **1. 安装[VirtualBox](https://www.virtualbox.org/wiki/Downloads)**\n\n#### **2. 安装[Vagrant](https://www.vagrantup.com/downloads.html)**\n\n#### **3. 创建虚拟机**\n\n```\nmkdir vagrant-ubuntu\ncd vagrant-ubuntu\nvagrant box add ubuntu/trusty64\nvagrant init ubuntu/trusty64\nvagrant up --provider virtualbox\nvagrant ssh\n```\n\n- **vagrant box add:** 下载创建虚拟机所依赖的**box**\n- **vagrant init:** 生成创建虚拟机的所依赖的**Vagrantfile**\n- **vagrant up:** 创建虚拟机\n- **vagrant ssh:** SSH登陆虚拟机\n\n不妨查看一下Vagrant自动生成的**Vagrantfile**, 我删除了所有注释：\n\n```\nVagrant.configure(2) do |config|\n   config.vm.box = \"ubuntu/trusty64\"\nend\n```\n**Vagrantfile**的内容非常简单，仅定义虚拟机所依赖的**Box**为**ubuntu/trusty64**。**Box**相当于虚拟机所依赖的镜像文件。因此，这里创建的虚拟机是ubuntu trusty(14.04)。如果你需要创建其他Linux发行版例如Debian或者CentOS，可以在[这里](https://atlas.hashicorp.com/boxes/search)搜索对应的**Box**.\n\nVagrant虚拟机的默认配置:\n\n- 用户/密码: vagrant/vagrant\n- 共享目录: 主机上的vagrant-ubuntu目录与虚拟机内的/vagrant目录内容实时同步\n- 内存：512MB\n- CPU: 1\n\n默认配置并不一定满足开发需求，下一小节将介绍如何进行自定义配置。\n\n\n## 二. 自定义配置\n\n#### **1. 修改Vagrantfile**\n\n```\nvim Vagrantfile\n```\n\n可以通过注释理解每个自定义配置的含义。\n\n```\nVagrant.configure(2) do |config|\n\n  # 设置虚拟机的Box\n  config.vm.box = \"ubuntu/trusty64\"\n  \n  # 设置虚拟机的主机名\n  config.vm.hostname=\"ubuntu\"\n  \n  # 设置虚拟机的IP\n  config.vm.network \"private_network\", ip: \"192.168.0.2\"\n  \n  # 设置主机与虚拟机的共享目录\n  config.vm.synced_folder \"~/Desktop/share\", \"/home/vagrant/share\"\n\n  # VirtaulBox相关配置\n  config.vm.provider \"virtualbox\" do |v|\n\n      # 设置虚拟机的名称\n      v.name = \"ubuntu\"\n\n      # 设置虚拟机的内存大小\n      v.memory = 2048\n\n      # 设置虚拟机的CPU个数\n      v.cpus = 1\n  end\n  \n  # 使用shell脚本进行软件安装和配置\n  config.vm.provision \"shell\", inline: <<-SHELL\n\n     # 安装Docker 1.11.0\n     apt-get update\n     apt-get install apt-transport-https ca-certificates\n     apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n     echo deb https://apt.dockerproject.org/repo ubuntu-trusty main > /etc/apt/sources.list.d/docker.list\n     apt-get update;\n     apt-get install -y -q docker-engine=1.11.0-0~trusty\n     usermod -aG docker vagrant\n\n  SHELL\n\nend\n```\n\n#### **2. 在桌面上创建share目录**\n\n主机上的share目录将与虚拟机内的/home/vagrant/share目录内容实时同步\n\n```\nmkdir ~/Desktop/share\n```\n\n#### **3. 创建虚拟机**\n\n```\nvagrant destroy\nvagrant up --provider virtualbox\n```\n\n#### **4. SSH免密码登陆**\n\n使用**vagrant ssh**命令登陆虚拟机必须切换到Vagrantfile所在的目录，而直接使用虚拟机IP登陆虚拟机则更为方便:\n\n```\nssh vagrant@192.168.0.2\n```\n\n此时SSH登陆需要输入虚拟机vagrant用户的密码，即**vagrant**\n\n将主机的公钥复制到虚拟机的authorized_keys文件中即可实现SSH无密码登陆:\n\n```\ncat $HOME/.ssh/id_rsa.pub | ssh vagrant@127.0.0.1 -p 2222 'cat >> $HOME/.ssh/authorized_keys'\n```\n\n其中，2222是主机SSH登陆虚拟机的转发端口，可以通过以下命令查看:\n\n```\nvagrant ssh-config | grep Port\n  Port 2222\n```\n\n此时SSH登陆虚拟机则不再需要输入密码。\n\n#### **5. 关于Provision**\n\nVagrant中有下面一段内容：\n\n```\n# 使用shell脚本进行软件安装和配置\n  config.vm.provision \"shell\", inline: <<-SHELL\n\n     # 安装Docker 1.11.0\n     apt-get update\n     apt-get install apt-transport-https ca-certificates\n     apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n     echo deb https://apt.dockerproject.org/repo ubuntu-trusty main > /etc/apt/sources.list.d/docker.list\n     apt-get update;\n     apt-get install -y -q docker-engine=1.11.0-0~trusty\n     usermod -aG docker vagrant\n\n  SHELL\n```\n\n其实就是嵌入了一段Shell脚本进行软件的安装和配置，这里我安装了[Docker](https://www.docker.com/)，当然也可以安装其他所需要的软件。修改此段内容之后，重新创建虚拟机需要使用\"--provision\"选项。\n\n```\nvagrant halt\nvagrant up --provider virtualbox --provision\n```\n\n其实，Vagrant支持创建[Docker Provision](https://www.vagrantup.com/docs/provisioning/docker.html)，可以用于创建Docker主机，功能很多，但是用起来不如使用Shell脚本灵活。\n\n## 三. Vagrant与Docker比较\n\n[Vagrant](https://www.vagrantup.com/)与[Docker](https://www.docker.com/)都可以用于快速创建开发环境，但是，Vagrant是用于创建虚拟机的，而Docker是用于创建容器的，所以两者的功能并不相同。实际工作中，我两个都用，Vagrant仅用于创建虚拟机作为容器运行环境，而Docker用于开发和运行实际应用。这样实现了开发环境两层隔离，MacBook不需要安装多余的软件，Vagrant所创建的虚拟机也仅需要安装Docker等少数软件，这样更加方便和安全。\n\nVagrant是基于Vagrantfile创建虚拟机，而Docker是基于Dockerfile创建容器镜像。两者都是将应用的运行环境代码化，所以非常灵活，易于重复，也可以作版本控制。但是，Vagrantfile的语法其实非常简陋，远没有Dockerfile灵活。因此，Vagrant仅适合于创建开发环境，或者作为容器运行的环境，并不适合打包应用。\n\nVagrant的功能与[Docker Machine](https://docs.docker.com/machine/)功能一致，都是用于创建虚拟机。但是，Docker Machine是专用于创建Docker主机的，而Vagrant可以用于创建不同的开发环境。理论上Docker用户使用Docker Machine会更方便，但是我并没有选择Docker Machine，因为感觉并没有实际需求。Docker Machine所创建的本地虚拟机默认基于Docker专用的Linux发行版[boot2docker](https://github.com/boot2docker/boot2docker)，云端虚拟机默认基于ubuntu，对其他Linux发行版的支持还处于实验阶段。而Vagrant稳定支持更多Linux发行版，所以可以满足更多需求。Vagrant可以通过Vagrantfile进行自定义配置，而Docker Machine并没有对应功能，因此Vagrant用于创建虚拟机更加灵活。\n\n","source":"_posts/2016/06/19/160619-vagrant-virtual-machine.md","raw":"title: 使用Vagrant快速创建虚拟机\n\ndate: 2016-06-19 10:00\n\ntags: [Vagrant]\n\n---\n\n**摘要:** 手动创建虚拟机非常不方便，重装起来也很麻烦，打包成镜像的话则不易修改，很难进行版本控制，也无法移植到云端。[Vagrant](https://www.vagrantup.com/)可以将创建虚拟机的过程代码化，有效地解决了以上所提的痛点。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-06-19](http://kiwenlau.com/2016/06/19/160619-vagrant-virtual-machine/)\n\n本文所有操作是在MacBook上进行的，Windows上的操作大部分一致，但是可能会有一些小问题。\n\n<img src=\"160619-vagrant-virtual-machine/vagrant-vm.png\" width = \"300\"/>\n\n## 一. 快速入门\n\n#### **1. 安装[VirtualBox](https://www.virtualbox.org/wiki/Downloads)**\n\n#### **2. 安装[Vagrant](https://www.vagrantup.com/downloads.html)**\n\n#### **3. 创建虚拟机**\n\n```\nmkdir vagrant-ubuntu\ncd vagrant-ubuntu\nvagrant box add ubuntu/trusty64\nvagrant init ubuntu/trusty64\nvagrant up --provider virtualbox\nvagrant ssh\n```\n\n- **vagrant box add:** 下载创建虚拟机所依赖的**box**\n- **vagrant init:** 生成创建虚拟机的所依赖的**Vagrantfile**\n- **vagrant up:** 创建虚拟机\n- **vagrant ssh:** SSH登陆虚拟机\n\n不妨查看一下Vagrant自动生成的**Vagrantfile**, 我删除了所有注释：\n\n```\nVagrant.configure(2) do |config|\n   config.vm.box = \"ubuntu/trusty64\"\nend\n```\n**Vagrantfile**的内容非常简单，仅定义虚拟机所依赖的**Box**为**ubuntu/trusty64**。**Box**相当于虚拟机所依赖的镜像文件。因此，这里创建的虚拟机是ubuntu trusty(14.04)。如果你需要创建其他Linux发行版例如Debian或者CentOS，可以在[这里](https://atlas.hashicorp.com/boxes/search)搜索对应的**Box**.\n\nVagrant虚拟机的默认配置:\n\n- 用户/密码: vagrant/vagrant\n- 共享目录: 主机上的vagrant-ubuntu目录与虚拟机内的/vagrant目录内容实时同步\n- 内存：512MB\n- CPU: 1\n\n默认配置并不一定满足开发需求，下一小节将介绍如何进行自定义配置。\n\n\n## 二. 自定义配置\n\n#### **1. 修改Vagrantfile**\n\n```\nvim Vagrantfile\n```\n\n可以通过注释理解每个自定义配置的含义。\n\n```\nVagrant.configure(2) do |config|\n\n  # 设置虚拟机的Box\n  config.vm.box = \"ubuntu/trusty64\"\n  \n  # 设置虚拟机的主机名\n  config.vm.hostname=\"ubuntu\"\n  \n  # 设置虚拟机的IP\n  config.vm.network \"private_network\", ip: \"192.168.0.2\"\n  \n  # 设置主机与虚拟机的共享目录\n  config.vm.synced_folder \"~/Desktop/share\", \"/home/vagrant/share\"\n\n  # VirtaulBox相关配置\n  config.vm.provider \"virtualbox\" do |v|\n\n      # 设置虚拟机的名称\n      v.name = \"ubuntu\"\n\n      # 设置虚拟机的内存大小\n      v.memory = 2048\n\n      # 设置虚拟机的CPU个数\n      v.cpus = 1\n  end\n  \n  # 使用shell脚本进行软件安装和配置\n  config.vm.provision \"shell\", inline: <<-SHELL\n\n     # 安装Docker 1.11.0\n     apt-get update\n     apt-get install apt-transport-https ca-certificates\n     apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n     echo deb https://apt.dockerproject.org/repo ubuntu-trusty main > /etc/apt/sources.list.d/docker.list\n     apt-get update;\n     apt-get install -y -q docker-engine=1.11.0-0~trusty\n     usermod -aG docker vagrant\n\n  SHELL\n\nend\n```\n\n#### **2. 在桌面上创建share目录**\n\n主机上的share目录将与虚拟机内的/home/vagrant/share目录内容实时同步\n\n```\nmkdir ~/Desktop/share\n```\n\n#### **3. 创建虚拟机**\n\n```\nvagrant destroy\nvagrant up --provider virtualbox\n```\n\n#### **4. SSH免密码登陆**\n\n使用**vagrant ssh**命令登陆虚拟机必须切换到Vagrantfile所在的目录，而直接使用虚拟机IP登陆虚拟机则更为方便:\n\n```\nssh vagrant@192.168.0.2\n```\n\n此时SSH登陆需要输入虚拟机vagrant用户的密码，即**vagrant**\n\n将主机的公钥复制到虚拟机的authorized_keys文件中即可实现SSH无密码登陆:\n\n```\ncat $HOME/.ssh/id_rsa.pub | ssh vagrant@127.0.0.1 -p 2222 'cat >> $HOME/.ssh/authorized_keys'\n```\n\n其中，2222是主机SSH登陆虚拟机的转发端口，可以通过以下命令查看:\n\n```\nvagrant ssh-config | grep Port\n  Port 2222\n```\n\n此时SSH登陆虚拟机则不再需要输入密码。\n\n#### **5. 关于Provision**\n\nVagrant中有下面一段内容：\n\n```\n# 使用shell脚本进行软件安装和配置\n  config.vm.provision \"shell\", inline: <<-SHELL\n\n     # 安装Docker 1.11.0\n     apt-get update\n     apt-get install apt-transport-https ca-certificates\n     apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n     echo deb https://apt.dockerproject.org/repo ubuntu-trusty main > /etc/apt/sources.list.d/docker.list\n     apt-get update;\n     apt-get install -y -q docker-engine=1.11.0-0~trusty\n     usermod -aG docker vagrant\n\n  SHELL\n```\n\n其实就是嵌入了一段Shell脚本进行软件的安装和配置，这里我安装了[Docker](https://www.docker.com/)，当然也可以安装其他所需要的软件。修改此段内容之后，重新创建虚拟机需要使用\"--provision\"选项。\n\n```\nvagrant halt\nvagrant up --provider virtualbox --provision\n```\n\n其实，Vagrant支持创建[Docker Provision](https://www.vagrantup.com/docs/provisioning/docker.html)，可以用于创建Docker主机，功能很多，但是用起来不如使用Shell脚本灵活。\n\n## 三. Vagrant与Docker比较\n\n[Vagrant](https://www.vagrantup.com/)与[Docker](https://www.docker.com/)都可以用于快速创建开发环境，但是，Vagrant是用于创建虚拟机的，而Docker是用于创建容器的，所以两者的功能并不相同。实际工作中，我两个都用，Vagrant仅用于创建虚拟机作为容器运行环境，而Docker用于开发和运行实际应用。这样实现了开发环境两层隔离，MacBook不需要安装多余的软件，Vagrant所创建的虚拟机也仅需要安装Docker等少数软件，这样更加方便和安全。\n\nVagrant是基于Vagrantfile创建虚拟机，而Docker是基于Dockerfile创建容器镜像。两者都是将应用的运行环境代码化，所以非常灵活，易于重复，也可以作版本控制。但是，Vagrantfile的语法其实非常简陋，远没有Dockerfile灵活。因此，Vagrant仅适合于创建开发环境，或者作为容器运行的环境，并不适合打包应用。\n\nVagrant的功能与[Docker Machine](https://docs.docker.com/machine/)功能一致，都是用于创建虚拟机。但是，Docker Machine是专用于创建Docker主机的，而Vagrant可以用于创建不同的开发环境。理论上Docker用户使用Docker Machine会更方便，但是我并没有选择Docker Machine，因为感觉并没有实际需求。Docker Machine所创建的本地虚拟机默认基于Docker专用的Linux发行版[boot2docker](https://github.com/boot2docker/boot2docker)，云端虚拟机默认基于ubuntu，对其他Linux发行版的支持还处于实验阶段。而Vagrant稳定支持更多Linux发行版，所以可以满足更多需求。Vagrant可以通过Vagrantfile进行自定义配置，而Docker Machine并没有对应功能，因此Vagrant用于创建虚拟机更加灵活。\n\n","slug":"2016/06/19/160619-vagrant-virtual-machine","published":1,"updated":"2016-07-03T11:58:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3ve000kgq8swxndz864"},{"title":"基于Docker搭建Hadoop集群之升级版","date":"2016-06-12T01:00:00.000Z","_content":"\n**摘要:** [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau/hadoop-cluster-docker)是去年参加[Docker巨好玩](http://www.alauda.cn/2015/04/28/docker-great/)比赛开发的，得了[二等奖](http://www.alauda.cn/2015/06/11/presentation/)并赢了一块苹果手表，目前这个项目已经在GitHub上获得了236个Star，DockerHub的镜像下载次数2000+。总之，项目还算很受欢迎吧，这篇博客将介绍项目的升级版。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-06-12](http://kiwenlau.com/2016/06/12/160612-hadoop-cluster-docker-update/)\n\n## 一. 项目介绍\n\n将[Hadoop](http://hadoop.apache.org/)打包到[Docker](https://www.docker.com/)镜像中，就可以快速地在单个机器上搭建Hadoop集群，这样可以方便新手测试和学习。\n\n如下图所示，Hadoop的master和slave分别运行在不同的Docker容器中，其中hadoop-master容器中运行NameNode和ResourceManager，hadoop-slave容器中运行DataNode和NodeManager。NameNode和DataNode是Hadoop分布式文件系统HDFS的组件，负责储存输入以及输出数据，而ResourceManager和NodeManager是Hadoop集群资源管理系统YARN的组件，负责CPU和内存资源的调度。\n\n<img src=\"160612-hadoop-cluster-docker-update/hadoop-cluster-docker.png\" width = \"500\"/>\n\n之前的版本使用serf/dnsmasq为Hadoop集群提供DNS服务，由于Docker网络功能更新，现在并不需要了。更新的版本中，使用以下命令为Hadoop集群创建单独的网络:\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n然后在运行Hadoop容器时，使用\"--net=hadoop\"选项，这时所有容器将运行在hadoop网络中，它们可以通过容器名称进行通信。\n\n**项目更新要点：**\n\n- 去除serf/dnsmasq\n- 合并Master和Slave镜像\n- 使用[kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)项目编译的Hadoo进行安装\n- 优化Hadoop配置\n\n## 二. 3节点Hadoop集群搭建步骤\n\n#### **1. 下载Docker镜像**\n\n```\nsudo docker pull kiwenlau/hadoop:1.0\n```\n\n#### **2. 下载GitHub仓库**\n\n```\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n#### **3. 创建Hadoop网络**\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n#### **4. 运行Docker容器**\n\n```\ncd hadoop-cluster-docker\n./start-container.sh\n```\n\n**运行结果**\n\n```\nstart hadoop-master container...\nstart hadoop-slave1 container...\nstart hadoop-slave2 container...\nroot@hadoop-master:~# \n```\n\n- 启动了3个容器，1个master, 2个slave\n- 运行后就进入了hadoop-master容器的/root目录\n\n#### **5. 启动hadoop**\n\n```\n./start-hadoop.sh\n```\n\n#### **6. 运行wordcount**\n\n```\n./run-wordcount.sh\n```\n\n**运行结果**\n\n```\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\nHadoop网页管理地址:\n\n- NameNode: [http://192.168.59.1:50070/](http://192.168.59.1:50070/)\n- ResourceManager: [http://192.168.59.1:8088/](http://192.168.59.1:8088/)\n\n192.168.59.1为运行容器的主机的IP。\n\n## 三. N节点Hadoop集群搭建步骤\n\n#### **1. 准备**\n\n- 参考第二部分1~3：下载Docker镜像，下载GitHub仓库，以及创建Hadoop网络\n\n#### **2. 重新构建Docker镜像**\n\n```\n./resize-cluster.sh 5\n```\n- 可以指定任意N(N>1)\n\n#### **3. 启动Docker容器**\n\n```\n./start-container.sh 5\n```\n- 与第2步中的N保持一致。\n\n#### **4. 运行Hadoop**\n\n- 参考第二部分5~6：启动Hadoop，并运行wordcount。\n\n## 参考\n\n1. [基于Docker搭建多节点Hadoop集群](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n2. [How to Install Hadoop on Ubuntu 13.10](https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-on-ubuntu-13-10)\n\n","source":"_posts/2016/06/12/160612-hadoop-cluster-docker-update.md","raw":"title: 基于Docker搭建Hadoop集群之升级版\n\ndate: 2016-06-12 10:00\n\ntags: [Docker, Hadoop]\n\n---\n\n**摘要:** [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau/hadoop-cluster-docker)是去年参加[Docker巨好玩](http://www.alauda.cn/2015/04/28/docker-great/)比赛开发的，得了[二等奖](http://www.alauda.cn/2015/06/11/presentation/)并赢了一块苹果手表，目前这个项目已经在GitHub上获得了236个Star，DockerHub的镜像下载次数2000+。总之，项目还算很受欢迎吧，这篇博客将介绍项目的升级版。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-06-12](http://kiwenlau.com/2016/06/12/160612-hadoop-cluster-docker-update/)\n\n## 一. 项目介绍\n\n将[Hadoop](http://hadoop.apache.org/)打包到[Docker](https://www.docker.com/)镜像中，就可以快速地在单个机器上搭建Hadoop集群，这样可以方便新手测试和学习。\n\n如下图所示，Hadoop的master和slave分别运行在不同的Docker容器中，其中hadoop-master容器中运行NameNode和ResourceManager，hadoop-slave容器中运行DataNode和NodeManager。NameNode和DataNode是Hadoop分布式文件系统HDFS的组件，负责储存输入以及输出数据，而ResourceManager和NodeManager是Hadoop集群资源管理系统YARN的组件，负责CPU和内存资源的调度。\n\n<img src=\"160612-hadoop-cluster-docker-update/hadoop-cluster-docker.png\" width = \"500\"/>\n\n之前的版本使用serf/dnsmasq为Hadoop集群提供DNS服务，由于Docker网络功能更新，现在并不需要了。更新的版本中，使用以下命令为Hadoop集群创建单独的网络:\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n然后在运行Hadoop容器时，使用\"--net=hadoop\"选项，这时所有容器将运行在hadoop网络中，它们可以通过容器名称进行通信。\n\n**项目更新要点：**\n\n- 去除serf/dnsmasq\n- 合并Master和Slave镜像\n- 使用[kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)项目编译的Hadoo进行安装\n- 优化Hadoop配置\n\n## 二. 3节点Hadoop集群搭建步骤\n\n#### **1. 下载Docker镜像**\n\n```\nsudo docker pull kiwenlau/hadoop:1.0\n```\n\n#### **2. 下载GitHub仓库**\n\n```\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n#### **3. 创建Hadoop网络**\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n#### **4. 运行Docker容器**\n\n```\ncd hadoop-cluster-docker\n./start-container.sh\n```\n\n**运行结果**\n\n```\nstart hadoop-master container...\nstart hadoop-slave1 container...\nstart hadoop-slave2 container...\nroot@hadoop-master:~# \n```\n\n- 启动了3个容器，1个master, 2个slave\n- 运行后就进入了hadoop-master容器的/root目录\n\n#### **5. 启动hadoop**\n\n```\n./start-hadoop.sh\n```\n\n#### **6. 运行wordcount**\n\n```\n./run-wordcount.sh\n```\n\n**运行结果**\n\n```\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\nHadoop网页管理地址:\n\n- NameNode: [http://192.168.59.1:50070/](http://192.168.59.1:50070/)\n- ResourceManager: [http://192.168.59.1:8088/](http://192.168.59.1:8088/)\n\n192.168.59.1为运行容器的主机的IP。\n\n## 三. N节点Hadoop集群搭建步骤\n\n#### **1. 准备**\n\n- 参考第二部分1~3：下载Docker镜像，下载GitHub仓库，以及创建Hadoop网络\n\n#### **2. 重新构建Docker镜像**\n\n```\n./resize-cluster.sh 5\n```\n- 可以指定任意N(N>1)\n\n#### **3. 启动Docker容器**\n\n```\n./start-container.sh 5\n```\n- 与第2步中的N保持一致。\n\n#### **4. 运行Hadoop**\n\n- 参考第二部分5~6：启动Hadoop，并运行wordcount。\n\n## 参考\n\n1. [基于Docker搭建多节点Hadoop集群](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n2. [How to Install Hadoop on Ubuntu 13.10](https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-on-ubuntu-13-10)\n\n","slug":"2016/06/12/160612-hadoop-cluster-docker-update","published":1,"updated":"2016-07-03T11:59:30.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3vn000mgq8sx4fas8k2"},{"title":"基于Docker编译Hadoop","date":"2016-06-05T01:00:00.000Z","_content":"\n**摘要:** 将编译Hadoop所需要的依赖软件安装到Docker镜像中，然后在Docker容器中编译Hadoop，可以提高编译效率，同时避免污染主机。编译其他软件时，也可以参考这篇博客的方法。\n\n**GitHub地址:**\n- [kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-06-05](http://kiwenlau.com/2016/06/05/160605-compile-hadoop-docker/)\n\n<img src=\"160605-compile-hadoop-docker/hadoop-docker.png\" width = \"300\"/>\n\n在前一篇博客中，我介绍了[64位Ubuntu中编译Hadoop的步骤](http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/)。这篇博客将介绍基于Docker编译Hadoop的方法。\n\n## 一. 编译步骤\n\n#### **1. 下载Docker镜像**\n\n```\nsudo docker pull kiwenlau/compile-hadoop\n```\n\n或者自行构建Docker镜像\n\n```\nsudo docker build -t kiwenlau/compile-hadoop .\n```\n\n\n#### **2. 下载并解压[Hadoop源文件](http://archive.apache.org/dist/hadoop/core/)**\n\n```\nexport VERSION=2.7.2\nwget http://archive.apache.org/dist/hadoop/core/hadoop-$VERSION/hadoop-$VERSION-src.tar.gz\ntar -xzvf hadoop-$VERSION-src.tar.gz\n```\n\n#### **3. 运行Docker容器，在容器中编译Hadoop**\n\n```\nsudo docker run -v $(pwd)/hadoop-$VERSION-src:/hadoop-$VERSION-src kiwenlau/compile-hadoop /root/compile.sh $VERSION\n```\n\n这一步比较耗时，大概需要15~30分钟。 \n\n正确执行的结果如下:\n\n```\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 23:46.056s\n[INFO] Finished at: Tue May 31 16:40:53 UTC 2016\n[INFO] Final Memory: 210M/915M\n[INFO] ------------------------------------------------------------------------\n\n\ncomile hadoop 2.7.2 success!\n```\n\n编译好的二进制文件包位于\n\n```\nhadoop-2.7.2-src/hadoop-dist/target/hadoop-2.7.2.tar.gz\n```\n\n编译其他版本的Hadoop的步骤一致，仅需改变VERSION的值。\n\n可以使用wget命令直接下载GitHub上的Hadoop二进制包:\n\n```\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/$VERSION/hadoop-VERSION.tar.gz\n```\n\n## 二. 方法总结\n\n编译其他软件时，也可以参考本文介绍的方法，具体细节可以参考源码[kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n\n#### **1. 构建编译所需的Docker镜像**\n\n编译软件往往需要安装很多依赖，而编译不同的软件有时需要不同版本的依赖，如果直接在主机上安装这些依赖会污染主机，而且也不易重复。\n\n#### **2. 下载软件源码**\n\n源码不放在Docker镜像里面，可以方便编译不同版本的软件，也可以提高构建Docker镜像的效率。\n\n#### **3. 运行Docker容器编译软件**\n\n软件源码以数据卷(volume)的形式挂载的容器内，编译所得的可执行文件也将位于数据卷内。\n\n","source":"_posts/2016/06/05/160605-compile-hadoop-docker.md","raw":"title: 基于Docker编译Hadoop\n\ndate: 2016-06-05 10:00\n\ntags: [Docker, Hadoop]\n\n---\n\n**摘要:** 将编译Hadoop所需要的依赖软件安装到Docker镜像中，然后在Docker容器中编译Hadoop，可以提高编译效率，同时避免污染主机。编译其他软件时，也可以参考这篇博客的方法。\n\n**GitHub地址:**\n- [kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-06-05](http://kiwenlau.com/2016/06/05/160605-compile-hadoop-docker/)\n\n<img src=\"160605-compile-hadoop-docker/hadoop-docker.png\" width = \"300\"/>\n\n在前一篇博客中，我介绍了[64位Ubuntu中编译Hadoop的步骤](http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/)。这篇博客将介绍基于Docker编译Hadoop的方法。\n\n## 一. 编译步骤\n\n#### **1. 下载Docker镜像**\n\n```\nsudo docker pull kiwenlau/compile-hadoop\n```\n\n或者自行构建Docker镜像\n\n```\nsudo docker build -t kiwenlau/compile-hadoop .\n```\n\n\n#### **2. 下载并解压[Hadoop源文件](http://archive.apache.org/dist/hadoop/core/)**\n\n```\nexport VERSION=2.7.2\nwget http://archive.apache.org/dist/hadoop/core/hadoop-$VERSION/hadoop-$VERSION-src.tar.gz\ntar -xzvf hadoop-$VERSION-src.tar.gz\n```\n\n#### **3. 运行Docker容器，在容器中编译Hadoop**\n\n```\nsudo docker run -v $(pwd)/hadoop-$VERSION-src:/hadoop-$VERSION-src kiwenlau/compile-hadoop /root/compile.sh $VERSION\n```\n\n这一步比较耗时，大概需要15~30分钟。 \n\n正确执行的结果如下:\n\n```\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 23:46.056s\n[INFO] Finished at: Tue May 31 16:40:53 UTC 2016\n[INFO] Final Memory: 210M/915M\n[INFO] ------------------------------------------------------------------------\n\n\ncomile hadoop 2.7.2 success!\n```\n\n编译好的二进制文件包位于\n\n```\nhadoop-2.7.2-src/hadoop-dist/target/hadoop-2.7.2.tar.gz\n```\n\n编译其他版本的Hadoop的步骤一致，仅需改变VERSION的值。\n\n可以使用wget命令直接下载GitHub上的Hadoop二进制包:\n\n```\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/$VERSION/hadoop-VERSION.tar.gz\n```\n\n## 二. 方法总结\n\n编译其他软件时，也可以参考本文介绍的方法，具体细节可以参考源码[kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n\n#### **1. 构建编译所需的Docker镜像**\n\n编译软件往往需要安装很多依赖，而编译不同的软件有时需要不同版本的依赖，如果直接在主机上安装这些依赖会污染主机，而且也不易重复。\n\n#### **2. 下载软件源码**\n\n源码不放在Docker镜像里面，可以方便编译不同版本的软件，也可以提高构建Docker镜像的效率。\n\n#### **3. 运行Docker容器编译软件**\n\n软件源码以数据卷(volume)的形式挂载的容器内，编译所得的可执行文件也将位于数据卷内。\n\n","slug":"2016/06/05/160605-compile-hadoop-docker","published":1,"updated":"2016-09-07T14:48:14.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3vu000pgq8sj1jaqy44"},{"title":"64位Ubuntu中编译Hadoop的步骤","date":"2016-05-29T00:00:00.000Z","_content":"\n**摘要:** 本文介绍了在64位Ubuntu 14.04中编译Hadoop的步骤。\n\nHadoop二进制包下载地址：[百度网盘](https://pan.baidu.com/s/1hrGLqlA) [GitHub](https://github.com/kiwenlau/compile-hadoop/releases)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-05-29](http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/)\n\n<img src=\"160529-compile-hadoop-ubuntu/ubuntu-hadoop.png\" width = \"300\"/>\n\nHadoop官网提供的二进制包是在32位系统上编译的，在64系统上运行会出错：\n\n```\nWARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n```\n\n这时需要自行编译Hadoop源码。以下为编译步骤:\n\n#### **1. 安装依赖软件**\n\n```\nsudo apt-get update\nsudo apt-get install -y openjdk-7-jdk libprotobuf-dev protobuf-compiler maven cmake build-essential pkg-config libssl-dev zlib1g-dev llvm-gcc automake autoconf make\n```\n\n#### **2. 下载并解压[Hadoop源文件](http://archive.apache.org/dist/hadoop/core/)**\n\n```\nwget http://archive.apache.org/dist/hadoop/core/hadoop-2.3.0/hadoop-2.3.0-src.tar.gz\ntar -xzvf hadoop-2.3.0-src.tar.gz\n```\n\n#### **3. 编译Hadoop**\n\n```\ncd hadoop-2.3.0-src\nmvn package -Pdist,native -DskipTests –Dtar\n```\n\n这一步比较耗时，大概需要15~30分钟。 \n\n正确执行的结果如下:\n\n```\n[INFO] ------------------------------------------------------------------------\n\n[INFO] BUILD SUCCESS\n\n[INFO] ------------------------------------------------------------------------\n\n[INFO] Total time: 14:59.240s\n\n[INFO] Finished at: Thu Jan 15 18:51:59 JST 2015\n\n[INFO] Final Memory: 168M/435M\n\n[INFO] ------------------------------------------------------------------------\n```\n\n编译好的二进制文件包位于\n\n```\nhadoop-2.3.0-src/hadoop-dist/target/hadoop-2.3.0.tar.gz\n```\n\n编译其他版本的Hadoop的步骤一致，也可以直接下载我编译好的Hadoop:\n\n- [百度网盘](https://pan.baidu.com/s/1hrGLqlA)\n- [GitHub](https://github.com/kiwenlau/compile-hadoop/releases)\n\nLinux终端用户可以使用wget命令直接下载GitHub上的Hadoop二进制包:\n\n```\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.3.0/hadoop-2.3.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.4.0/hadoop-2.4.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.5.0/hadoop-2.5.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.6.0/hadoop-2.6.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.7.0/hadoop-2.7.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.7.2/hadoop-2.7.2.tar.gz\n```\n\n另外, 使用自行编译的Hadoop二进制包安装Hadoop时需要删除.bashrc文件与hadoop-env.sh文件中下面两行（默认不会有这两行，但是尝试解决报错时可能改写了）\n\n```\nexport HADOOP_COMMON_LIB_NATIVE_DIR=\"~/hadoop/lib/\"\nexport HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=~/hadoop/lib/\"\n```\n\n","source":"_posts/2016/05/29/160529-compile-hadoop-ubuntu.md","raw":"title: 64位Ubuntu中编译Hadoop的步骤\n\ndate: 2016-05-29 09:00\n\ntags: [Hadoop]\n\n---\n\n**摘要:** 本文介绍了在64位Ubuntu 14.04中编译Hadoop的步骤。\n\nHadoop二进制包下载地址：[百度网盘](https://pan.baidu.com/s/1hrGLqlA) [GitHub](https://github.com/kiwenlau/compile-hadoop/releases)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-05-29](http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/)\n\n<img src=\"160529-compile-hadoop-ubuntu/ubuntu-hadoop.png\" width = \"300\"/>\n\nHadoop官网提供的二进制包是在32位系统上编译的，在64系统上运行会出错：\n\n```\nWARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n```\n\n这时需要自行编译Hadoop源码。以下为编译步骤:\n\n#### **1. 安装依赖软件**\n\n```\nsudo apt-get update\nsudo apt-get install -y openjdk-7-jdk libprotobuf-dev protobuf-compiler maven cmake build-essential pkg-config libssl-dev zlib1g-dev llvm-gcc automake autoconf make\n```\n\n#### **2. 下载并解压[Hadoop源文件](http://archive.apache.org/dist/hadoop/core/)**\n\n```\nwget http://archive.apache.org/dist/hadoop/core/hadoop-2.3.0/hadoop-2.3.0-src.tar.gz\ntar -xzvf hadoop-2.3.0-src.tar.gz\n```\n\n#### **3. 编译Hadoop**\n\n```\ncd hadoop-2.3.0-src\nmvn package -Pdist,native -DskipTests –Dtar\n```\n\n这一步比较耗时，大概需要15~30分钟。 \n\n正确执行的结果如下:\n\n```\n[INFO] ------------------------------------------------------------------------\n\n[INFO] BUILD SUCCESS\n\n[INFO] ------------------------------------------------------------------------\n\n[INFO] Total time: 14:59.240s\n\n[INFO] Finished at: Thu Jan 15 18:51:59 JST 2015\n\n[INFO] Final Memory: 168M/435M\n\n[INFO] ------------------------------------------------------------------------\n```\n\n编译好的二进制文件包位于\n\n```\nhadoop-2.3.0-src/hadoop-dist/target/hadoop-2.3.0.tar.gz\n```\n\n编译其他版本的Hadoop的步骤一致，也可以直接下载我编译好的Hadoop:\n\n- [百度网盘](https://pan.baidu.com/s/1hrGLqlA)\n- [GitHub](https://github.com/kiwenlau/compile-hadoop/releases)\n\nLinux终端用户可以使用wget命令直接下载GitHub上的Hadoop二进制包:\n\n```\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.3.0/hadoop-2.3.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.4.0/hadoop-2.4.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.5.0/hadoop-2.5.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.6.0/hadoop-2.6.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.7.0/hadoop-2.7.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.7.2/hadoop-2.7.2.tar.gz\n```\n\n另外, 使用自行编译的Hadoop二进制包安装Hadoop时需要删除.bashrc文件与hadoop-env.sh文件中下面两行（默认不会有这两行，但是尝试解决报错时可能改写了）\n\n```\nexport HADOOP_COMMON_LIB_NATIVE_DIR=\"~/hadoop/lib/\"\nexport HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=~/hadoop/lib/\"\n```\n\n","slug":"2016/05/29/160529-compile-hadoop-ubuntu","published":1,"updated":"2016-07-03T11:46:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3w0000sgq8sbscq21nd"},{"title":"如何运行多进程Docker容器？","date":"2016-01-09T12:00:00.000Z","_content":"\n**摘要:** 本文介绍了两种在Docker容器中运行多个进程的方法: **shell脚本**和**supervisor**。\n\n**GitHub地址:**\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-01-09](http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/)\n\n## 一. 简介\n\n一般来说，Docker容器比较适合运行单个进程。例如，项目\"**使用多个Docker容器运行Kubernetes**\"，Kubernetes的各个组件分别运行在各个容器之中，每个容器只运行单个进程。\n\n然而，很多时候我们需要在Docker容器中运行多个进程。例如，项目\"**使用单个Docker容器运行Kubernetes**\"，kubernetes的各个组件均运行在同一个容器中，该容器中运行了多个进程。那么，**如何运行多进程Docker容器？**\n\n一种方法是使用**Shell脚本**，另一种方法是使用进程管理工具[Supervisor](http://supervisord.org/)。[kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)和[kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)分别采用了这两种方法，用于启动多个进程来运行Kubernetes的各个组件，从而实现\"**使用单个Docker容器运行Kubernetes**\"。下面我将分别介绍两种不同方法。\n\n## 二. 使用Shell脚本运行多进程Docker容器\n\n这个方法大家应该会比较熟悉，使用Shell脚本依次启动Kubernetes的各个组件即可。以下为**start-kubernetes.sh**\n\n```\n#!/bin/bash\n\n# start docker daemon\ndocker daemon > /var/log/docker.log 2>&1 &\n\n# start etcd\netcd --data-dir=/var/etcd/data > /var/log/etcd.log 2>&1 &\n\n# wait for ectd to setup\nsleep 5\n\n# start apiserver\nkube-apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd_servers=http://127.0.0.1:4001 > /var/log/kube-apiserver.log 2>&1 &\n\n# wait for apiserver to setup\nsleep 5\n\n# start controller manager, sheduler, kubelet and proxy\nkube-controller-manager --master=http://0.0.0.0:8080 > /var/log/kube-controller-manager.log 2>&1 &\nkube-scheduler --master=http://0.0.0.0:8080 > /var/log/kube-scheduler.log 2>&1 &\nkubelet --api_servers=http://0.0.0.0:8080 --address=0.0.0.0 --cluster_dns=10.0.0.10 --cluster_domain=\"kubernetes.local\" --pod-infra-container-image=\"kiwenlau/pause:0.8.0\"  > /var/log/kubelet.log 2>&1 &\nkube-proxy --master=http://0.0.0.0:8080 > /var/log/kube-proxy.log 2>&1 &\n\n# just keep this script running\nwhile [[ true ]]; do\n\tsleep 1\ndone\n```\n\n然后在Dockerfile中，将**start-kubernetes.sh**指定为Docker容器默认执行的命令即可:\n\n```\nCMD [\"start-kubernetes.sh\"]\n```\n\n**需要注意**的一点在于，**start-kubernetes.sh**脚本将作为Docker容器的1号进程运行，必须始终保持运行。因为**Docker容器仅在1号进程运行时保持运行**，换言之，Docker容器将在1号进程退出后**Exited**。由于Kubernetes的各个组件都以后台进程方式执行，我在脚本末尾添加了死循环，以保持**start-kubernetes.sh**脚本始终处于运行状态。\n\n```\n# just keep this script running\nwhile [[ true ]]; do\n\tsleep 1\ndone\n```\n\n## 三. 使用supervisor运行多进程Docker容器\n\n[Supervisor](http://supervisord.org/)是进程管理工具。这时，需要编写supervisor的配置文件**kubernetes.conf**:\n\n```\n[supervisord]\nnodaemon=true\n\n[program:etcd]\ncommand=etcd --data-dir=/var/etcd/data\nautorestart=true\nstdout_logfile=/var/log/etcd.stdout.log\nstderr_logfile=/var/log/etcd.stderr.log\n\n[program:kube-apiserver]\ncommand=kube-apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd_servers=http://127.0.0.1:4001\nautorestart=true\nstdout_logfile=/var/log/kube-apiserver.stdout.log\nstderr_logfile=/var/log/kube-apiserver.stderr.log\n\n[program:kube-controller-manager]\ncommand=kube-controller-manager --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/controller-manager.stdout.log\nstderr_logfile=/var/log/controller-manager.stderr.log\n\n[program:kube-scheduler]\ncommand=kube-scheduler --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/kube-scheduler.stdout.log\nstderr_logfile=/var/log/kube-scheduler.stderr.log\n\n[program:kubelet]\ncommand=kubelet --api_servers=http://0.0.0.0:8080 --address=0.0.0.0 --cluster_dns=10.0.0.10 --cluster_domain=\"kubernetes.local\" --pod-infra-container-image=\"kiwenlau/pause:0.8.0\"\nautorestart=true\nstdout_logfile=/var/log/kubelet.stdout.log\nstderr_logfile=/var/log/kubelet.stderr.log\n\n[program:kube-proxy]\ncommand=kube-proxy --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/kube-proxy.stdout.log\nstderr_logfile=/var/log/kube-proxy.stderr.log\n\n[program:docker]\ncommand=docker daemon\nautorestart=true\nstdout_logfile=/var/log/docker.stdout.log\nstderr_logfile=/var/log/docker.stderr.log\n```\n\n可知，将Kubernetes的各个组件的启动命令设为command即可。autorestart参数设为true，意味着supervisor将负责重启意外退出的组件。stdout_logfile和stderr_logfile参数则可以用于设置命令的标准输出文件和标准错误输出文件。\n\n然后在Dockerfile中，将**supervisord**指定为Docker容器默认执行的命令即可:\n\n```\nCMD [\"supervisord\", \"-c\", \"/etc/supervisor/conf.d/kubernetes.conf\"]\n```\n\n此时, supervisord是Docker容器中的1号进程，也需要始终保持运行状态。nodaemon设为true时，表示supervisor保持前台运行而非在后台运行。若supervisor在后台运行，则Docker容器也会在执行supervisord命令后立即Exited.\n\n```\n[supervisord]\nnodaemon=true\n```\n\n\n\n## 四. 总结\n\n使用Shell脚本运行多进程Docker容器，优势是大家比较熟悉。由于需要保持Docker容器的1号进程始终运行，这一点比较容易出错。若要实现进程意外退出后自动重启的话，使用shell脚本比较麻烦。\n\n使用supervisor运行多进程Docker容器，非常方便。另外，保持1号进程保持运行，以及进程意外退出后自动重启，实现起来都很简单。\n\n\n#### **使用多个Docker容器运行Kubernetes**\n\n**GitHub地址**\n\n- [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。\n\n<img src=\"160109-multiple-processes--docker-container/kubernetes-multiple-docker.png\" width = \"500\"/>\n\n#### **使用单个Docker容器运行Kubernetes**\n\n**GitHub地址:**\n\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n该项目中，我将kubernetes的所有组件：etcd, controller manager, apiserver, scheduler, kubelet, proxy以及docker daemon均运行在同一个Docker容器之中。\n\n容器启动时，各个组件由shell脚本或者supervisor启动。\n\n<img src=\"160109-multiple-processes--docker-container/kubernetes-single-docker.png\" width = \"500\"/>\n\n## 参考\n1. [Using Supervisor with Docker](https://docs.docker.com/engine/articles/using_supervisord/)\n2. [How To Install and Manage Supervisor on Ubuntu and Debian VPS](https://www.digitalocean.com/community/tutorials/how-to-install-and-manage-supervisor-on-ubuntu-and-debian-vps)\n3. [基于Docker搭建单机版Kuberntes](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n4. [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n","source":"_posts/2016/01/09/160109-multiple-processes--docker-container.md","raw":"title: 如何运行多进程Docker容器？\n\ndate: 2016-01-09 21:00:00\n\ntags: [Docker, Kubernetes]\n\n---\n\n**摘要:** 本文介绍了两种在Docker容器中运行多个进程的方法: **shell脚本**和**supervisor**。\n\n**GitHub地址:**\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-01-09](http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/)\n\n## 一. 简介\n\n一般来说，Docker容器比较适合运行单个进程。例如，项目\"**使用多个Docker容器运行Kubernetes**\"，Kubernetes的各个组件分别运行在各个容器之中，每个容器只运行单个进程。\n\n然而，很多时候我们需要在Docker容器中运行多个进程。例如，项目\"**使用单个Docker容器运行Kubernetes**\"，kubernetes的各个组件均运行在同一个容器中，该容器中运行了多个进程。那么，**如何运行多进程Docker容器？**\n\n一种方法是使用**Shell脚本**，另一种方法是使用进程管理工具[Supervisor](http://supervisord.org/)。[kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)和[kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)分别采用了这两种方法，用于启动多个进程来运行Kubernetes的各个组件，从而实现\"**使用单个Docker容器运行Kubernetes**\"。下面我将分别介绍两种不同方法。\n\n## 二. 使用Shell脚本运行多进程Docker容器\n\n这个方法大家应该会比较熟悉，使用Shell脚本依次启动Kubernetes的各个组件即可。以下为**start-kubernetes.sh**\n\n```\n#!/bin/bash\n\n# start docker daemon\ndocker daemon > /var/log/docker.log 2>&1 &\n\n# start etcd\netcd --data-dir=/var/etcd/data > /var/log/etcd.log 2>&1 &\n\n# wait for ectd to setup\nsleep 5\n\n# start apiserver\nkube-apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd_servers=http://127.0.0.1:4001 > /var/log/kube-apiserver.log 2>&1 &\n\n# wait for apiserver to setup\nsleep 5\n\n# start controller manager, sheduler, kubelet and proxy\nkube-controller-manager --master=http://0.0.0.0:8080 > /var/log/kube-controller-manager.log 2>&1 &\nkube-scheduler --master=http://0.0.0.0:8080 > /var/log/kube-scheduler.log 2>&1 &\nkubelet --api_servers=http://0.0.0.0:8080 --address=0.0.0.0 --cluster_dns=10.0.0.10 --cluster_domain=\"kubernetes.local\" --pod-infra-container-image=\"kiwenlau/pause:0.8.0\"  > /var/log/kubelet.log 2>&1 &\nkube-proxy --master=http://0.0.0.0:8080 > /var/log/kube-proxy.log 2>&1 &\n\n# just keep this script running\nwhile [[ true ]]; do\n\tsleep 1\ndone\n```\n\n然后在Dockerfile中，将**start-kubernetes.sh**指定为Docker容器默认执行的命令即可:\n\n```\nCMD [\"start-kubernetes.sh\"]\n```\n\n**需要注意**的一点在于，**start-kubernetes.sh**脚本将作为Docker容器的1号进程运行，必须始终保持运行。因为**Docker容器仅在1号进程运行时保持运行**，换言之，Docker容器将在1号进程退出后**Exited**。由于Kubernetes的各个组件都以后台进程方式执行，我在脚本末尾添加了死循环，以保持**start-kubernetes.sh**脚本始终处于运行状态。\n\n```\n# just keep this script running\nwhile [[ true ]]; do\n\tsleep 1\ndone\n```\n\n## 三. 使用supervisor运行多进程Docker容器\n\n[Supervisor](http://supervisord.org/)是进程管理工具。这时，需要编写supervisor的配置文件**kubernetes.conf**:\n\n```\n[supervisord]\nnodaemon=true\n\n[program:etcd]\ncommand=etcd --data-dir=/var/etcd/data\nautorestart=true\nstdout_logfile=/var/log/etcd.stdout.log\nstderr_logfile=/var/log/etcd.stderr.log\n\n[program:kube-apiserver]\ncommand=kube-apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd_servers=http://127.0.0.1:4001\nautorestart=true\nstdout_logfile=/var/log/kube-apiserver.stdout.log\nstderr_logfile=/var/log/kube-apiserver.stderr.log\n\n[program:kube-controller-manager]\ncommand=kube-controller-manager --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/controller-manager.stdout.log\nstderr_logfile=/var/log/controller-manager.stderr.log\n\n[program:kube-scheduler]\ncommand=kube-scheduler --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/kube-scheduler.stdout.log\nstderr_logfile=/var/log/kube-scheduler.stderr.log\n\n[program:kubelet]\ncommand=kubelet --api_servers=http://0.0.0.0:8080 --address=0.0.0.0 --cluster_dns=10.0.0.10 --cluster_domain=\"kubernetes.local\" --pod-infra-container-image=\"kiwenlau/pause:0.8.0\"\nautorestart=true\nstdout_logfile=/var/log/kubelet.stdout.log\nstderr_logfile=/var/log/kubelet.stderr.log\n\n[program:kube-proxy]\ncommand=kube-proxy --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/kube-proxy.stdout.log\nstderr_logfile=/var/log/kube-proxy.stderr.log\n\n[program:docker]\ncommand=docker daemon\nautorestart=true\nstdout_logfile=/var/log/docker.stdout.log\nstderr_logfile=/var/log/docker.stderr.log\n```\n\n可知，将Kubernetes的各个组件的启动命令设为command即可。autorestart参数设为true，意味着supervisor将负责重启意外退出的组件。stdout_logfile和stderr_logfile参数则可以用于设置命令的标准输出文件和标准错误输出文件。\n\n然后在Dockerfile中，将**supervisord**指定为Docker容器默认执行的命令即可:\n\n```\nCMD [\"supervisord\", \"-c\", \"/etc/supervisor/conf.d/kubernetes.conf\"]\n```\n\n此时, supervisord是Docker容器中的1号进程，也需要始终保持运行状态。nodaemon设为true时，表示supervisor保持前台运行而非在后台运行。若supervisor在后台运行，则Docker容器也会在执行supervisord命令后立即Exited.\n\n```\n[supervisord]\nnodaemon=true\n```\n\n\n\n## 四. 总结\n\n使用Shell脚本运行多进程Docker容器，优势是大家比较熟悉。由于需要保持Docker容器的1号进程始终运行，这一点比较容易出错。若要实现进程意外退出后自动重启的话，使用shell脚本比较麻烦。\n\n使用supervisor运行多进程Docker容器，非常方便。另外，保持1号进程保持运行，以及进程意外退出后自动重启，实现起来都很简单。\n\n\n#### **使用多个Docker容器运行Kubernetes**\n\n**GitHub地址**\n\n- [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。\n\n<img src=\"160109-multiple-processes--docker-container/kubernetes-multiple-docker.png\" width = \"500\"/>\n\n#### **使用单个Docker容器运行Kubernetes**\n\n**GitHub地址:**\n\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n该项目中，我将kubernetes的所有组件：etcd, controller manager, apiserver, scheduler, kubelet, proxy以及docker daemon均运行在同一个Docker容器之中。\n\n容器启动时，各个组件由shell脚本或者supervisor启动。\n\n<img src=\"160109-multiple-processes--docker-container/kubernetes-single-docker.png\" width = \"500\"/>\n\n## 参考\n1. [Using Supervisor with Docker](https://docs.docker.com/engine/articles/using_supervisord/)\n2. [How To Install and Manage Supervisor on Ubuntu and Debian VPS](https://www.digitalocean.com/community/tutorials/how-to-install-and-manage-supervisor-on-ubuntu-and-debian-vps)\n3. [基于Docker搭建单机版Kuberntes](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n4. [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n","slug":"2016/01/09/160109-multiple-processes--docker-container","published":1,"updated":"2016-07-03T15:48:36.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3w7000ugq8sxkpuf3a9"},{"title":"基于Docker搭建单机版Kuberntes","date":"2015-11-28T00:00:00.000Z","_content":"\n**摘要:** 本文介绍了基于Docker搭建单机版Kuberntes的方法，Kubernetes的所有组件均运行于单个容器中。\n\n**GitHub地址:**\n- [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2015-11-28](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n\n## 一. Kubernetes简介\n\n2006年，Google工程师Rohit Seth发起了Cgroups内核项目。Cgroups是容器实现CPU，内存等资源隔离的基础，由此可见Google其实很早就开始涉足容器技术。而事实上，Google内部使用容器技术已经长达十年，目前谷歌所有业务包括搜索，Gmail，MapReduce等均运行在容器之中。Google内部使用的集群管理系统--Borg，堪称其容器技术的瑞士军刀。\n\n2014年，Google发起了开源容器集群管理系统--Kubernetes，其设计之初就吸取了Borg的经验和教训，并原生支持了Docker。因此，Kubernetees与较早的集群管理系统Mesos和YARN相比，对容器技术尤其是Docker的支持更加原生，同时提供了更强大的机制实现资源调度，负载均衡，高可用等底层功能，使开发者可以专注于开发应用。\n\n与其他集群系统一致，Kubernetes也采用了Master/Slave结构。下表显示了Kubernetes的各个组件及其功能。\n\n| 角色     | 组件               | 功能                                           |\n| ------- |:-----------------: | :--------------------------------------------:|\n| Master  | apiserver          | 提供RESTful接口                                |\n| Master  | scheduler          | 负责调度，将pod分配到Slave节点                   |\n| Master  | controller-manager | 负责Master的其他功能                           |\n| Master  | etde               | 储存配置信息，节点信息，pod信息等                 |\n| Slave   | kubelet            | 负责管理Pod、容器和容器镜像                       |\n| Slave   | proxy              | 将访问Service的请求转发给对应的Pod，做一些负载均衡  |\n| 客户端   | kubectl            | 命令行工具，向apiserver发起创建Pod等请求          |\n\n\n## 二. kiwenlau/kubernetes镜像简介\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。事实上，Kuberenetes未来的开发目标正是将Kubernetes的各个组件运行到容器之中，这样可以方便Kubernetes的部署和升级。现在我将Kubernetes的各个组件全部运行在容器中必然存在很多问题且很多问题是未知的，因此这个项目仅做学习测试而不宜部署到生产环境中。Kubernetes各个组件容器之间的通信通过docker link实现，其中apiserver与ectd的4001端口进行通信，scheduler，controller-manager，kubelet，proxy以及kubectl与apiserver的8080端口进行通信。\n\n<img src=\"151128-single-kubernetes-docker/kubernetes-multiple-docker.png\" width = \"500\"/>\n\n集群的大致运行流程是这样的: 用户通过kubectl命令向apiserver发起创建Pod的请求; scheduler将创建Pod的任务分配给kubelet；kubelet中包含了一个docker命令行工具，该工具会向Docker deamon发起创建容器的请求; Docker deamon负责下载镜像然后创建容器。\n\n我将Docker deamon运行在Ubuntu主机上，因此Docker daemon所创建的应用容器与Kubernetes各个组件的容器均运行在Ubuntu主机上。docker socket采用volume的形式挂载到kubelet容器内，因此kubelet中的docker命令行工具可以直接与主机上的Docker daemon进行通信。\n\n我是直接将kubernetes发布的各个组件的二进制可执行文件安装在/usr/local/bin目录下，因此，修改Dockerfile中的Kubernetes下载链接的版本号，就可以快速构建其他版本的Kubernetes镜像。另外，仅需修改网络配置，就可以很方便地在多个节点上部署Kubernetes。\n\nkiwenlau/kubernetes:1.0.7镜像版本信息:\n\n- ubuntu: 14.04\n- Kubernetes: 1.0.7\n- ectd: 2.2.1\n\nUbuntu主机版本信息:\n\n- ubuntu: 14.04.3 LTS\n- kernel: 3.16.0-30-generic\n- docker: 1.9.1\n\n\n\n## 三. 运行步骤\n\n#### **1. 安装Docker**\n\nubuntu 14.04上安装Docker: \n\n```\ncurl -fLsS https://get.docker.com/ | sh\n```\n\n其他系统请参考: [https://docs.docker.com/](https://docs.docker.com/)\n\n#### **2. 下载Docker镜像**\n\n我将kiwenlau/kubernetes:1.07以及其他用到的Docker镜像都放在[灵雀云](http://www.alauda.cn/)\n\n```\nsudo docker pull index.alauda.cn/kiwenlau/kubernetes:1.0.7\nsudo docker pull index.alauda.cn/kiwenlau/etcd:v2.2.1\nsudo docker pull index.alauda.cn/kiwenlau/nginx:1.9.7\nsudo docker pull index.alauda.cn/kiwenlau/pause:0.8.0\n```\n\n#### **3. 启动Kubernetes**\n\n```sh\ngit clone https://github.com/kiwenlau/single-kubernetes-docker\ncd single-kubernetes-docker/\nsudo chmod +x start-kubernetes-alauda.sh stop-kubernetes.sh\nsudo ./start-kubernetes-alauda.sh\n```\n\n运行结束后进入kubectl容器。容器主机名为kubeclt。可以通过\"exit\"命令退出容器返回到主机，然后可以通过\"sudo docker exec -it kubectl bash\"命令再次进入kubectl容器。\n\n\n#### **4. 测试Kubernetes**\n\n运行测试脚本，该脚本会启动一个nginx pod。\n\n```\nchmod +x test-kubernetes-alauda.sh\n./test-kubernetes-alauda.sh \n```\n\n输出\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```\n\n\n## 四. 参考\n\n1. [meteorhacks/hyperkube](https://github.com/meteorhacks/hyperkube)\n2. [meteorhacks/kube-init](https://github.com/meteorhacks/kube-init)\n3. [Kubernetes: The Future of Cloud Hosting](https://meteorhacks.com/learn-kubernetes-the-future-of-the-cloud)\n4. [Kubernetes 架构浅析](http://weibo.com/p/1001603912843031387951?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)\n5. [An Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)\n\n","source":"_posts/2015/11/28/151128-single-kubernetes-docker.md","raw":"title: 基于Docker搭建单机版Kuberntes\n\ndate: 2015-11-28 09:00:00\n\ntags: [Docker, Kubernetes]\n\n---\n\n**摘要:** 本文介绍了基于Docker搭建单机版Kuberntes的方法，Kubernetes的所有组件均运行于单个容器中。\n\n**GitHub地址:**\n- [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2015-11-28](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n\n## 一. Kubernetes简介\n\n2006年，Google工程师Rohit Seth发起了Cgroups内核项目。Cgroups是容器实现CPU，内存等资源隔离的基础，由此可见Google其实很早就开始涉足容器技术。而事实上，Google内部使用容器技术已经长达十年，目前谷歌所有业务包括搜索，Gmail，MapReduce等均运行在容器之中。Google内部使用的集群管理系统--Borg，堪称其容器技术的瑞士军刀。\n\n2014年，Google发起了开源容器集群管理系统--Kubernetes，其设计之初就吸取了Borg的经验和教训，并原生支持了Docker。因此，Kubernetees与较早的集群管理系统Mesos和YARN相比，对容器技术尤其是Docker的支持更加原生，同时提供了更强大的机制实现资源调度，负载均衡，高可用等底层功能，使开发者可以专注于开发应用。\n\n与其他集群系统一致，Kubernetes也采用了Master/Slave结构。下表显示了Kubernetes的各个组件及其功能。\n\n| 角色     | 组件               | 功能                                           |\n| ------- |:-----------------: | :--------------------------------------------:|\n| Master  | apiserver          | 提供RESTful接口                                |\n| Master  | scheduler          | 负责调度，将pod分配到Slave节点                   |\n| Master  | controller-manager | 负责Master的其他功能                           |\n| Master  | etde               | 储存配置信息，节点信息，pod信息等                 |\n| Slave   | kubelet            | 负责管理Pod、容器和容器镜像                       |\n| Slave   | proxy              | 将访问Service的请求转发给对应的Pod，做一些负载均衡  |\n| 客户端   | kubectl            | 命令行工具，向apiserver发起创建Pod等请求          |\n\n\n## 二. kiwenlau/kubernetes镜像简介\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。事实上，Kuberenetes未来的开发目标正是将Kubernetes的各个组件运行到容器之中，这样可以方便Kubernetes的部署和升级。现在我将Kubernetes的各个组件全部运行在容器中必然存在很多问题且很多问题是未知的，因此这个项目仅做学习测试而不宜部署到生产环境中。Kubernetes各个组件容器之间的通信通过docker link实现，其中apiserver与ectd的4001端口进行通信，scheduler，controller-manager，kubelet，proxy以及kubectl与apiserver的8080端口进行通信。\n\n<img src=\"151128-single-kubernetes-docker/kubernetes-multiple-docker.png\" width = \"500\"/>\n\n集群的大致运行流程是这样的: 用户通过kubectl命令向apiserver发起创建Pod的请求; scheduler将创建Pod的任务分配给kubelet；kubelet中包含了一个docker命令行工具，该工具会向Docker deamon发起创建容器的请求; Docker deamon负责下载镜像然后创建容器。\n\n我将Docker deamon运行在Ubuntu主机上，因此Docker daemon所创建的应用容器与Kubernetes各个组件的容器均运行在Ubuntu主机上。docker socket采用volume的形式挂载到kubelet容器内，因此kubelet中的docker命令行工具可以直接与主机上的Docker daemon进行通信。\n\n我是直接将kubernetes发布的各个组件的二进制可执行文件安装在/usr/local/bin目录下，因此，修改Dockerfile中的Kubernetes下载链接的版本号，就可以快速构建其他版本的Kubernetes镜像。另外，仅需修改网络配置，就可以很方便地在多个节点上部署Kubernetes。\n\nkiwenlau/kubernetes:1.0.7镜像版本信息:\n\n- ubuntu: 14.04\n- Kubernetes: 1.0.7\n- ectd: 2.2.1\n\nUbuntu主机版本信息:\n\n- ubuntu: 14.04.3 LTS\n- kernel: 3.16.0-30-generic\n- docker: 1.9.1\n\n\n\n## 三. 运行步骤\n\n#### **1. 安装Docker**\n\nubuntu 14.04上安装Docker: \n\n```\ncurl -fLsS https://get.docker.com/ | sh\n```\n\n其他系统请参考: [https://docs.docker.com/](https://docs.docker.com/)\n\n#### **2. 下载Docker镜像**\n\n我将kiwenlau/kubernetes:1.07以及其他用到的Docker镜像都放在[灵雀云](http://www.alauda.cn/)\n\n```\nsudo docker pull index.alauda.cn/kiwenlau/kubernetes:1.0.7\nsudo docker pull index.alauda.cn/kiwenlau/etcd:v2.2.1\nsudo docker pull index.alauda.cn/kiwenlau/nginx:1.9.7\nsudo docker pull index.alauda.cn/kiwenlau/pause:0.8.0\n```\n\n#### **3. 启动Kubernetes**\n\n```sh\ngit clone https://github.com/kiwenlau/single-kubernetes-docker\ncd single-kubernetes-docker/\nsudo chmod +x start-kubernetes-alauda.sh stop-kubernetes.sh\nsudo ./start-kubernetes-alauda.sh\n```\n\n运行结束后进入kubectl容器。容器主机名为kubeclt。可以通过\"exit\"命令退出容器返回到主机，然后可以通过\"sudo docker exec -it kubectl bash\"命令再次进入kubectl容器。\n\n\n#### **4. 测试Kubernetes**\n\n运行测试脚本，该脚本会启动一个nginx pod。\n\n```\nchmod +x test-kubernetes-alauda.sh\n./test-kubernetes-alauda.sh \n```\n\n输出\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```\n\n\n## 四. 参考\n\n1. [meteorhacks/hyperkube](https://github.com/meteorhacks/hyperkube)\n2. [meteorhacks/kube-init](https://github.com/meteorhacks/kube-init)\n3. [Kubernetes: The Future of Cloud Hosting](https://meteorhacks.com/learn-kubernetes-the-future-of-the-cloud)\n4. [Kubernetes 架构浅析](http://weibo.com/p/1001603912843031387951?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)\n5. [An Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)\n\n","slug":"2015/11/28/151128-single-kubernetes-docker","published":1,"updated":"2016-07-03T11:50:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3wn000ygq8stfahoz83"},{"title":"基于Docker搭建单机版Mesos/Marathon","date":"2015-09-18T03:00:00.000Z","_content":"\n**摘要:** 本文介绍了基于Docker搭建单机版Mesos/Marathon的方法，Mesos/Marathon的所有组件均运行于单个容器中。\n\n**GitHub地址:**\n- [kiwenlau/single-mesos-docker](https://github.com/kiwenlau/single-mesos-docker)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2015-09-18](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n\n## 一. 简介\n\n[Mesos](http://mesos.apache.org)是集群资源管理系统，[Marathon](http://mesosphere.github.io/marathon)是运行在Mesos之上的集群计算架构。将Mesos和Marathon打包到[Docker](https://www.docker.com/)镜像中，开发者便可以在本机上快速搭建Mesos/Marathon集群，进行学习和测试。\n\n**kiwenlau/single-mesos**镜像非常简单。Docker容器运行在Ubuntu主机之上，Mesos和Marathon运行在该容器之中。具体来讲，Docker容器中运行了一个Mesos Master和一个Mesos Slave，以及Marathon和[ZooKeeper](https://zookeeper.apache.org/)。集群架构如下图：\n\n<img src=\"150918-single-mesos-docker/single-mesos-marathon.png\" width = \"500\"/>\n\n\n## 二. 搭建Mesos/Marathon集群\n\n#### **1. 下载Docker镜像:**\n\n```sh\nsudo docker pull kiwenlau/single-mesos:3.0\n```\n\n#### **2. 运行Docker容器:**\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root kiwenlau/single-mesos:3.0\n```\n\ndocker run命令运行成功后即进入容器内部，以下为输出：\n\n```bash\nStart ZooKeeper...\nStart Mesos master...\nStart Mesos slave...\nStart Marathon...\n```\n\n\n## 三. 测试Mesos/Marathon集群\n\n#### **1. 通过curl命令调用Marathon的REST API, 创建一个hello程序：**\n\n```sh\ncurl -v -H \"Content-Type: application/json\" -X POST --data \"@hello.json\" http://127.0.0.1:8080/v2/apps\n```\n\n下面为hello.json。由cmd可知，该程序每隔1秒往output.txt文件中写入hello。\n\n```bash\n{\n  \"id\": \"hello\",\n  \"cmd\": \"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\n  \"cpus\": 0.1,\n  \"mem\": 10.0,\n  \"instances\": 1\n}\n```\n\ncurl执行结果:\n\n```bash\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\n> POST /v2/apps HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: 127.0.0.1:8080\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 139\n> \n* upload completely sent off: 139 out of 139 bytes\n< HTTP/1.1 201 Created\n< X-Marathon-Leader: http://ec054cabb9af:8080\n< Cache-Control: no-cache, no-store, must-revalidate\n< Pragma: no-cache\n< Expires: 0\n< Location: http://127.0.0.1:8080/v2/apps/hello\n< Content-Type: application/json; qs=2\n< Transfer-Encoding: chunked\n* Server Jetty(8.y.z-SNAPSHOT) is not blacklisted\n< Server: Jetty(8.y.z-SNAPSHOT)\n< \n* Connection #0 to host 127.0.0.1 left intact\n{\"id\":\"/hello\",\"cmd\":\"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\"args\":null,\"user\":null,\"env\":{},\"instances\":1,\"cpus\":0.1,\"mem\":10.0,\"disk\":0.0,\"executor\":\"\",\"constraints\":[],\"uris\":[],\"storeUrls\":[],\"ports\":[0],\"requirePorts\":false,\"backoffFactor\":1.15,\"container\":null,\"healthChecks\":[],\"dependencies\":[],\"upgradeStrategy\":{\"minimumHealthCapacity\":1.0,\"maximumOverCapacity\":1.0},\"labels\":{},\"acceptedResourceRoles\":null,\"version\":\"2015-09-16T11:22:27.967Z\",\"deployments\":[{\"id\":\"2cd2fdd4-e5f9-4088-895f-7976349b7a19\"}],\"tasks\":[],\"tasksStaged\":0,\"tasksRunning\":0,\"tasksHealthy\":0,\"tasksUnhealthy\":0,\"backoffSeconds\":1,\"maxLaunchDelaySeconds\":3600}\n```\n\n#### **2. 查看hello程序的运行结果：**\n\n```sh\ntail -f output.txt\n```\n当你看到终端不断输出\"hello\"时说明运行成功。\n\n#### **3. 使用浏览器查看Mesos和Marathon的网页管理界面**\n\n**注意**将IP替换运行Docker容器的主机IP地址\n\nMesos网页管理界面地址：[http://192.168.59.10:5050](http://192.168.59.10:5050)\n\nMesos网页管理界面如图，可知hello程序正在运行：\n\n<img src=\"150918-single-mesos-docker/Mesos.png\" width = \"500\"/>\n\nMarathon网页管理界面地址：[http://192.168.59.10:8080](http://192.168.59.10:8080)\n\nMarathon网页管理界面如图，可知hello程序正在运行：\n\n<img src=\"150918-single-mesos-docker/Marathon.png\" width = \"500\"/>\n\n#### **4. 通过Marathon网页管理界面创建测试程序**\n\n在Marathon的网页管理界面上点击\"New APP\"，在弹框中配置测试程序。ID为\"hello\", Command为\"echo hello >> /root/output.txt\", 然后点击\"Create\"即可。如下图：\n\n<img src=\"150918-single-mesos-docker/hello.png\" width = \"300\"/>\n\n\n## 四. 存在的问题\n\n其实，参考[Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)，可以很快地在ubuntu主机上直接搭建一个单节点的Mesos/Marathon集群。但是，当我安装该教程的步骤将Mesos/Marathon集群打包到Docker镜像中时，遇到了一个比较奇怪的问题。\n\n在Docker容器中使用**\"sudo service mesos-master start\"**和**\"sudo service mesos-slave start\"**命令启动Mesos Master和Mesos Slave时，出现**\"mesos-master: unrecognized service\"**和**\"mesos-slave: unrecognized service\"**错误。但是，我在ubuntu主机上安装Mesos/Marathon集群后，使用同样的命令启动Mesos并没有问题。后来，我是通过直接执行mesos-master和mesos-slave命令启动Mesos，命令如下：\n\n```sh\n/usr/sbin/mesos-master --zk=zk://127.0.0.1:2181/mesos --quorum=1 --work_dir=/var/lib/mesos --log_dir=/log/mesos  \n```\n\n```sh\n/usr/sbin/mesos-slave --master=zk://127.0.0.1:2181/mesos --log_dir=/log/mesos\n```\n\n由这个问题可知，虽然在Docker容器几乎可以运行任意程序，似乎和Ubuntu主机没有区别。但是事实上，**Docker容器与ubuntu主机并非完全一致**，而且这些细节的不同点比较坑。这一点很值得探讨，可以让大家在使用Docker时少走些弯路。对于提到的问题，虽然是解决了，然而我仍然不清楚其中的原因:(\n\n\n## 五. Docker镜像备份\n\n我将Docker镜像上传到了灵雀云（Alaudo）的Docker仓库，可以通过以下命令下载和运行：\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n## 六. 参考\n\n1. [Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)\n2. [Setting up a Cluster on Mesos and Marathon](https://open.mesosphere.com/getting-started/datacenter/install/#master-setup)\n3. [An Introduction to Mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)\n4. [How To Configure a Production-Ready Mesosphere Cluster on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04)\n5. [Deploy a Mesos Cluster with 7 Commands Using Docker](https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586)\n6. [sekka1/mesosphere-docker](https://github.com/sekka1/mesosphere-docker)\n7. [Marathon: Application Basics](http://mesosphere.github.io/marathon/docs/application-basics.html)\n8. [Marathon: REST API](http://mesosphere.github.io/marathon/docs/rest-api.html)\n\n","source":"_posts/2015/09/18/150918-single-mesos-docker.md","raw":"title: 基于Docker搭建单机版Mesos/Marathon\n\ndate: 2015-09-18 12:00:00\n\ntags: [Docker,Mesos,Marathon]\n\n---\n\n**摘要:** 本文介绍了基于Docker搭建单机版Mesos/Marathon的方法，Mesos/Marathon的所有组件均运行于单个容器中。\n\n**GitHub地址:**\n- [kiwenlau/single-mesos-docker](https://github.com/kiwenlau/single-mesos-docker)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2015-09-18](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n\n## 一. 简介\n\n[Mesos](http://mesos.apache.org)是集群资源管理系统，[Marathon](http://mesosphere.github.io/marathon)是运行在Mesos之上的集群计算架构。将Mesos和Marathon打包到[Docker](https://www.docker.com/)镜像中，开发者便可以在本机上快速搭建Mesos/Marathon集群，进行学习和测试。\n\n**kiwenlau/single-mesos**镜像非常简单。Docker容器运行在Ubuntu主机之上，Mesos和Marathon运行在该容器之中。具体来讲，Docker容器中运行了一个Mesos Master和一个Mesos Slave，以及Marathon和[ZooKeeper](https://zookeeper.apache.org/)。集群架构如下图：\n\n<img src=\"150918-single-mesos-docker/single-mesos-marathon.png\" width = \"500\"/>\n\n\n## 二. 搭建Mesos/Marathon集群\n\n#### **1. 下载Docker镜像:**\n\n```sh\nsudo docker pull kiwenlau/single-mesos:3.0\n```\n\n#### **2. 运行Docker容器:**\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root kiwenlau/single-mesos:3.0\n```\n\ndocker run命令运行成功后即进入容器内部，以下为输出：\n\n```bash\nStart ZooKeeper...\nStart Mesos master...\nStart Mesos slave...\nStart Marathon...\n```\n\n\n## 三. 测试Mesos/Marathon集群\n\n#### **1. 通过curl命令调用Marathon的REST API, 创建一个hello程序：**\n\n```sh\ncurl -v -H \"Content-Type: application/json\" -X POST --data \"@hello.json\" http://127.0.0.1:8080/v2/apps\n```\n\n下面为hello.json。由cmd可知，该程序每隔1秒往output.txt文件中写入hello。\n\n```bash\n{\n  \"id\": \"hello\",\n  \"cmd\": \"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\n  \"cpus\": 0.1,\n  \"mem\": 10.0,\n  \"instances\": 1\n}\n```\n\ncurl执行结果:\n\n```bash\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\n> POST /v2/apps HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: 127.0.0.1:8080\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 139\n> \n* upload completely sent off: 139 out of 139 bytes\n< HTTP/1.1 201 Created\n< X-Marathon-Leader: http://ec054cabb9af:8080\n< Cache-Control: no-cache, no-store, must-revalidate\n< Pragma: no-cache\n< Expires: 0\n< Location: http://127.0.0.1:8080/v2/apps/hello\n< Content-Type: application/json; qs=2\n< Transfer-Encoding: chunked\n* Server Jetty(8.y.z-SNAPSHOT) is not blacklisted\n< Server: Jetty(8.y.z-SNAPSHOT)\n< \n* Connection #0 to host 127.0.0.1 left intact\n{\"id\":\"/hello\",\"cmd\":\"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\"args\":null,\"user\":null,\"env\":{},\"instances\":1,\"cpus\":0.1,\"mem\":10.0,\"disk\":0.0,\"executor\":\"\",\"constraints\":[],\"uris\":[],\"storeUrls\":[],\"ports\":[0],\"requirePorts\":false,\"backoffFactor\":1.15,\"container\":null,\"healthChecks\":[],\"dependencies\":[],\"upgradeStrategy\":{\"minimumHealthCapacity\":1.0,\"maximumOverCapacity\":1.0},\"labels\":{},\"acceptedResourceRoles\":null,\"version\":\"2015-09-16T11:22:27.967Z\",\"deployments\":[{\"id\":\"2cd2fdd4-e5f9-4088-895f-7976349b7a19\"}],\"tasks\":[],\"tasksStaged\":0,\"tasksRunning\":0,\"tasksHealthy\":0,\"tasksUnhealthy\":0,\"backoffSeconds\":1,\"maxLaunchDelaySeconds\":3600}\n```\n\n#### **2. 查看hello程序的运行结果：**\n\n```sh\ntail -f output.txt\n```\n当你看到终端不断输出\"hello\"时说明运行成功。\n\n#### **3. 使用浏览器查看Mesos和Marathon的网页管理界面**\n\n**注意**将IP替换运行Docker容器的主机IP地址\n\nMesos网页管理界面地址：[http://192.168.59.10:5050](http://192.168.59.10:5050)\n\nMesos网页管理界面如图，可知hello程序正在运行：\n\n<img src=\"150918-single-mesos-docker/Mesos.png\" width = \"500\"/>\n\nMarathon网页管理界面地址：[http://192.168.59.10:8080](http://192.168.59.10:8080)\n\nMarathon网页管理界面如图，可知hello程序正在运行：\n\n<img src=\"150918-single-mesos-docker/Marathon.png\" width = \"500\"/>\n\n#### **4. 通过Marathon网页管理界面创建测试程序**\n\n在Marathon的网页管理界面上点击\"New APP\"，在弹框中配置测试程序。ID为\"hello\", Command为\"echo hello >> /root/output.txt\", 然后点击\"Create\"即可。如下图：\n\n<img src=\"150918-single-mesos-docker/hello.png\" width = \"300\"/>\n\n\n## 四. 存在的问题\n\n其实，参考[Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)，可以很快地在ubuntu主机上直接搭建一个单节点的Mesos/Marathon集群。但是，当我安装该教程的步骤将Mesos/Marathon集群打包到Docker镜像中时，遇到了一个比较奇怪的问题。\n\n在Docker容器中使用**\"sudo service mesos-master start\"**和**\"sudo service mesos-slave start\"**命令启动Mesos Master和Mesos Slave时，出现**\"mesos-master: unrecognized service\"**和**\"mesos-slave: unrecognized service\"**错误。但是，我在ubuntu主机上安装Mesos/Marathon集群后，使用同样的命令启动Mesos并没有问题。后来，我是通过直接执行mesos-master和mesos-slave命令启动Mesos，命令如下：\n\n```sh\n/usr/sbin/mesos-master --zk=zk://127.0.0.1:2181/mesos --quorum=1 --work_dir=/var/lib/mesos --log_dir=/log/mesos  \n```\n\n```sh\n/usr/sbin/mesos-slave --master=zk://127.0.0.1:2181/mesos --log_dir=/log/mesos\n```\n\n由这个问题可知，虽然在Docker容器几乎可以运行任意程序，似乎和Ubuntu主机没有区别。但是事实上，**Docker容器与ubuntu主机并非完全一致**，而且这些细节的不同点比较坑。这一点很值得探讨，可以让大家在使用Docker时少走些弯路。对于提到的问题，虽然是解决了，然而我仍然不清楚其中的原因:(\n\n\n## 五. Docker镜像备份\n\n我将Docker镜像上传到了灵雀云（Alaudo）的Docker仓库，可以通过以下命令下载和运行：\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n## 六. 参考\n\n1. [Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)\n2. [Setting up a Cluster on Mesos and Marathon](https://open.mesosphere.com/getting-started/datacenter/install/#master-setup)\n3. [An Introduction to Mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)\n4. [How To Configure a Production-Ready Mesosphere Cluster on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04)\n5. [Deploy a Mesos Cluster with 7 Commands Using Docker](https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586)\n6. [sekka1/mesosphere-docker](https://github.com/sekka1/mesosphere-docker)\n7. [Marathon: Application Basics](http://mesosphere.github.io/marathon/docs/application-basics.html)\n8. [Marathon: REST API](http://mesosphere.github.io/marathon/docs/rest-api.html)\n\n","slug":"2015/09/18/150918-single-mesos-docker","published":1,"updated":"2016-07-03T11:51:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3wt0011gq8szge3o274"},{"title":"基于Docker搭建多节点Hadoop集群","date":"2015-06-08T03:44:40.000Z","_content":"\n**摘要:** 本文已更新，请查看: [基于Docker搭建Hadoop集群之升级版](http://kiwenlau.com/2016/06/12/160612-hadoop-cluster-docker-update/)!!!\n\n**GitHub地址:**\n- [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2015-06-08](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n\n本文介绍了基于Docker在单机上搭建多节点Hadopp集群方法，Hadoop的Master和Slave分别运行在不同容器中。\n\n可以直接进入第三部分，快速在本机搭建一个3个节点的Hadoop集群\n\n## 一. 项目简介\n\n直接用机器搭建Hadoop集群是一个相当痛苦的过程，尤其对初学者来说。他们还没开始跑wordcount，可能就被这个问题折腾的体无完肤了。\n\n我的目标是将Hadoop集群运行在Docker容器中，使Hadoop开发者能够快速便捷地在本机搭建多节点的Hadoop集群。其实这个想法已经有了不少实现，但是都不是很理想，他们或者镜像太大，或者使用太慢，或者使用了第三方工具使得使用起来过于复杂...下表为一些已知的Hadoop on Docker项目以及其存在的问题。\n\n|项目                              |镜像大小   |问题                  |\n|:--------------------------------|:--------|:---------------------|\n|sequenceiq/hadoop-docker:latest  |1.491GB  | 镜像太大，只支持单个节点 |\n|sequenceiq/hadoop-docker:2.7.0   |1.76 GB  | 同上                  |\n|sequenceiq/hadoop-docker:2.6.0   |1.624GB  | 同上                  |\n|sequenceiq/ambari:latest         |1.782GB   | 镜像太大，使用太慢     |\n|sequenceiq/ambari:2.0.0          |4.804GB   | 同上                 |\n|sequenceiq/ambari:latest:1.70    |4.761GB   | 同上                 |\n|alvinhenrick/hadoop-mutinode     |4.331GB   | 镜像太大，构建太慢，增加节点麻烦，有bug|\n\n我的项目参考了alvinhenrick/hadoop-mutinode项目，不过我做了大量的优化和重构。alvinhenrick/hadoop-mutinode项目的Github主页以及作者所写的博客地址：[GitHub](https://github.com/alvinhenrick/hadoop-mutinode)，[博客](http://alvinhenrick.com/2014/07/16/hadoop-yarn-multinode-cluster-with-docker/)\n\n下面两个表是alvinhenrick/hadoop-mutinode项目与我的kiwenlau/hadoop-cluster-docker项目的参数对比\n\n|镜像名称\t                    |构建时间\t      | 镜像层数  | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n|alvinhenrick/serf          | 258.213s    | 21\t     | 239.4MB |\n|alvinhenrick/hadoop-base\t| 2236.055s   | 58\t     | 4.328GB |\n|alvinhenrick/hadoop-dn\t    | 51.959s     | 74\t     | 4.331GB |\n|alvinhenrick/hadoop-nn-dn  | 49.548s     |  84      | 4.331GB |\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n| kiwenlau/serf-dnsmasq          | 509.46s        |  8\t        | 206.6 MB |\n|kiwenlau/hadoop-base\t         | 400.29s\t        |  7\t        | 775.4 MB |\n|kiwenlau/hadoop-master         | 5.41s            |  9\t        | 775.4 MB |\n|kiwenlau/hadoop-slave\t         | 2.41s\t        |  8 \t        | 775.4 MB |\n\n可知，我主要优化了这样几点\n- 更小的镜像大小\n- 更快的构造时间\n- 更少的镜像层数\n\n#### **更快更方便地改变Hadoop集群节点数目**\n\n另外，alvinhenrick/hadoop-mutinode项目增加节点时需要手动修改Hadoop配置文件然后重新构建hadoop-nn-dn镜像,然后修改容器启动脚本，才能实现增加节点的功能。而我通过shell脚本实现自动话，不到1分钟可以重新构建hadoop-master镜像，然后立即运行！！！本项目默认启动3个节点的Hadoop集群，支持任意节点数的hadoop集群。\n\n另外，启动hadoop, 运行wordcount以及重新构建镜像都采用了shell脚本实现自动化。这样使得整个项目的使用以及开发都变得非常方便快捷:)\n\n#### **开发测试环境**\n\n- 操作系统：ubuntu 14.04 和 ubuntu 12.04\n- 内核版本: 3.13.0-32-generic\n- Docker版本：1.5.0 和1.6.2\n\n#### **硬盘不够，内存不够，尤其是内核版本过低会导致运行失败:(**\n\n## 二. 镜像简介\n\n#### **本项目一共开发了4个镜像**\n\n- serf-dnsmasq\n- hadoop-base\n- hadoop-master\n- hadoop-slave\n\n#### **serf-dnsmasq镜像**\n\n- 基于ubuntu:15.04 (选它是因为它最小，不是因为它最新...)\n- 安装serf: serf是一个分布式的机器节点管理工具。它可以动态地发现所有hadoop集群节点。\n- 安装dnsmasq: dnsmasq作为轻量级的dns服务器。它可以为hadoop集群提供域名解析服务。\n\n容器启动时，master节点的IP会传给所有slave节点。serf会在container启动后立即启动。slave节点上的serf agent会马上发现master节点（master IP它们都知道嘛），master节点就马上发现了所有slave节点。然后它们之间通过互相交换信息，所有节点就能知道其他所有节点的存在了！(Everyone will know Everyone). serf发现新的节点时，就会重新配置dnsmasq,然后重启dnsmasq. 所以dnsmasq就能够解析集群的所有节点的域名啦。这个过程随着节点的增加会耗时更久，因此，若配置的Hadoop节点比较多，则在启动容器后需要测试serf是否发现了所有节点，dns是否能够解析所有节点域名。稍等片刻才能启动Hadoop。这个解决方案是由SequenceIQ公司提出的，该公司专注于将Hadoop运行在Docker中。请参考这个PPT：[Docker-based Hadoop Provisioning](http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning)\n\n#### **hadoop-base镜像**\n\n- 基于serf-dnsmasq镜像\n- 安装JDK(openjdk)\n- 安装openssh-server, 配置无密码ssh\n- 安装vim：介样就可以愉快地在容器中敲代码了:)\n- 安装Hadoop 2.3.0: 安装编译过的hadoop （2.5.2， 2.6.0， 2.7.0 都比2.3.0大，所以我懒得升级了）\n\n编译Hadoop的步骤请参考我的博客：[[Hadoop 2.30 在Ubuntu 14.04 中编译](http://www.cnblogs.com/kiwenlau/p/4227204.html)](http://www.cnblogs.com/kiwenlau/p/4227204.html)\n\n如果需要重新开发我的hadoop-base, 需要下载编译过的hadoop-2.3.0安装包，放到hadoop-cluster-docker/hadoop-base/files目录内。我编译的64位hadoop-2.3.0下载地址：[hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n\n另外，我还编译了64位的hadoop 2.5.2, 2.6.0, 2.7.0, 其下载地址如下：\n\n- [hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n- [hadoop-2.5.2](http://pan.baidu.com/s/1jGw24aa)\n- [hadoop-2.6.0](http://pan.baidu.com/s/1eQgvF2M)\n- [hadoop-2.7.0]( http://pan.baidu.com/s/1c0HD0Nu)\n\n#### **hadoop-master镜像**\n\n- 基于hadoop-base镜像\n- 配置hadoop的master节点\n- 格式化namenode\n\n这一步需要配置slaves文件，而slaves文件需要列出所有节点的域名或者IP。因此，Hadoop节点数目不同时，slaves文件自然也不一样。因此，更改Hadoop集群节点数目时，需要修改slaves文件然后重新构建hadoop-master镜像。我编写了一个resize-cluster.sh脚本自动化这一过程。仅需给定节点数目作为脚本参数就可以轻松实现Hadoop集群节点数目的更改。由于hadoop-master镜像仅仅做一些配置工作，也无需下载任何文件，整个过程非常快，1分钟就足够了。\n\n#### **hadoop-slave镜像**\n\n- 基于hadoop-base镜像\n- 配置hadoop的slave节点\n\n#### **镜像大小分析**\n\n下表为sudo docker images的运行结果\n\n|REPOSITORY       |   TAG    |  IMAGE ID       | CREATED     |   VIRTUAL SIZE |\n| ------------- | ------- | ---------- | ---------- | ------- |\n|index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0|    d63869855c03 |   17 hours ago  |  777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master|    0.1.0   | 7c9d32ede450  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   | 5571bd5de58e    |17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/serf-dnsmasq  |  0.1.0    |09ed89c24ee8   | 17 hours ago  |  206.7 MB |\n|ubuntu     |                               15.04   | bd94ae587483  |  3 weeks ago    |131.3 MB |\n\n\n易知以下几个结论：\n- serf-dnsmasq镜像在ubuntu:15.04镜像的基础上增加了75.4MB\n- hadoop-base镜像在serf-dnsmasq镜像的基础上增加了570.7MB\n- hadoop-master和hadoop-slave镜像在hadoop-base镜像的基础上大小几乎没有增加\n\n下表为docker history index.alauda.cn/kiwenlau/hadoop-base:0.1.0命令的部分运行结果\n\n|IMAGE      |    CREATED        |    CREATED BY         |                             SIZE\n |  -----  | ---------------  | ---------------  |  -----------  | \n|2039b9b81146 |   44 hours ago   |       /bin/sh -c #(nop) ADD   multi:a93c971a49514e787  |  158.5 MB | \n | cdb620312f30    |  44 hours ago   |       /bin/sh -c apt-get install -y openjdk-7-jdk    |  324.6 MB  | \n | da7d10c790c1   |   44 hours ago      |    /bin/sh -c apt-get install -y openssh-server   |   87.58 MB  | \n |  c65cb568defc    |  44 hours ago     |     /bin/sh -c curl -Lso serf.zip https://dl.bint  |  14.46 MB  | \n | 3e22b3d72e33     | 44 hours ago       |   /bin/sh -c apt-get update && apt-get install     |  60.89 MB   | \n |  b68f8c8d2140    |  3 weeks ago     |     /bin/sh -c #(nop) ADD file:d90f7467c470bfa9a3  |   131.3 MB  | \n\n可知\n- 基础镜像ubuntu:15.04为131.3MB\n- 安装openjdk需要324.6MB\n- 安装hadoop需要158.5MB\n- ubuntu,openjdk与hadoop均为镜像所必须，三者一共占了:614.4MB\n\n#### **因此，我所开发的hadoop镜像以及接近最小，优化空间已经很小了**\n\n下图显示了项目的Docker镜像结构：\n\n<img src=\"150608-hadoop-cluster-docker/image-architecture.png\" width = \"500\"/>\n\n## 三. 3节点Hadoop集群搭建步骤\n\n#### 1. **拉取镜像**\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-master:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-slave:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-base:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/serf-dnsmasq:0.1.0\n```\n\n- 3~5分钟OK~\n\n*查看下载的镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n|REPOSITORY    |    TAG  |    IMAGE ID      |  CREATED  |      VIRTUAL SIZE |\n |  ----------  |  -----------  |  ---------  |   --------  |  \n| index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0    |d63869855c03  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master   |   0.1.0 |     7c9d32ede450   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base |       0.1.0     | 5571bd5de58e   |   17 hours ago  |    777.4 MB | \n |  index.alauda.cn/kiwenlau/serf-dnsmasq   |   0.1.0   |   09ed89c24ee8     | 17 hours ago     |  206.7 MB | \n\n- hadoop-base镜像是基于serf-dnsmasq镜像的，hadoop-slave镜像和hadoop-master镜像都是基于hadoop-base镜像\n- 所以其实4个镜像一共也就777.4MB:)\n\n#### **2. 修改镜像tag**\n\n```sh\nsudo docker tag d63869855c03 kiwenlau/hadoop-slave:0.1.0\nsudo docker tag 7c9d32ede450 kiwenlau/hadoop-master:0.1.0\nsudo docker tag 5571bd5de58e kiwenlau/hadoop-base:0.1.0\nsudo docker tag 09ed89c24ee8 kiwenlau/serf-dnsmasq:0.1.0\n```\n\n*查看修改tag后镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n| REPOSITORY   |   TAG   |    IMAGE ID    |    CREATED    |      VIRTUAL SIZE  | \n |  ----------  |  -----  |  ---------  |  ----------  |  -----------  | \n| index.alauda.cn/kiwenlau/hadoop-slave  |    0.1.0   |   d63869855c03    |  17 hours ago   |   777.4 MB\n | kiwenlau/hadoop-slave    |                  0.1.0   |   d63869855c03   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-master  |  0.1.0 |     7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-master  |                  0.1.0   |   7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-base   |                   0.1.0  |    5571bd5de58e  |    17 hours ago   |   777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   |   5571bd5de58e   |   17 hours ago |     777.4 MB | \n | kiwenlau/serf-dnsmasq          |            0.1.0    |  09ed89c24ee8  |    17 hours ago    |  206.7 MB | \n | index.alauda.cn/kiwenlau/serf-dnsmasq  |    0.1.0   |   09ed89c24ee8     | 17 hours ago   |   206.7 MB | \n\n- 之所以要修改镜像，是因为我默认是将镜像上传到Dockerhub, 因此Dokerfile以及shell脚本中得镜像名称都是没有alauada前缀的，sorry for this....不过改tag还是很快滴\n- 若直接下载我在DockerHub中的镜像，自然就不需要修改tag...不过Alauda镜像下载速度很快的哈~\n\n#### **3.下载源代码**\n\n```sh\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n- 为了防止Github被XX, 我把代码导入到了开源中国的git仓库\n\n```sh\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n```\n\n#### **4. 运行容器**\n\n```sh\ncd hadoop-cluster-docker\n./start-container.sh\n\n```\n\n*运行结果*\n\n```bash\nstart master container...\nstart slave1 container...\nstart slave2 container...\nroot@master:~#\n```\n\n- 一共开启了3个容器，1个master, 2个slave\n- 开启容器后就进入了master容器root用户的家目录（/root）\n\n*查看master的root用户家目录的文件*\n\n```sh\nls\n```\n\n*运行结果*\n\n```plain\nhdfs  run-wordcount.sh\tserf_log  start-hadoop.sh  start-ssh-serf.sh\n```\n\n- start-hadoop.sh是开启hadoop的shell脚本\n- run-wordcount.sh是运行wordcount的shell脚本，可以测试镜像是否正常工作\n\n#### **5.测试容器是否正常启动(此时已进入master容器)**\n\n*查看hadoop集群成员*\n\n```sh\nserf members\n```\n\n*运行结果*\n\n```bash\nmaster.kiwenlau.com  172.17.0.65:7946  alive\nslave1.kiwenlau.com  172.17.0.66:7946  alive\nslave2.kiwenlau.com  172.17.0.67:7946  alive\n```\n\n- 若结果缺少节点，可以稍等片刻，再执行“serf members”命令。因为serf agent需要时间发现所有节点。\n\n*测试ssh*\n\n```sh\nssh slave2.kiwenlau.com\n```\n\n*运行结果*\n\n```bash\nWarning: Permanently added 'slave2.kiwenlau.com,172.17.0.67' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 15.04 (GNU/Linux 3.13.0-53-generic x86_64)\n* Documentation:  https://help.ubuntu.com/\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\nroot@slave2:~#\n```\n\n*退出slave2*\n\n```sh\nexit\n```\n\n*运行结果*\n\n```bash\nlogout\nConnection to slave2.kiwenlau.com closed.\n```\n\n- 若ssh失败，请稍等片刻再测试，因为dnsmasq的dns服务器启动需要时间。\n- 测试成功后，就可以开启Hadoop集群了！其实你也可以不进行测试，开启容器后耐心等待一分钟即可！\n\n#### **6. 开启hadoop**\n\n```sh\n./start-hadoop.sh\n```\n\n- 上一步ssh到slave2之后，请记得回到master啊!!！\n- 运行结果太多，忽略....\n- hadoop的启动速度取决于机器性能....\n\n#### **7. 运行wordcount**\n\n```sh\n./run-wordcount.sh\n```\n\n*运行结果*\n```bash\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\n- wordcount的执行速度取决于机器性能....\n\n## 四. N节点Hadoop集群搭建步骤\n\n#### **1. 准备工作**\n\n- 参考第二部分1~3：下载镜像，修改tag，下载源代码\n- 注意，你可以不下载serf-dnsmasq, 但是请最好下载hadoop-base，因为hadoop-master是基于hadoop-base构建的\n\n#### **2. 重新构建hadoop-master镜像**\n\n```sh\n./resize-cluster.sh 5\n```\n\n- 不要担心，1分钟就能搞定\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n\n#### **3. 启动容器**\n\n```sh\n./start-container.sh 5\n```\n\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n- 这个参数呢，最好还是得和上一步的参数一致:)\n- 这个参数如果比上一步的参数大，你多启动的节点，Hadoop不认识它们..\n- 这个参数如果比上一步的参数小，Hadoop觉得少启动的节点挂掉了..\n\n#### **4. 测试工作**\n\n- 参考第三部分5~7：测试容器，开启Hadoop，运行wordcount\n- 请注意，若节点增加，请务必先测试容器，然后再开启Hadoop, 因为serf可能还没有发现所有节点，而dnsmasq的DNS服务器表示还没有配置好服务\n- 测试等待时间取决于机器性能....\n\n","source":"_posts/2015/06/08/150608-hadoop-cluster-docker.md","raw":"title: 基于Docker搭建多节点Hadoop集群\n\ndate: 2015-06-08 12:44:40\n\ntags: [Hadoop, Docker]\n\n---\n\n**摘要:** 本文已更新，请查看: [基于Docker搭建Hadoop集群之升级版](http://kiwenlau.com/2016/06/12/160612-hadoop-cluster-docker-update/)!!!\n\n**GitHub地址:**\n- [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau)\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2015-06-08](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n\n本文介绍了基于Docker在单机上搭建多节点Hadopp集群方法，Hadoop的Master和Slave分别运行在不同容器中。\n\n可以直接进入第三部分，快速在本机搭建一个3个节点的Hadoop集群\n\n## 一. 项目简介\n\n直接用机器搭建Hadoop集群是一个相当痛苦的过程，尤其对初学者来说。他们还没开始跑wordcount，可能就被这个问题折腾的体无完肤了。\n\n我的目标是将Hadoop集群运行在Docker容器中，使Hadoop开发者能够快速便捷地在本机搭建多节点的Hadoop集群。其实这个想法已经有了不少实现，但是都不是很理想，他们或者镜像太大，或者使用太慢，或者使用了第三方工具使得使用起来过于复杂...下表为一些已知的Hadoop on Docker项目以及其存在的问题。\n\n|项目                              |镜像大小   |问题                  |\n|:--------------------------------|:--------|:---------------------|\n|sequenceiq/hadoop-docker:latest  |1.491GB  | 镜像太大，只支持单个节点 |\n|sequenceiq/hadoop-docker:2.7.0   |1.76 GB  | 同上                  |\n|sequenceiq/hadoop-docker:2.6.0   |1.624GB  | 同上                  |\n|sequenceiq/ambari:latest         |1.782GB   | 镜像太大，使用太慢     |\n|sequenceiq/ambari:2.0.0          |4.804GB   | 同上                 |\n|sequenceiq/ambari:latest:1.70    |4.761GB   | 同上                 |\n|alvinhenrick/hadoop-mutinode     |4.331GB   | 镜像太大，构建太慢，增加节点麻烦，有bug|\n\n我的项目参考了alvinhenrick/hadoop-mutinode项目，不过我做了大量的优化和重构。alvinhenrick/hadoop-mutinode项目的Github主页以及作者所写的博客地址：[GitHub](https://github.com/alvinhenrick/hadoop-mutinode)，[博客](http://alvinhenrick.com/2014/07/16/hadoop-yarn-multinode-cluster-with-docker/)\n\n下面两个表是alvinhenrick/hadoop-mutinode项目与我的kiwenlau/hadoop-cluster-docker项目的参数对比\n\n|镜像名称\t                    |构建时间\t      | 镜像层数  | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n|alvinhenrick/serf          | 258.213s    | 21\t     | 239.4MB |\n|alvinhenrick/hadoop-base\t| 2236.055s   | 58\t     | 4.328GB |\n|alvinhenrick/hadoop-dn\t    | 51.959s     | 74\t     | 4.331GB |\n|alvinhenrick/hadoop-nn-dn  | 49.548s     |  84      | 4.331GB |\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n| kiwenlau/serf-dnsmasq          | 509.46s        |  8\t        | 206.6 MB |\n|kiwenlau/hadoop-base\t         | 400.29s\t        |  7\t        | 775.4 MB |\n|kiwenlau/hadoop-master         | 5.41s            |  9\t        | 775.4 MB |\n|kiwenlau/hadoop-slave\t         | 2.41s\t        |  8 \t        | 775.4 MB |\n\n可知，我主要优化了这样几点\n- 更小的镜像大小\n- 更快的构造时间\n- 更少的镜像层数\n\n#### **更快更方便地改变Hadoop集群节点数目**\n\n另外，alvinhenrick/hadoop-mutinode项目增加节点时需要手动修改Hadoop配置文件然后重新构建hadoop-nn-dn镜像,然后修改容器启动脚本，才能实现增加节点的功能。而我通过shell脚本实现自动话，不到1分钟可以重新构建hadoop-master镜像，然后立即运行！！！本项目默认启动3个节点的Hadoop集群，支持任意节点数的hadoop集群。\n\n另外，启动hadoop, 运行wordcount以及重新构建镜像都采用了shell脚本实现自动化。这样使得整个项目的使用以及开发都变得非常方便快捷:)\n\n#### **开发测试环境**\n\n- 操作系统：ubuntu 14.04 和 ubuntu 12.04\n- 内核版本: 3.13.0-32-generic\n- Docker版本：1.5.0 和1.6.2\n\n#### **硬盘不够，内存不够，尤其是内核版本过低会导致运行失败:(**\n\n## 二. 镜像简介\n\n#### **本项目一共开发了4个镜像**\n\n- serf-dnsmasq\n- hadoop-base\n- hadoop-master\n- hadoop-slave\n\n#### **serf-dnsmasq镜像**\n\n- 基于ubuntu:15.04 (选它是因为它最小，不是因为它最新...)\n- 安装serf: serf是一个分布式的机器节点管理工具。它可以动态地发现所有hadoop集群节点。\n- 安装dnsmasq: dnsmasq作为轻量级的dns服务器。它可以为hadoop集群提供域名解析服务。\n\n容器启动时，master节点的IP会传给所有slave节点。serf会在container启动后立即启动。slave节点上的serf agent会马上发现master节点（master IP它们都知道嘛），master节点就马上发现了所有slave节点。然后它们之间通过互相交换信息，所有节点就能知道其他所有节点的存在了！(Everyone will know Everyone). serf发现新的节点时，就会重新配置dnsmasq,然后重启dnsmasq. 所以dnsmasq就能够解析集群的所有节点的域名啦。这个过程随着节点的增加会耗时更久，因此，若配置的Hadoop节点比较多，则在启动容器后需要测试serf是否发现了所有节点，dns是否能够解析所有节点域名。稍等片刻才能启动Hadoop。这个解决方案是由SequenceIQ公司提出的，该公司专注于将Hadoop运行在Docker中。请参考这个PPT：[Docker-based Hadoop Provisioning](http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning)\n\n#### **hadoop-base镜像**\n\n- 基于serf-dnsmasq镜像\n- 安装JDK(openjdk)\n- 安装openssh-server, 配置无密码ssh\n- 安装vim：介样就可以愉快地在容器中敲代码了:)\n- 安装Hadoop 2.3.0: 安装编译过的hadoop （2.5.2， 2.6.0， 2.7.0 都比2.3.0大，所以我懒得升级了）\n\n编译Hadoop的步骤请参考我的博客：[[Hadoop 2.30 在Ubuntu 14.04 中编译](http://www.cnblogs.com/kiwenlau/p/4227204.html)](http://www.cnblogs.com/kiwenlau/p/4227204.html)\n\n如果需要重新开发我的hadoop-base, 需要下载编译过的hadoop-2.3.0安装包，放到hadoop-cluster-docker/hadoop-base/files目录内。我编译的64位hadoop-2.3.0下载地址：[hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n\n另外，我还编译了64位的hadoop 2.5.2, 2.6.0, 2.7.0, 其下载地址如下：\n\n- [hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n- [hadoop-2.5.2](http://pan.baidu.com/s/1jGw24aa)\n- [hadoop-2.6.0](http://pan.baidu.com/s/1eQgvF2M)\n- [hadoop-2.7.0]( http://pan.baidu.com/s/1c0HD0Nu)\n\n#### **hadoop-master镜像**\n\n- 基于hadoop-base镜像\n- 配置hadoop的master节点\n- 格式化namenode\n\n这一步需要配置slaves文件，而slaves文件需要列出所有节点的域名或者IP。因此，Hadoop节点数目不同时，slaves文件自然也不一样。因此，更改Hadoop集群节点数目时，需要修改slaves文件然后重新构建hadoop-master镜像。我编写了一个resize-cluster.sh脚本自动化这一过程。仅需给定节点数目作为脚本参数就可以轻松实现Hadoop集群节点数目的更改。由于hadoop-master镜像仅仅做一些配置工作，也无需下载任何文件，整个过程非常快，1分钟就足够了。\n\n#### **hadoop-slave镜像**\n\n- 基于hadoop-base镜像\n- 配置hadoop的slave节点\n\n#### **镜像大小分析**\n\n下表为sudo docker images的运行结果\n\n|REPOSITORY       |   TAG    |  IMAGE ID       | CREATED     |   VIRTUAL SIZE |\n| ------------- | ------- | ---------- | ---------- | ------- |\n|index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0|    d63869855c03 |   17 hours ago  |  777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master|    0.1.0   | 7c9d32ede450  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   | 5571bd5de58e    |17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/serf-dnsmasq  |  0.1.0    |09ed89c24ee8   | 17 hours ago  |  206.7 MB |\n|ubuntu     |                               15.04   | bd94ae587483  |  3 weeks ago    |131.3 MB |\n\n\n易知以下几个结论：\n- serf-dnsmasq镜像在ubuntu:15.04镜像的基础上增加了75.4MB\n- hadoop-base镜像在serf-dnsmasq镜像的基础上增加了570.7MB\n- hadoop-master和hadoop-slave镜像在hadoop-base镜像的基础上大小几乎没有增加\n\n下表为docker history index.alauda.cn/kiwenlau/hadoop-base:0.1.0命令的部分运行结果\n\n|IMAGE      |    CREATED        |    CREATED BY         |                             SIZE\n |  -----  | ---------------  | ---------------  |  -----------  | \n|2039b9b81146 |   44 hours ago   |       /bin/sh -c #(nop) ADD   multi:a93c971a49514e787  |  158.5 MB | \n | cdb620312f30    |  44 hours ago   |       /bin/sh -c apt-get install -y openjdk-7-jdk    |  324.6 MB  | \n | da7d10c790c1   |   44 hours ago      |    /bin/sh -c apt-get install -y openssh-server   |   87.58 MB  | \n |  c65cb568defc    |  44 hours ago     |     /bin/sh -c curl -Lso serf.zip https://dl.bint  |  14.46 MB  | \n | 3e22b3d72e33     | 44 hours ago       |   /bin/sh -c apt-get update && apt-get install     |  60.89 MB   | \n |  b68f8c8d2140    |  3 weeks ago     |     /bin/sh -c #(nop) ADD file:d90f7467c470bfa9a3  |   131.3 MB  | \n\n可知\n- 基础镜像ubuntu:15.04为131.3MB\n- 安装openjdk需要324.6MB\n- 安装hadoop需要158.5MB\n- ubuntu,openjdk与hadoop均为镜像所必须，三者一共占了:614.4MB\n\n#### **因此，我所开发的hadoop镜像以及接近最小，优化空间已经很小了**\n\n下图显示了项目的Docker镜像结构：\n\n<img src=\"150608-hadoop-cluster-docker/image-architecture.png\" width = \"500\"/>\n\n## 三. 3节点Hadoop集群搭建步骤\n\n#### 1. **拉取镜像**\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-master:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-slave:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-base:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/serf-dnsmasq:0.1.0\n```\n\n- 3~5分钟OK~\n\n*查看下载的镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n|REPOSITORY    |    TAG  |    IMAGE ID      |  CREATED  |      VIRTUAL SIZE |\n |  ----------  |  -----------  |  ---------  |   --------  |  \n| index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0    |d63869855c03  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master   |   0.1.0 |     7c9d32ede450   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base |       0.1.0     | 5571bd5de58e   |   17 hours ago  |    777.4 MB | \n |  index.alauda.cn/kiwenlau/serf-dnsmasq   |   0.1.0   |   09ed89c24ee8     | 17 hours ago     |  206.7 MB | \n\n- hadoop-base镜像是基于serf-dnsmasq镜像的，hadoop-slave镜像和hadoop-master镜像都是基于hadoop-base镜像\n- 所以其实4个镜像一共也就777.4MB:)\n\n#### **2. 修改镜像tag**\n\n```sh\nsudo docker tag d63869855c03 kiwenlau/hadoop-slave:0.1.0\nsudo docker tag 7c9d32ede450 kiwenlau/hadoop-master:0.1.0\nsudo docker tag 5571bd5de58e kiwenlau/hadoop-base:0.1.0\nsudo docker tag 09ed89c24ee8 kiwenlau/serf-dnsmasq:0.1.0\n```\n\n*查看修改tag后镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n| REPOSITORY   |   TAG   |    IMAGE ID    |    CREATED    |      VIRTUAL SIZE  | \n |  ----------  |  -----  |  ---------  |  ----------  |  -----------  | \n| index.alauda.cn/kiwenlau/hadoop-slave  |    0.1.0   |   d63869855c03    |  17 hours ago   |   777.4 MB\n | kiwenlau/hadoop-slave    |                  0.1.0   |   d63869855c03   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-master  |  0.1.0 |     7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-master  |                  0.1.0   |   7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-base   |                   0.1.0  |    5571bd5de58e  |    17 hours ago   |   777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   |   5571bd5de58e   |   17 hours ago |     777.4 MB | \n | kiwenlau/serf-dnsmasq          |            0.1.0    |  09ed89c24ee8  |    17 hours ago    |  206.7 MB | \n | index.alauda.cn/kiwenlau/serf-dnsmasq  |    0.1.0   |   09ed89c24ee8     | 17 hours ago   |   206.7 MB | \n\n- 之所以要修改镜像，是因为我默认是将镜像上传到Dockerhub, 因此Dokerfile以及shell脚本中得镜像名称都是没有alauada前缀的，sorry for this....不过改tag还是很快滴\n- 若直接下载我在DockerHub中的镜像，自然就不需要修改tag...不过Alauda镜像下载速度很快的哈~\n\n#### **3.下载源代码**\n\n```sh\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n- 为了防止Github被XX, 我把代码导入到了开源中国的git仓库\n\n```sh\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n```\n\n#### **4. 运行容器**\n\n```sh\ncd hadoop-cluster-docker\n./start-container.sh\n\n```\n\n*运行结果*\n\n```bash\nstart master container...\nstart slave1 container...\nstart slave2 container...\nroot@master:~#\n```\n\n- 一共开启了3个容器，1个master, 2个slave\n- 开启容器后就进入了master容器root用户的家目录（/root）\n\n*查看master的root用户家目录的文件*\n\n```sh\nls\n```\n\n*运行结果*\n\n```plain\nhdfs  run-wordcount.sh\tserf_log  start-hadoop.sh  start-ssh-serf.sh\n```\n\n- start-hadoop.sh是开启hadoop的shell脚本\n- run-wordcount.sh是运行wordcount的shell脚本，可以测试镜像是否正常工作\n\n#### **5.测试容器是否正常启动(此时已进入master容器)**\n\n*查看hadoop集群成员*\n\n```sh\nserf members\n```\n\n*运行结果*\n\n```bash\nmaster.kiwenlau.com  172.17.0.65:7946  alive\nslave1.kiwenlau.com  172.17.0.66:7946  alive\nslave2.kiwenlau.com  172.17.0.67:7946  alive\n```\n\n- 若结果缺少节点，可以稍等片刻，再执行“serf members”命令。因为serf agent需要时间发现所有节点。\n\n*测试ssh*\n\n```sh\nssh slave2.kiwenlau.com\n```\n\n*运行结果*\n\n```bash\nWarning: Permanently added 'slave2.kiwenlau.com,172.17.0.67' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 15.04 (GNU/Linux 3.13.0-53-generic x86_64)\n* Documentation:  https://help.ubuntu.com/\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\nroot@slave2:~#\n```\n\n*退出slave2*\n\n```sh\nexit\n```\n\n*运行结果*\n\n```bash\nlogout\nConnection to slave2.kiwenlau.com closed.\n```\n\n- 若ssh失败，请稍等片刻再测试，因为dnsmasq的dns服务器启动需要时间。\n- 测试成功后，就可以开启Hadoop集群了！其实你也可以不进行测试，开启容器后耐心等待一分钟即可！\n\n#### **6. 开启hadoop**\n\n```sh\n./start-hadoop.sh\n```\n\n- 上一步ssh到slave2之后，请记得回到master啊!!！\n- 运行结果太多，忽略....\n- hadoop的启动速度取决于机器性能....\n\n#### **7. 运行wordcount**\n\n```sh\n./run-wordcount.sh\n```\n\n*运行结果*\n```bash\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\n- wordcount的执行速度取决于机器性能....\n\n## 四. N节点Hadoop集群搭建步骤\n\n#### **1. 准备工作**\n\n- 参考第二部分1~3：下载镜像，修改tag，下载源代码\n- 注意，你可以不下载serf-dnsmasq, 但是请最好下载hadoop-base，因为hadoop-master是基于hadoop-base构建的\n\n#### **2. 重新构建hadoop-master镜像**\n\n```sh\n./resize-cluster.sh 5\n```\n\n- 不要担心，1分钟就能搞定\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n\n#### **3. 启动容器**\n\n```sh\n./start-container.sh 5\n```\n\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n- 这个参数呢，最好还是得和上一步的参数一致:)\n- 这个参数如果比上一步的参数大，你多启动的节点，Hadoop不认识它们..\n- 这个参数如果比上一步的参数小，Hadoop觉得少启动的节点挂掉了..\n\n#### **4. 测试工作**\n\n- 参考第三部分5~7：测试容器，开启Hadoop，运行wordcount\n- 请注意，若节点增加，请务必先测试容器，然后再开启Hadoop, 因为serf可能还没有发现所有节点，而dnsmasq的DNS服务器表示还没有配置好服务\n- 测试等待时间取决于机器性能....\n\n","slug":"2015/06/08/150608-hadoop-cluster-docker","published":1,"updated":"2016-07-03T12:02:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisvxd3xe0015gq8sh2e9my0k"}],"PostAsset":[{"_id":"source/_posts/2016/09/11/mongodb-inverted-index/mongodb-inverted-index.graffle","post":"cisvxd3tq0000gq8s2krgs2zb","slug":"mongodb-inverted-index.graffle","modified":1},{"_id":"source/_posts/2016/09/04/what-is-service-discovery/service-discovery.graffle","post":"cisvxd3u60003gq8shrb9zkqg","slug":"service-discovery.graffle","modified":1},{"_id":"source/_posts/2016/09/04/what-is-service-discovery/service-discovery.png","post":"cisvxd3u60003gq8shrb9zkqg","slug":"service-discovery.png","modified":1},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/marathon.png","post":"cisvxd3ug0006gq8s4hqbd3l8","slug":"marathon.png","modified":1},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/mesos-marathon-platform.graffle","post":"cisvxd3ug0006gq8s4hqbd3l8","slug":"mesos-marathon-platform.graffle","modified":1},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/mesos-marathon-platform.png","post":"cisvxd3ug0006gq8s4hqbd3l8","slug":"mesos-marathon-platform.png","modified":1},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/mesos.png","post":"cisvxd3ug0006gq8s4hqbd3l8","slug":"mesos.png","modified":1},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/nginx.png","post":"cisvxd3ug0006gq8s4hqbd3l8","slug":"nginx.png","modified":1},{"_id":"source/_posts/2016/07/10/mesos-marathon-platform/zookeeper.png","post":"cisvxd3ug0006gq8s4hqbd3l8","slug":"zookeeper.png","modified":1},{"_id":"source/_posts/2016/07/03/vagrant-vm-cluster/vagrant-vm-cluster.graffle","post":"cisvxd3ux000dgq8suztrt2cc","slug":"vagrant-vm-cluster.graffle","modified":1},{"_id":"source/_posts/2016/07/03/vagrant-vm-cluster/vagrant-vm-cluster.png","post":"cisvxd3ux000dgq8suztrt2cc","slug":"vagrant-vm-cluster.png","modified":1},{"_id":"source/_posts/2016/06/26/hadoop-cluster-docker-update-english/hadoop-cluster-docker.graffle","post":"cisvxd3v5000ggq8sq53r4uwv","slug":"hadoop-cluster-docker.graffle","modified":1},{"_id":"source/_posts/2016/06/26/hadoop-cluster-docker-update-english/hadoop-cluster-docker.png","post":"cisvxd3v5000ggq8sq53r4uwv","slug":"hadoop-cluster-docker.png","modified":1},{"_id":"source/_posts/2016/06/19/160619-vagrant-virtual-machine/vagrant-vm.graffle","post":"cisvxd3ve000kgq8swxndz864","slug":"vagrant-vm.graffle","modified":1},{"_id":"source/_posts/2016/06/19/160619-vagrant-virtual-machine/vagrant-vm.png","post":"cisvxd3ve000kgq8swxndz864","slug":"vagrant-vm.png","modified":1},{"_id":"source/_posts/2016/06/12/160612-hadoop-cluster-docker-update/hadoop-cluster-docker.graffle","post":"cisvxd3vn000mgq8sx4fas8k2","slug":"hadoop-cluster-docker.graffle","modified":1},{"_id":"source/_posts/2016/06/12/160612-hadoop-cluster-docker-update/hadoop-cluster-docker.png","post":"cisvxd3vn000mgq8sx4fas8k2","slug":"hadoop-cluster-docker.png","modified":1},{"_id":"source/_posts/2016/06/05/160605-compile-hadoop-docker/hadoop-docker.graffle","post":"cisvxd3vu000pgq8sj1jaqy44","slug":"hadoop-docker.graffle","modified":1},{"_id":"source/_posts/2016/06/05/160605-compile-hadoop-docker/hadoop-docker.png","post":"cisvxd3vu000pgq8sj1jaqy44","slug":"hadoop-docker.png","modified":1},{"_id":"source/_posts/2016/05/29/160529-compile-hadoop-ubuntu/ubuntu-hadoop.graffle","post":"cisvxd3w0000sgq8sbscq21nd","slug":"ubuntu-hadoop.graffle","modified":1},{"_id":"source/_posts/2016/05/29/160529-compile-hadoop-ubuntu/ubuntu-hadoop.png","post":"cisvxd3w0000sgq8sbscq21nd","slug":"ubuntu-hadoop.png","modified":1},{"_id":"source/_posts/2016/01/09/160109-multiple-processes--docker-container/kubernetes-multiple-docker.graffle","post":"cisvxd3w7000ugq8sxkpuf3a9","slug":"kubernetes-multiple-docker.graffle","modified":1},{"_id":"source/_posts/2016/01/09/160109-multiple-processes--docker-container/kubernetes-multiple-docker.png","post":"cisvxd3w7000ugq8sxkpuf3a9","slug":"kubernetes-multiple-docker.png","modified":1},{"_id":"source/_posts/2016/01/09/160109-multiple-processes--docker-container/kubernetes-single-docker.graffle","post":"cisvxd3w7000ugq8sxkpuf3a9","slug":"kubernetes-single-docker.graffle","modified":1},{"_id":"source/_posts/2016/01/09/160109-multiple-processes--docker-container/kubernetes-single-docker.png","post":"cisvxd3w7000ugq8sxkpuf3a9","slug":"kubernetes-single-docker.png","modified":1},{"_id":"source/_posts/2015/11/28/151128-single-kubernetes-docker/kubernetes-multiple-docker.graffle","post":"cisvxd3wn000ygq8stfahoz83","slug":"kubernetes-multiple-docker.graffle","modified":1},{"_id":"source/_posts/2015/11/28/151128-single-kubernetes-docker/kubernetes-multiple-docker.png","post":"cisvxd3wn000ygq8stfahoz83","slug":"kubernetes-multiple-docker.png","modified":1},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/Marathon.png","post":"cisvxd3wt0011gq8szge3o274","slug":"Marathon.png","modified":1},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/Mesos.png","post":"cisvxd3wt0011gq8szge3o274","slug":"Mesos.png","modified":1},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/hello.png","post":"cisvxd3wt0011gq8szge3o274","slug":"hello.png","modified":1},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/single-mesos-marathon.graffle","post":"cisvxd3wt0011gq8szge3o274","slug":"single-mesos-marathon.graffle","modified":1},{"_id":"source/_posts/2015/09/18/150918-single-mesos-docker/single-mesos-marathon.png","post":"cisvxd3wt0011gq8szge3o274","slug":"single-mesos-marathon.png","modified":1},{"_id":"source/_posts/2015/06/08/150608-hadoop-cluster-docker/image-architecture.graffle","post":"cisvxd3xe0015gq8sh2e9my0k","slug":"image-architecture.graffle","modified":1},{"_id":"source/_posts/2015/06/08/150608-hadoop-cluster-docker/image-architecture.png","post":"cisvxd3xe0015gq8sh2e9my0k","slug":"image-architecture.png","modified":1},{"_id":"source/_posts/2016/09/11/mongodb-inverted-index/mongodb-inverted-index.png","slug":"mongodb-inverted-index.png","post":"cisvxd3tq0000gq8s2krgs2zb","modified":1}],"PostCategory":[],"PostTag":[{"post_id":"cisvxd3tq0000gq8s2krgs2zb","tag_id":"cisvxd3tw0001gq8sbxyqelsw","_id":"cisvxd3ty0002gq8sgll4m1jk"},{"post_id":"cisvxd3u60003gq8shrb9zkqg","tag_id":"cisvxd3u70004gq8svvmm9d6b","_id":"cisvxd3u80005gq8s6yw7idy6"},{"post_id":"cisvxd3ug0006gq8s4hqbd3l8","tag_id":"cisvxd3uh0007gq8s5arvwy1v","_id":"cisvxd3ui000agq8surmw5dmn"},{"post_id":"cisvxd3ug0006gq8s4hqbd3l8","tag_id":"cisvxd3ui0008gq8ss3f7p8mx","_id":"cisvxd3ui000bgq8s7amotlve"},{"post_id":"cisvxd3ug0006gq8s4hqbd3l8","tag_id":"cisvxd3ui0009gq8sc4y1t3u1","_id":"cisvxd3ui000cgq8s3rfgm53z"},{"post_id":"cisvxd3ux000dgq8suztrt2cc","tag_id":"cisvxd3uy000egq8s378lxjhm","_id":"cisvxd3uy000fgq8s0by5o3jb"},{"post_id":"cisvxd3v5000ggq8sq53r4uwv","tag_id":"cisvxd3uh0007gq8s5arvwy1v","_id":"cisvxd3v7000igq8sl6je7hl4"},{"post_id":"cisvxd3v5000ggq8sq53r4uwv","tag_id":"cisvxd3v6000hgq8s4cbv51oo","_id":"cisvxd3v7000jgq8s39orc70t"},{"post_id":"cisvxd3ve000kgq8swxndz864","tag_id":"cisvxd3uy000egq8s378lxjhm","_id":"cisvxd3vf000lgq8so5q8y1uh"},{"post_id":"cisvxd3vn000mgq8sx4fas8k2","tag_id":"cisvxd3uh0007gq8s5arvwy1v","_id":"cisvxd3vn000ngq8s72vwz984"},{"post_id":"cisvxd3vn000mgq8sx4fas8k2","tag_id":"cisvxd3v6000hgq8s4cbv51oo","_id":"cisvxd3vo000ogq8sizfa59yy"},{"post_id":"cisvxd3vu000pgq8sj1jaqy44","tag_id":"cisvxd3uh0007gq8s5arvwy1v","_id":"cisvxd3vv000qgq8s4ozbrb22"},{"post_id":"cisvxd3vu000pgq8sj1jaqy44","tag_id":"cisvxd3v6000hgq8s4cbv51oo","_id":"cisvxd3vw000rgq8sncyc71sj"},{"post_id":"cisvxd3w0000sgq8sbscq21nd","tag_id":"cisvxd3v6000hgq8s4cbv51oo","_id":"cisvxd3w2000tgq8sfizqj2gp"},{"post_id":"cisvxd3w7000ugq8sxkpuf3a9","tag_id":"cisvxd3uh0007gq8s5arvwy1v","_id":"cisvxd3w9000wgq8shdm6yzz8"},{"post_id":"cisvxd3w7000ugq8sxkpuf3a9","tag_id":"cisvxd3w9000vgq8s3x68d0kr","_id":"cisvxd3w9000xgq8s48bdrt0p"},{"post_id":"cisvxd3wn000ygq8stfahoz83","tag_id":"cisvxd3uh0007gq8s5arvwy1v","_id":"cisvxd3wo000zgq8smkzdn42n"},{"post_id":"cisvxd3wn000ygq8stfahoz83","tag_id":"cisvxd3w9000vgq8s3x68d0kr","_id":"cisvxd3wo0010gq8sytnoxs25"},{"post_id":"cisvxd3wt0011gq8szge3o274","tag_id":"cisvxd3uh0007gq8s5arvwy1v","_id":"cisvxd3wu0012gq8s1dgliw66"},{"post_id":"cisvxd3wt0011gq8szge3o274","tag_id":"cisvxd3ui0008gq8ss3f7p8mx","_id":"cisvxd3wu0013gq8svm6uupov"},{"post_id":"cisvxd3wt0011gq8szge3o274","tag_id":"cisvxd3ui0009gq8sc4y1t3u1","_id":"cisvxd3wu0014gq8sudz0hsz2"},{"post_id":"cisvxd3xe0015gq8sh2e9my0k","tag_id":"cisvxd3v6000hgq8s4cbv51oo","_id":"cisvxd3xf0016gq8sbq2u45ad"},{"post_id":"cisvxd3xe0015gq8sh2e9my0k","tag_id":"cisvxd3uh0007gq8s5arvwy1v","_id":"cisvxd3xf0017gq8sxjyuphzp"}],"Tag":[{"name":"MongoDB","_id":"cisvxd3tw0001gq8sbxyqelsw"},{"name":"服务发现","_id":"cisvxd3u70004gq8svvmm9d6b"},{"name":"Docker","_id":"cisvxd3uh0007gq8s5arvwy1v"},{"name":"Mesos","_id":"cisvxd3ui0008gq8ss3f7p8mx"},{"name":"Marathon","_id":"cisvxd3ui0009gq8sc4y1t3u1"},{"name":"Vagrant","_id":"cisvxd3uy000egq8s378lxjhm"},{"name":"Hadoop","_id":"cisvxd3v6000hgq8s4cbv51oo"},{"name":"Kubernetes","_id":"cisvxd3w9000vgq8s3x68d0kr"}]}}